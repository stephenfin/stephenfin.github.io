<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Stephen Finucane (Fin-oo-can)</title>
    <link>https://that.guru/blog/</link>
    <description>Recent content in Blogs on Stephen Finucane (Fin-oo-can)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-IE</language>
    <lastBuildDate>Tue, 16 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://that.guru/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UEFI Support in Libvirt</title>
      <link>https://that.guru/blog/uefi-secure-boot-in-libvirt/</link>
      <pubDate>Tue, 16 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/uefi-secure-boot-in-libvirt/</guid>
      <description>&lt;p&gt;Support for UEFI Secure Boot is &lt;a href=&#34;https://specs.openstack.org/openstack/nova-specs/specs/wallaby/approved/allow-secure-boot-for-qemu-kvm-guests.html&#34;&gt;one of the features planned&lt;/a&gt; for the
Wallaby release of the OpenStack Compute project, Nova.  Nova has supported
UEFI for instances via the libvirt virt driver since the &lt;a href=&#34;https://github.com/openstack/nova/commit/9e2dfb61ed1c8f8c891c34ca4da2b46b69abd661&#34;&gt;Mitaka release (nova
13.0.0)&lt;/a&gt; and it is in fact required to boot &lt;a href=&#34;https://github.com/openstack/nova/commit/6f54f5c1e37a42b395ca793f869b73aa902602ed&#34;&gt;AArch64 (ARM64) guests&lt;/a&gt;,
however, how this has been implemented leaves a lot to be desired. The
introduction of Secure Boot functionality has given us the opportunity to clean
up some of the tech debt around this feature.&lt;/p&gt;
&lt;h2 id=&#34;uefi-support-in-qemu-and-libvirt&#34;&gt;UEFI support in QEMU and libvirt&lt;/h2&gt;
&lt;p&gt;Naturally, for nova&amp;rsquo;s libvirt virt driver to support UEFI, both libvirt and
QEMU need to support it. This has been the case for many years, but recent
versions of libvirt and QEMU have made working with UEFI significantly easier
than previously. As noted in the [Secure Boot spec][0], &lt;a href=&#34;https://www.libvirt.org/news.html#v5-3-0-2019-05-04&#34;&gt;libvirt 5.3&lt;/a&gt;
introduced support for the firmware auto-selection functionality provided by
QEMU since QEMU 2.9. This QEMU feature relies on firmware JSON files that
describe what each firmware file is for and how it can be described, as
described in the &lt;a href=&#34;https://gitlab.com/qemu-project/qemu/-/blob/v5.2.0/docs/interop/firmware.json&#34;&gt;QEMU spec&lt;/a&gt;. These files are typically provided by your
distro and on a Fedora 33 host can be found at &lt;code&gt;/usr/share/qemu/firmware&lt;/code&gt;.
Here&amp;rsquo;s one such file, &lt;code&gt;40-edk2-ovmf-x64-sb-enrolled.json&lt;/code&gt;, taken from my
host and provided by Fedora&amp;rsquo;s &lt;code&gt;edk2-ovmf&lt;/code&gt; package:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;description&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;OVMF for x86_64, with SB+SMM, SB enabled, MS certs enrolled&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;interface-types&amp;#34;&lt;/span&gt;: [
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;uefi&amp;#34;&lt;/span&gt;
    ],
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;mapping&amp;#34;&lt;/span&gt;: {
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;device&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;flash&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;executable&amp;#34;&lt;/span&gt;: {
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;filename&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/share/edk2/ovmf/OVMF_CODE.secboot.fd&amp;#34;&lt;/span&gt;,
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;format&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;raw&amp;#34;&lt;/span&gt;
        },
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;nvram-template&amp;#34;&lt;/span&gt;: {
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;filename&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/share/edk2/ovmf/OVMF_VARS.secboot.fd&amp;#34;&lt;/span&gt;,
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;format&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;raw&amp;#34;&lt;/span&gt;
        }
    },
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;targets&amp;#34;&lt;/span&gt;: [
        {
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;architecture&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x86_64&amp;#34;&lt;/span&gt;,
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;machines&amp;#34;&lt;/span&gt;: [
                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pc-q35-*&amp;#34;&lt;/span&gt;
            ]
        }
    ],
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;features&amp;#34;&lt;/span&gt;: [
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;acpi-s3&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;amd-sev&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;enrolled-keys&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;requires-smm&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;secure-boot&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;verbose-dynamic&amp;#34;&lt;/span&gt;
    ],
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;tags&amp;#34;&lt;/span&gt;: [

    ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since libvirt 5.3, libvirt has parsed these files and included them in the
domain capabilities output, accessible via &lt;code&gt;virsh domcapabilities&lt;/code&gt; and
equivalent APIs. For example, I can see what firmwares are available to me when
using the &lt;code&gt;q35&lt;/code&gt; machine type like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ virsh domcapabilities --machine pc-q35-5.1 | xmllint --xpath &#39;/domainCapabilities/os&#39; -
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will yield:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;supported=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yes&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;enum&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;firmware&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;efi&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/enum&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;supported=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yes&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/usr/share/edk2/ovmf/OVMF_CODE.secboot.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/usr/share/edk2/ovmf/OVMF_CODE.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;enum&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;rom&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;pflash&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/enum&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;enum&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;readonly&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;yes&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;no&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/enum&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;enum&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;secure&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;yes&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;value&amp;gt;&lt;/span&gt;no&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/enum&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/loader&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;However, this isn&amp;rsquo;t all this can do. Libvirt can also negotiate the firmware
for you when creating a new guest. Previously, to create a UEFI-based guest,
one would need to specify something like the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;domain&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kvm&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;!-- ... --&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x86_64&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;machine=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pc-q35-5.1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;hvm&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/type&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;readonly=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yes&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;secure=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;no&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pflash&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;/usr/share/edk2/ovmf/OVMF_CODE.secboot.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/loader&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;nvram&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;template=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/share/edk2/ovmf/OVMF_VARS.secboot.fd&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;/home/stephenfin/.config/libvirt/qemu/nvram/q35-uefi-experiment_VARS.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/nvram&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;boot&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hda&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;!-- ... --&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/domain&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I won&amp;rsquo;t go into the specifics of what each file means - libvirt has some &lt;a href=&#34;https://libvirt.org/formatdomain.html#bios-bootloader&#34;&gt;good
documentation on the matter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is no longer necessary, and libvirt will now do this for us. We can
instead specify e.g.:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;domain&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kvm&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;!-- ... --&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;firmware=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;efi&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x86_64&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;machine=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pc-q35-5.1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;hvm&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/type&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;secure=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;no&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;boot&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hda&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;!-- ... --&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/domain&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;How helpful! Using this, it&amp;rsquo;s now easier than ever to create guests using UEFI.&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;The above is all well and good, but a worked example is even better. Let&amp;rsquo;s
create a &lt;em&gt;Fedora 33 Workstation&lt;/em&gt; guest with UEFI enabled to demonstrate this.
This should work with any OS (&lt;strong&gt;note:&lt;/strong&gt; though maybe not right now, as
discussed later) but using the Fedora 33 Workstation live image, without any
other drives attached, let&amp;rsquo;s us minimise the amount of irrelevant XML present.
You can download the Fedora 33 Workstation image from the &lt;a href=&#34;https://getfedora.org/en/workstation/download/&#34;&gt;Fedora website&lt;/a&gt;,
but before we do that, let&amp;rsquo;s start by ensuring that we have all required
packages installed.  This should be as simple as installing libvirt, at least
on a Fedora 33-based host:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ sudo dnf install libvirt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It should also be noted that this guide was only tested on an x86_64 host and
results may vary on other platforms.&lt;/p&gt;
&lt;p&gt;Next up, let&amp;rsquo;s grab the image. As discussed above, we&amp;rsquo;re going to use the
&lt;em&gt;Fedora 33 Workstation&lt;/em&gt; image here since it&amp;rsquo;s easy to use for testing purposes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ cd /tmp
$ wget https://download.fedoraproject.org/pub/fedora/linux/releases/33/Workstation/x86_64/iso/Fedora-Workstation-Live-x86_64-33-1.2.iso
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the dependencies installed and the image downloaded, we can now create our
guest or &amp;ldquo;domain&amp;rdquo;. Dump the following to e.g. &lt;code&gt;/tmp/fedora-q35-uefi.xml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;domain&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kvm&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fedora-q35-uefi-experiment&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;metadata&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;libosinfo:libosinfo&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xmlns:libosinfo=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://libosinfo.org/xmlns/libvirt/domain/1.0&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;libosinfo:os&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;id=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://fedoraproject.org/fedora/33&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/libosinfo:libosinfo&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/metadata&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;memory&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;unit=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KiB&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;4194304&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/memory&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;currentMemory&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;unit=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KiB&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;4194304&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/currentMemory&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;vcpu&amp;gt;&lt;/span&gt;2&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/vcpu&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;firmware=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;efi&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x86_64&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;machine=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pc-q35-5.1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;hvm&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/type&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;secure=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;no&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;boot&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cdrom&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;features&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;acpi/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;apic/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;vmport&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;state=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;off&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/features&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;cpu&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mode=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;host-model&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;check=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;partial&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;clock&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;offset=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;utc&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;timer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rtc&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tickpolicy=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catchup&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;timer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pit&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tickpolicy=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;delay&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;timer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hpet&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;present=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;no&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/clock&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;devices&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;emulator&amp;gt;&lt;/span&gt;/usr/bin/qemu-system-x86_64&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/emulator&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;disk&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;file&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;device=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cdrom&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;driver&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;qemu&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;raw&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;source&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/tmp/Fedora-Workstation-Live-x86_64-33-1.2.iso&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;target&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sda&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bus=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sata&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;readonly/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/disk&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;serial&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pty&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;target&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;isa-serial&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;port=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;0&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;model&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;isa-serial&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/target&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/serial&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;console&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pty&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;target&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;serial&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;port=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;0&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/console&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;graphics&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vnc&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;port=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tlsPort=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;autoport=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yes&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;image&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compression=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;off&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/graphics&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;video&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;model&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;virtio&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/video&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/devices&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/domain&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, create the instance:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ virsh create fedora-q35-uefi.xml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Since we&amp;rsquo;re using the workstation image, you&amp;rsquo;re going to need a graphical
console. As a result, you&amp;rsquo;ll note that we&amp;rsquo;ve attached a VNC graphics device to
the instance and you can use VNC to connect to the guest. The easiest way to do
this is via &lt;code&gt;virt-manager&lt;/code&gt; (use the &lt;strong&gt;QEMU/KVM User Session&lt;/strong&gt; connection),
but of course you could also use &lt;code&gt;virsh dumpxml fedora-q35-uefi-experiment&lt;/code&gt;
after creating the guest to view the final XML and get the port that libvirt
assigned to the VNC interface, and then use this to connect to the instance
from your favourite VNC viewer.&lt;/p&gt;
&lt;p&gt;Once you&amp;rsquo;re done, destroy the guest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ virsh destroy fedora-q35-uefi-experiment
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;secure-boot-woes&#34;&gt;Secure boot woes&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;d be remiss if I didn&amp;rsquo;t highlight one issue with this approach, at least
using the versions of libvirt (6.6.0) and QEMU (5.1.0) installed on my host at
the time of writing. My initial attempts at this didn&amp;rsquo;t use Fedora but rather
&lt;em&gt;Alpine Linux&lt;/em&gt;, a distro many will be familiar with from Docker containers. In
theory this should be a good candidate for playing around with since it&amp;rsquo;s small
and well suited to use in guests with limited CPU and memory. However, the
&lt;a href=&#34;https://wiki.alpinelinux.org/wiki/Alpine_and_UEFI#What.27s_this_infamous_.22Secure_Boot.22.3F&#34;&gt;Alpine Linux images are not signed&lt;/a&gt; and this prevents booting of the guest.
To demonstrate the issue and explain why it occurs, it&amp;rsquo;s probably best to go
with another example.&lt;/p&gt;
&lt;p&gt;To work through this, let&amp;rsquo;s first grab the &lt;em&gt;Alpine Linux Virtual&lt;/em&gt; image,
available from the &lt;a href=&#34;https://alpinelinux.org/downloads/&#34;&gt;Alpine Linux website&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ wget https://dl-cdn.alpinelinux.org/alpine/v3.13/releases/x86_64/alpine-virt-3.13.1-x86_64.iso
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The domain XML we&amp;rsquo;ll use for booting this image is virtually identical to the
one used for Fedora previously. We could of course use the same specs as the
Fedora 33-based guest and it would work just fine, but we&amp;rsquo;ve chosen to remove
the VNC graphics device and display adapter, which are unnecessary for a distro
like this, as well as reduce the RAM allocation. Dump the following XML to e.g.
&lt;code&gt;/tmp/alpine-q35-uefi.xml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;domain&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kvm&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;name&amp;gt;&lt;/span&gt;alpinelinux-q35-uefi-experiment&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;metadata&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;libosinfo:libosinfo&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xmlns:libosinfo=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://libosinfo.org/xmlns/libvirt/domain/1.0&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;libosinfo:os&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;id=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://alpinelinux.org/alpinelinux/3.13&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/libosinfo:libosinfo&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/metadata&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;memory&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;unit=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KiB&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;1048576&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/memory&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;currentMemory&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;unit=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KiB&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;1048576&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/currentMemory&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;vcpu&amp;gt;&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/vcpu&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;firmware=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;efi&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;x86_64&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;machine=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pc-q35-5.1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;hvm&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/type&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;secure=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;no&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;boot&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cdrom&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;features&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;acpi/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;apic/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;vmport&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;state=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;off&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/features&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;cpu&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mode=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;host-model&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;check=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;partial&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;clock&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;offset=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;utc&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;timer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rtc&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tickpolicy=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catchup&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;timer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pit&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;tickpolicy=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;delay&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;timer&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hpet&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;present=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;no&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/clock&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;devices&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;emulator&amp;gt;&lt;/span&gt;/usr/bin/qemu-system-x86_64&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/emulator&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;disk&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;file&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;device=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cdrom&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;driver&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;qemu&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;raw&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;source&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;file=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/tmp/alpine-virt-3.13.1-x86_64.iso&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;target&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sda&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;bus=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sata&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;readonly/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/disk&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;serial&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pty&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;target&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;isa-serial&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;port=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;0&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;model&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;isa-serial&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/target&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/serial&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;console&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pty&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;target&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;serial&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;port=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;0&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/console&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/devices&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/domain&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the image downloaded and domain XML file created, we can create the guest:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ virsh create alpine-q35-uefi.xml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Since this isn&amp;rsquo;t using a graphical device, you can instead connect via the
serial console:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ virsh console alpinelinux-q35-uefi-experiment
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;However, upon connecting you&amp;rsquo;ll note that you eventually end up at the
bootloader rather than a login prompt and if you connect early enough, you&amp;rsquo;ll
probably see something like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;BdsDxe: loading Boot0001 &amp;quot;UEFI QEMU DVD-ROM QM00001 &amp;quot; from PciRoot(0x0)/Pci(0x1F,0x2)/Sata(0x0,0xFFFF,0x0)
BdsDxe: failed to load Boot0001 &amp;quot;UEFI QEMU DVD-ROM QM00001 &amp;quot; from PciRoot(0x0)/Pci(0x1F,0x2)/Sata(0x0,0xFFFF,0x0): Access Denied
BdsDxe: No bootable option or device was found.
BdsDxe: Press any key to enter the Boot Manager Menu.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(You can use &lt;code&gt;Ctrl + ]&lt;/code&gt; to quit the console).&lt;/p&gt;
&lt;p&gt;The apparent root cause for this can be seen by investigating the domain XML:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ virsh dumpxml alpinelinux-q35-uefi-experiment | xmllint --xpath &#39;//os&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x86_64&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;machine=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pc-q35-5.1&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;hvm&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/type&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;readonly=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;yes&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;secure=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;no&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pflash&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;/usr/share/edk2/ovmf/OVMF_CODE.secboot.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/loader&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;nvram&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;template=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/usr/share/edk2/ovmf/OVMF_VARS.secboot.fd&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;/home/stephenfin/.config/libvirt/qemu/nvram/alpinelinux-q35-uefi-experiment_VARS.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/nvram&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;boot&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cdrom&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You&amp;rsquo;ll note that we requested &lt;code&gt;secure=&#39;no&#39;&lt;/code&gt; and libvirt has included this in
the final XML, however, UEFI firmware with secure boot support,
&lt;code&gt;OVMF_CODE.secboot.fd&lt;/code&gt;, has been used, rather than &lt;code&gt;OVMF_CODE.fd&lt;/code&gt;. In theory,
this firmware should work for non-secure boot instances also but that&amp;rsquo;s not
happening. The solution for now is to specify the path to the non-secure boot
UEFI firmware when creating the instance, replacing the &lt;code&gt;&amp;lt;os&amp;gt;&lt;/code&gt; element included
in the XML above with the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;os&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;type&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x86_64&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;machine=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pc-q35-5.1&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;hvm&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/type&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;loader&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;readonly=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;yes&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;type=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;pflash&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;/usr/share/edk2/ovmf/OVMF_CODE.fd&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/loader&amp;gt;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;boot&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dev=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cdrom&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/os&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Obviously this is less than ideal. The issue has been &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=1929357&#34;&gt;reported&lt;/a&gt; and will
hopefully be resolved sooner rather than later but until then, be aware that
secure boot with &lt;strong&gt;always&lt;/strong&gt; be enabled when using this auto-configuration of
firmware.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Plain Text Email in Evolution 3.38</title>
      <link>https://that.guru/blog/plain-text-in-evolution-3-38/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/plain-text-in-evolution-3-38/</guid>
      <description>&lt;p&gt;I recently upgraded to Fedora 33, which comes with Evolution 3.38 (previously
3.36, iirc). Following the upgrade, I noticed that quoting in plain text emails
appeared to have regressed. Previously, Evolution would wrap quotes at your
recommended width, but by selecting the email and changing the style from
&amp;ldquo;Normal&amp;rdquo; to &amp;ldquo;Preformatted&amp;rdquo;, you could avoid this text wrapping. With Evolution
3.38, this no longer works, necessitating manual line wrapping which is rather
hit and miss: I managed to send mails where the message I was replying to ended
up unquoted.&lt;/p&gt;
&lt;p&gt;I haven&amp;rsquo;t figured out the quoting issue, though &lt;a href=&#34;https://gitlab.gnome.org/GNOME/evolution/-/issues/1235&#34;&gt;issue 1235&lt;/a&gt; looks
promising, but the wrapping issue is thankfully solvable. Turns out there&amp;rsquo;s a
new (to me) &amp;ldquo;Wrap quoted text in replies&amp;rdquo; option available in the &amp;ldquo;Composer
Preferences&amp;rdquo; pane of the &amp;ldquo;Evolution Preferences&amp;rdquo; pane, which accessible via
&amp;ldquo;Edit &amp;gt; Preferences&amp;rdquo;.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/plain-text-in-evolution-3-38-1.png&#34;
         alt=&#34;The Composer Preferences Pane&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Composer Preferences&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Compare before and after:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/plain-text-in-evolution-3-38-2.png&#34;
         alt=&#34;Before (with &amp;#39;Wrap quoted text in replies&amp;#39; checked)&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Before (with &amp;#39;Wrap quoted text in replies&amp;#39; checked)&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/plain-text-in-evolution-3-38-3.png&#34;
         alt=&#34;After (with &amp;#39;Wrap quoted text in replies&amp;#39; unchecked)&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;After (with &amp;#39;Wrap quoted text in replies&amp;#39; unchecked)&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Usable email again!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Nova Database Migrations</title>
      <link>https://that.guru/blog/comparing-nova-db-migrations/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/comparing-nova-db-migrations/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://blueprints.launchpad.net/nova/+spec/compact-db-migrations-wallaby&#34;&gt;One of the goals&lt;/a&gt; for the Wallaby release of OpenStack Nova is to compact
many of the database migrations that have been slowly building up since the
Icehouse release some 6 years ago. We used to do this regularly but stopped
based on operator feedback suggesting it made upgrades harder than necessary.
However, things have changed since then and the amount of database migrations
in each release has slowed considerably. In fact, the latest release, Victoria,
contained no database migrations whatsoever. This change, coupled with the fact
that we&amp;rsquo;re still using the effectively dead &lt;a href=&#34;https://sqlalchemy-migrate.readthedocs.io/en/latest/&#34;&gt;sqlalchemy-migrate&lt;/a&gt; library
rather than something like &lt;a href=&#34;https://alembic.sqlalchemy.org/en/latest/&#34;&gt;alembic&lt;/a&gt; means we now have good reason to kick
off the compaction.&lt;/p&gt;
&lt;p&gt;Below are my notes on this exercises, which demonstrate how to use the current
migrations without using &lt;code&gt;nova-manage&lt;/code&gt; and everything it entails. This should
allow people to test the changes we&amp;rsquo;re making locally and might even help
people interested in writing their own migrations in the future.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;When compressing migrations, the expectation is that the before and after of
the migration should be identical. Nova doesn&amp;rsquo;t have any built-in tests to do
this (why would it) so we&amp;rsquo;re going to do this manually. The steps to do this
are effectively:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a new empty database.&lt;/li&gt;
&lt;li&gt;Apply migrations N to M from current &lt;code&gt;master&lt;/code&gt;, where N is the current base
migration and M is the migration you wish to compact up to.&lt;/li&gt;
&lt;li&gt;Dump the schema for the database.&lt;/li&gt;
&lt;li&gt;Drop and recreate the database, then apply the compaction patch.&lt;/li&gt;
&lt;li&gt;Apply the new base migration.&lt;/li&gt;
&lt;li&gt;Dump the schema for the database.&lt;/li&gt;
&lt;li&gt;Compare the schemas from steps 3 and 6, looking for any serious changes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps are implementation differently depending on the RDBMS in use, so
sample steps for both SQLite and MySQL are provided below. These already assume
you have your system configured for nova development and can successfully run
unit tests using &lt;code&gt;tox&lt;/code&gt;. You should also have SQLite and MySQL packages
installed. For all cases, we&amp;rsquo;re going to use a virtualenv to ensure the
required dependencies are installed so do that first:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ cd nova
$ virtualenv .venv
$ source .venv/bin/activate
$ pip install &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -c https://releases.openstack.org/constraints/upper/master &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -r requirements.txt -r test-requirements.txt
$ pip install -e .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should also navigate to the &lt;em&gt;migration repository&lt;/em&gt; in nova, to avoid having
to manually specify this for the various &lt;em&gt;sqlalchemy-migrate&lt;/em&gt; commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-base&#34; data-lang=&#34;base&#34;&gt;$ cd nova/db/sqlalchemy/migrate_repo
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;sqlite&#34;&gt;SQLite&lt;/h2&gt;
&lt;p&gt;Since SQLite databases are stored as a single file, there is no additional
setup necessary. SQLite also provides a handy tool to compare databases,
&lt;code&gt;sqldiff&lt;/code&gt;, which is packaged on Fedora 32 at least. Install that first:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ sudo dnf install sqlite-tools
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With that installed, we can get right to generating two databases that we can
then compare using this &lt;code&gt;sqldiff&lt;/code&gt; tool. It would of course be possible to dump
the schemas and compare those, but doing so means we&amp;rsquo;d have to filter out
things like simple ordering changes that SQLite seems prone to.&lt;/p&gt;
&lt;p&gt;Generating the database using &lt;em&gt;sqlalchemy-migrate&lt;/em&gt; is a two steps process. We
first need to mark our database as version controlled, which will create the
necessary &lt;em&gt;version table&lt;/em&gt;. The name of this version table is configurable in
the &lt;code&gt;migrate.cfg&lt;/code&gt; configuration file found in your migration repository and is
called &lt;code&gt;migrate_version&lt;/code&gt; in nova. The &lt;code&gt;version_control&lt;/code&gt; command will create
this table and initialize it for version N, which is the base migration you
wish to test. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python manage.py version_control --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sqlite:///nova_before.db&amp;#39;&lt;/span&gt; --version &lt;span style=&#34;color:#ae81ff&#34;&gt;215&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once this is configured, apply all migrations up to M. You can use the
&lt;code&gt;upgrade&lt;/code&gt; command for this. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python manage.py upgrade --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sqlite:///nova_before.db&amp;#39;&lt;/span&gt; --version &lt;span style=&#34;color:#ae81ff&#34;&gt;216&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have a database instance using the &lt;em&gt;before&lt;/em&gt; schema, we can apply the
code change and repeat the steps above, this time creating a &lt;code&gt;nova_after.db&lt;/code&gt;
database. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python manage.py version_control --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sqlite:///nova_after.db&amp;#39;&lt;/span&gt; --version &lt;span style=&#34;color:#ae81ff&#34;&gt;215&lt;/span&gt;
$ python manage.py upgrade --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sqlite:///nova_after.db&amp;#39;&lt;/span&gt; --version &lt;span style=&#34;color:#ae81ff&#34;&gt;216&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, with these two databases to hand, we an diff them to ensure nothing has
changed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqldiff --schema nova_before.db nova_after.db
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;mysql&#34;&gt;MySQL&lt;/h2&gt;
&lt;p&gt;The steps for MySQL are quite similar to those for SQLite, but we do need to do
some pre-work - namely, creating a suitable database and user - before we can
get to validating schemas. Do this now. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ mysql
MariaDB &lt;span style=&#34;color:#f92672&#34;&gt;[(&lt;/span&gt;none&lt;span style=&#34;color:#f92672&#34;&gt;)]&lt;/span&gt;&amp;gt; CREATE DATABASE nova_before;
MariaDB &lt;span style=&#34;color:#f92672&#34;&gt;[(&lt;/span&gt;none&lt;span style=&#34;color:#f92672&#34;&gt;)]&lt;/span&gt;&amp;gt; GRANT ALL PRIVILEGES ON nova_before.* TO &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nova&amp;#39;&lt;/span&gt;@&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; IDENTIFIED BY &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;password&amp;#39;&lt;/span&gt;;
MariaDB &lt;span style=&#34;color:#f92672&#34;&gt;[(&lt;/span&gt;none&lt;span style=&#34;color:#f92672&#34;&gt;)]&lt;/span&gt;&amp;gt; quit;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this created, the steps are similar to those for SQLite. Once again, we
will use the &lt;code&gt;version_control&lt;/code&gt; and &lt;code&gt;upgrade&lt;/code&gt; management commands:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python manage.py version_control &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mysql+pymysql://nova:password@localhost/nova_before&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --version &lt;span style=&#34;color:#ae81ff&#34;&gt;215&lt;/span&gt;
$ python manage.py upgrade &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mysql+pymysql://nova:password@localhost/nova_before&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --version &lt;span style=&#34;color:#ae81ff&#34;&gt;216&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;However, unlike SQLite, we have a specific tool available to dump the DB
schemas - &lt;code&gt;mysqldump&lt;/code&gt;. Use that to dump the &lt;em&gt;before&lt;/em&gt; schema:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ mysqldump --no-data --skip-comments -u nova -ppassword nova_before &amp;gt; nova_before.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;Fedora previously provided a &lt;code&gt;mysql-utilities&lt;/code&gt; package that provided a
&lt;code&gt;mysqldiff&lt;/code&gt; utility for doing this exact thing. However, it was retired in
Fedora 31 as it was no longer maintained.
See &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=1691185&#34;&gt;Bugzilla&lt;/a&gt; for more
details.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;Once we have a database instance using the &lt;em&gt;before&lt;/em&gt; schema, we can apply the
code change and repeat the steps above, this time creating a &lt;code&gt;nova_after&lt;/code&gt;
database. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ mysql
MariaDB &lt;span style=&#34;color:#f92672&#34;&gt;[(&lt;/span&gt;none&lt;span style=&#34;color:#f92672&#34;&gt;)]&lt;/span&gt;&amp;gt; CREATE DATABASE nova_after;
MariaDB &lt;span style=&#34;color:#f92672&#34;&gt;[(&lt;/span&gt;none&lt;span style=&#34;color:#f92672&#34;&gt;)]&lt;/span&gt;&amp;gt; GRANT ALL PRIVILEGES ON nova_after.* TO &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nova&amp;#39;&lt;/span&gt;@&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt; IDENTIFIED BY &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;password&amp;#39;&lt;/span&gt;;
MariaDB &lt;span style=&#34;color:#f92672&#34;&gt;[(&lt;/span&gt;none&lt;span style=&#34;color:#f92672&#34;&gt;)]&lt;/span&gt;&amp;gt; quit;
$ python manage.py version_control --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mysql+pymysql://nova:password@localhost/nova_after&amp;#39;&lt;/span&gt; --version &lt;span style=&#34;color:#ae81ff&#34;&gt;215&lt;/span&gt;
$ python manage.py upgrade --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mysql+pymysql://nova:password@localhost/nova_after&amp;#39;&lt;/span&gt; --version &lt;span style=&#34;color:#ae81ff&#34;&gt;216&lt;/span&gt;
$ mysqldump --no-data --skip-comments -u nova -ppassword nova_after &amp;gt; nova_after.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, diff the two schemas to identify any differences:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ diff nova_before.sql nova_after.sql
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;postgresql&#34;&gt;PostgreSQL&lt;/h2&gt;
&lt;p&gt;PostgreSQL exists in a funny space in nova, where it&amp;rsquo;s technically supported
but not many people use it. Nonetheless, there are PostgreSQL-specific
migrations in-tree so we must test them.&lt;/p&gt;
&lt;p&gt;Once again, this is quite similar to the above, though Postgres&#39; user model
makes things a little more complicated. You need to create a user to run the
operations as, but you can&amp;rsquo;t simply create this as Postgres defaults to the
&lt;code&gt;ident&lt;/code&gt; auth (local UNIX user) scheme rather than &lt;code&gt;password&lt;/code&gt; (or, more
specifically, &lt;code&gt;md5&lt;/code&gt;). You need to &lt;a href=&#34;https://ubuntu.com/server/docs/databases-postgresql&#34;&gt;configure &lt;code&gt;pg_hba.conf&lt;/code&gt; to enable DB-specific
users&lt;/a&gt; and create e.g. a &lt;code&gt;nova&lt;/code&gt; user with a &lt;code&gt;password&lt;/code&gt; password.&lt;/p&gt;
&lt;p&gt;Once the user is created, you can create the DB using the &lt;code&gt;postgres&lt;/code&gt; tool:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ sudo -u postgres dropdb --if-exists nova_before
$ sudo -u postgres createdb --owner&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nova nova_before
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the DB created, the steps to generate the schema dump are once again
quite similar to SQLite and MySQL. Using &lt;code&gt;version_control&lt;/code&gt; and &lt;code&gt;upgrade&lt;/code&gt;
management commands once more:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ python manage.py version_control &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;postgresql://nova:password@localhost/nova_before&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --version &lt;span style=&#34;color:#ae81ff&#34;&gt;215&lt;/span&gt;
$ python manage.py upgrade &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;postgresql://nova:password@localhost/nova_before&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --version &lt;span style=&#34;color:#ae81ff&#34;&gt;216&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As with MySQL, Postgres comes with a specific tool for dumping the DB schemas -
&lt;code&gt;pg_dump&lt;/code&gt;. Use that to dump the &lt;em&gt;before&lt;/em&gt; schema:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ pg_dump postgresql://nova:password@localhost/nova_before &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --schema-only &amp;gt; nova_before.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have a database instance using the &lt;em&gt;before&lt;/em&gt; schema, we can apply the
code change and repeat the steps above, this time creating a &lt;code&gt;nova_after&lt;/code&gt;
database. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ sudo -u postgres dropdb --if-exists nova_after
$ sudo -u postgres createdb --owner&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nova nova_after
$ python manage.py version_control &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;postgresql://nova:password@localhost/nova_after&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --version &lt;span style=&#34;color:#ae81ff&#34;&gt;215&lt;/span&gt;
$ python manage.py upgrade &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --database &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;postgresql://nova:password@localhost/nova_after&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --version &lt;span style=&#34;color:#ae81ff&#34;&gt;216&lt;/span&gt;
$ pg_dump postgresql://nova:password@localhost/nova_after &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --schema-only &amp;gt; nova_after.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, diff the two schemas to identify any differences:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ diff nova_before.sql nova_after.sql
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;The below script can be used to diff the database. It should be placed in the
root directory of the nova repo and executed from there. It relies on the
&lt;code&gt;py36&lt;/code&gt; tox virtualenv, which you probably have on your local system already (if
not, &lt;code&gt;tox -e py36&lt;/code&gt; will create it). It also uses a single database for each
backend, rather than the separate &lt;code&gt;nova_before&lt;/code&gt; and &lt;code&gt;nova_after&lt;/code&gt; databases.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/stephenfin/adc84a21f5f5ae3793f6a8ffa0b3f40f.js&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Understanding the &#39;admin_or_owner&#39; rule in nova policies</title>
      <link>https://that.guru/blog/understanding-the-admin-or-owner-policy/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/understanding-the-admin-or-owner-policy/</guid>
      <description>&lt;p&gt;The OpenStack Compute service, nova, exposes a &lt;a href=&#34;https://docs.openstack.org/nova/latest/configuration/policy.html&#34;&gt;rich policy framework&lt;/a&gt; that
provides the ability to configure what certain types of users are allowed to
do across the API. The policy defaults are stored in code, allowing us to
define, for example, who is allowed to create or delete an instance, who can
configure flavors, and so forth. However, these policies are slightly
confusing. Take the aforementioned policy defining who can delete a specific
instance. This is configured using the &lt;a href=&#34;https://github.com/openstack/nova/blob/20.0.0/nova/policies/servers.py#L219-L228&#34;&gt;&lt;code&gt;os_compute_api:servers:delete&lt;/code&gt;
policy&lt;/a&gt; and, prior to the 21.0.0 (Ussuri) release, defaulted to the
&lt;a href=&#34;https://github.com/openstack/nova/blob/20.0.0/nova/policies/servers.py#L221&#34;&gt;&lt;code&gt;admin_or_owner&lt;/code&gt;&lt;/a&gt; rule. The name of this would suggest that the only person
that could delete an instance would be an admin or the person that created the
instance, but a quick test shows this not to be the case. Consider the
following case, used on a standard DevStack-based installation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ source openrc admin
$ openstack project create --domain default test_project
$ openstack user create --project test_project --password test testuser1
$ openstack user create --project test_project --password test testuser2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This creates a new project, &lt;code&gt;test_project&lt;/code&gt;, along with two new users,
&lt;code&gt;testuser1&lt;/code&gt; and &lt;code&gt;testuser2&lt;/code&gt;. We can then use the first of these users to create
a new instance:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; openrc-testuser1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_PROJECT_DOMAIN_ID=default
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_USER_DOMAIN_ID=default
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_AUTH_TYPE=password
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_USERNAME=testuser1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_PASSWORD=test
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_PROJECT_NAME=test_project
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_TENANT_NAME=test_project
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_AUTH_URL=http://172.20.4.203/identity  # adjust accordingly
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
$ source openrc-testuser1
$ openstack server create --flavor m1.tiny --image cirros-0.5.1-x86_64-disk test-server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the instance created, let&amp;rsquo;s now attempt to delete the server as the second
user:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; openrc-testuser2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_PROJECT_DOMAIN_ID=default
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_USER_DOMAIN_ID=default
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_AUTH_TYPE=password
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_USERNAME=testuser2
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_PASSWORD=test
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_PROJECT_NAME=test_project
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_TENANT_NAME=test_project
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;export OS_AUTH_URL=http://172.20.4.203/identity  # adjust accordingly
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
$ openstack server delete test-server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This succeeds, which at first glance seems wrong but is actually correct.
Remember, the default rule is &lt;code&gt;admin_or_owner&lt;/code&gt; and the &amp;ldquo;owner&amp;rdquo; of an instance
is not the user - it&amp;rsquo;s the project that the server is created in.  Because both
&lt;code&gt;testuser1&lt;/code&gt; and &lt;code&gt;testuser2&lt;/code&gt; are members of the project that the instance was
created in, &lt;code&gt;test_project&lt;/code&gt;, it&amp;rsquo;s possible for &lt;code&gt;testuser2&lt;/code&gt; to delete the server
created by &lt;code&gt;testuser1&lt;/code&gt;. This &amp;ldquo;project as owner&amp;rdquo; design is consistent across
nova, with only a &lt;a href=&#34;https://specs.openstack.org/openstack/nova-specs/specs/newton/implemented/user-id-based-policy-enforcement.html&#34;&gt;few, limited exceptions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is confusing, and really lessens the power of policy is nova, which is why
in the 21.0.0 (Ussuri) release, we have introduced the concept of scope types
and roles via the &lt;a href=&#34;https://review.opendev.org/#/c/686058/&#34;&gt;policy defaults refresh blueprint&lt;/a&gt;. Though not yet
enabled by default, this (&lt;a href=&#34;https://review.opendev.org/#/q/topic:bp/policy-defaults-refresh&#34;&gt;huge&lt;/a&gt;) effort allows us to do things like mark a
user as having read-only permissions for things like servers or project-level
admin privileges. For example, if we wanted to say that &lt;code&gt;testuser1&lt;/code&gt; was an
admin but &lt;code&gt;testuser2&lt;/code&gt; was only allowed read-only permissions, we could do the
following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ openstack role add --project test_project --user testuser1 admin
$ openstack role add --project test_project --user testuser2 reader
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is a big change with significant impacts, but should make policy
significantly more useful. It is explained in great detail in the nova
&lt;a href=&#34;https://docs.openstack.org/nova/latest/configuration/policy-concepts.html&#34;&gt;documentation&lt;/a&gt; and &lt;a href=&#34;https://docs.openstack.org/releasenotes/nova/ussuri.html#relnotes-21-0-0-stable-ussuri&#34;&gt;release notes&lt;/a&gt;, and I recommend reading both.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emulated Trusted Platform Module (vTPM) in OpenStack </title>
      <link>https://that.guru/blog/emulated-tpm-in-openstack/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/emulated-tpm-in-openstack/</guid>
      <description>&lt;p&gt;Work is &lt;a href=&#34;https://review.opendev.org/#/q/topic:bp/add-emulated-virtual-tpm+(status:open+OR+status:merged)&#34;&gt;ongoing&lt;/a&gt; in nova to provide support for attaching virtual Trusted
Platform Modules (vTPMs) to instances. The below guide demonstrates how you can
go about testing this feature for yourself. This work was conducted on a Fedora
31 VM (with nested virt, though that&amp;rsquo;s not relevant) using DevStack master.&lt;/p&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;This is all very much work-in-progress at the moment. I aim to come back and
update this if/when things are merged.&lt;/div&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;initial-steps&#34;&gt;Initial Steps&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re going to use DevStack on Fedora 31 to test this. Fedora 31 is necessary
since Ubuntu 18.04 (Bionic) does not provide new enough versions of
&lt;a href=&#34;https://packages.ubuntu.com/bionic/libvirt-daemon&#34;&gt;libvirt&lt;/a&gt; or &lt;a href=&#34;https://packages.ubuntu.com/xenial/qemu-kvm&#34;&gt;QEMU&lt;/a&gt;, while Ubuntu 20.04 (Focal) is not supported by
DevStack at the time of writing and was affected by a &lt;a href=&#34;https://github.com/sqlalchemy/alembic/issues/699&#34;&gt;bug in barbican&lt;/a&gt;.
With a Fedora 31 installation at the ready, let&amp;rsquo;s update and get DevStack:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ sudo dnf upgrade -y
$ git clone https://opendev.org/openstack/devstack
$ cd devstack
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The DevStack installation is pretty bog standard with the key differences being
that you need to enable the OpenStack Key Manager service, &lt;a href=&#34;https://docs.openstack.org/barbican/latest/&#34;&gt;barbican&lt;/a&gt;, for
storing secrets along with the virt preview repo to get new versions of libvirt
and QEMU. Here&amp;rsquo;s a sample &lt;code&gt;local.conf&lt;/code&gt;, to be placed into the &lt;code&gt;devstack&lt;/code&gt;
directory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[[&lt;/span&gt;local|localrc&lt;span style=&#34;color:#f92672&#34;&gt;]]&lt;/span&gt;
RECLONE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True

&lt;span style=&#34;color:#75715e&#34;&gt;## Passwords&lt;/span&gt;

ADMIN_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password
DATABASE_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password
RABBIT_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password
HORIZON_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password
SERVICE_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password
SERVICE_TOKEN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;no-token-password

&lt;span style=&#34;color:#75715e&#34;&gt;## Additional plugins and configuration&lt;/span&gt;

enable_plugin barbican https://opendev.org/openstack/barbican
enable_service rabbit mysql key

ENABLE_FEDORA_VIRT_PREVIEW_REPO&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There&amp;rsquo;s also a minor tweak necessary to work around pip 10 refusing to
uninstall system-provided Python packages:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;diff --git inc/python inc/python
index dd772960..63a3dc19 100644
&lt;span style=&#34;color:#f92672&#34;&gt;--- inc/python
&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;+++ inc/python
&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;@@ -196,7 +196,7 @@ function pip_install {
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;         no_proxy=&amp;#34;${no_proxy:-}&amp;#34; \
         PIP_FIND_LINKS=$PIP_FIND_LINKS \
         SETUPTOOLS_SYS_PATH_TECHNIQUE=rewrite \
&lt;span style=&#34;color:#f92672&#34;&gt;-        $cmd_pip $upgrade \
&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;+        $cmd_pip --ignore-installed $upgrade \
&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;         $@
     result=$?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;Yes, I&amp;rsquo;m aware that there can be negative implications to this, but it&amp;rsquo;s good
enough for our purposes here. Hopefully one of us will eventually get around to
configuring DevStack to install in virtualenvs by default.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;We also need to install the &lt;code&gt;swtpm&lt;/code&gt; and &lt;code&gt;swtpm_setup&lt;/code&gt; binaries. These are
provided in the Fedora repos (yay!):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ sudo dnf install swtpm swtpm-tools
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Finally, &lt;code&gt;crudini&lt;/code&gt; makes working with INI files a breeze. Let&amp;rsquo;s install that
too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ sudo dnf install crudini
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With all this done, you can stack:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;./stack.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now go make a cup of tea. &lt;/p&gt;
&lt;h2 id=&#34;configuring-nova&#34;&gt;Configuring nova&lt;/h2&gt;
&lt;p&gt;With the stack (hopefully) complete, we need to configure nova appropriately.
This is a simple, single-node &amp;ldquo;can I boot an instance&amp;rdquo; test so we don&amp;rsquo;t need to
do too much. The key steps are to check out the correct code, given that it&amp;rsquo;s
not yet merged, add some &lt;code&gt;nova.conf&lt;/code&gt; configuration options and create a new
flavor. First up, let&amp;rsquo;s checkout the correct nova code. You can use the
checkout link from the &lt;a href=&#34;https://packages.ubuntu.com/bionic/libvirt-daemon&#34;&gt;Gerrit review&lt;/a&gt; for this purpose:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ cd /opt/stack/nova
# checkout using the &amp;quot;Checkout&amp;quot; link in the &amp;quot;Download&amp;quot; dropdown on the review,
# which I&#39;m not providing here since it won&#39;t age well
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, let&amp;rsquo;s modify the &lt;code&gt;nova.conf&lt;/code&gt; file the &lt;code&gt;nova-compute&lt;/code&gt; service. We&amp;rsquo;ll use
&lt;code&gt;crudini&lt;/code&gt; for this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ crudini --set /etc/nova/nova-cpu.conf libvirt swtpm_enabled True
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let&amp;rsquo;s restart the various nova services to load both the correct code and the
configuration changes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ sudo systemctl restart devstack@n-*
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With everything restarted, we should be able to see the relevant
&lt;code&gt;COMPUTE_SECURITY_TPM_*&lt;/code&gt; traits reported by our sole compute node:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack --os-placement-api-version 1.20 resource provider trait list \
    --format value $RP_UUID | grep TPM
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(where &lt;code&gt;$RP_UUID&lt;/code&gt; is the UUID of the resource provider of the compute node,
which can be found via &lt;code&gt;openstack resource provider list&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;This should return the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COMPUTE_SECURITY_TPM_1_2
COMPUTE_SECURITY_TPM_2_0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With that done, let&amp;rsquo;s create a suitable flavor. The key feature here is the
configuration of the &lt;code&gt;hw:tpm_version&lt;/code&gt; and &lt;code&gt;hw:tpm_model&lt;/code&gt; extra specs.
&lt;code&gt;hw:tpm_model&lt;/code&gt; is optional but &lt;code&gt;hw:tpm_version&lt;/code&gt; is required to enable the
feature:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack flavor create test.vtpm \
    --ram 512 --disk 1 --vcpus 1 --wait \
    --property hw:tpm_version=1.2 \
    --property hw:tpm_model=tpm-tis
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;create-an-instance-with-vtpm&#34;&gt;Create an instance with vTPM&lt;/h2&gt;
&lt;p&gt;With configuration complete, we can finally proceed to creating an instance.
Nothing to funky here: simply create an instance using the flavor we created
previously.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack --os-compute-api-version 2.latest server create test.server \
    --image cirros-0.5.1-x86_64-disk --flavor test.vtpm \
    --nic none --wait test.server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once booted, let&amp;rsquo;s check if the generated XML includes the &lt;a href=&#34;https://libvirt.org/formatdomain.html#elementsTpm&#34;&gt;&lt;code&gt;&amp;lt;tpm&amp;gt;&lt;/code&gt;&lt;/a&gt; device
as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ sudo virsh list
 Id   Name                State
-----------------------------------
 10   instance-0000000d   running

$ sudo virsh dumpxml instance-0000000d | xmllint --xpath &#39;/domain/devices/tpm&#39; -
&amp;lt;tpm model=&amp;quot;tpm-tis&amp;quot;&amp;gt;
      &amp;lt;backend type=&amp;quot;emulator&amp;quot; version=&amp;quot;1.2&amp;quot;&amp;gt;
        &amp;lt;encryption secret=&amp;quot;8cc4e70c-d805-4d48-9302-e3c970d1217b&amp;quot;/&amp;gt;
      &amp;lt;/backend&amp;gt;
      &amp;lt;alias name=&amp;quot;tpm0&amp;quot;/&amp;gt;
    &amp;lt;/tpm&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can also query barbican to see if nova correctly stored keys as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack secret list --format yaml
- Algorithm: null
  Bit length: null
  Content types:
    default: application/octet-stream
  Created: &#39;2020-05-28T15:41:05+00:00&#39;
  Expiration: null
  Mode: null
  Name: vTPM secret for instance bbe8bc62-8403-490b-bce3-bd9c8267147e
  Secret href: http://172.20.4.203/key-manager/v1/secrets/8cc4e70c-d805-4d48-9302-e3c970d1217b
  Secret type: passphrase
  Status: ACTIVE
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can also validate that the rebuild operation works as expected:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack server rebuild --wait test.server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Finally, we can ensure that things are properly cleaned up once we&amp;rsquo;re finished:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack server delete test.server
$ openstack secret list --format yaml
[]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;QED. &lt;/p&gt;
&lt;h2 id=&#34;further-reading&#34;&gt;Further reading&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://review.opendev.org/#/c/728505/&#34;&gt;spec&lt;/a&gt; and &lt;a href=&#34;https://review.opendev.org/#/q/topic:bp/add-emulated-virtual-tpm+(status:open+OR+status:merged)&#34;&gt;WIP code&lt;/a&gt; are well worth a read and contain background
information on most of the topics discussed here.&lt;/p&gt;
&lt;h2 id=&#34;package-versions&#34;&gt;Package versions&lt;/h2&gt;
&lt;p&gt;The following software versions were used. In summary, these correspond to the
master versions of various OpenStack projects and latest Fedora package
versions on the day of test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distro: Fedora 31 (with virt preview repo via DevStack)&lt;/li&gt;
&lt;li&gt;Kernel: &lt;code&gt;5.3.7-301.fc31.x86_64&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Libvirt: 6.3.0&lt;/li&gt;
&lt;li&gt;QEMU: 5.0.0&lt;/li&gt;
&lt;li&gt;DevStack: &lt;code&gt;9a6ae3419c6412a55456aa87b7a790c255f01028&lt;/code&gt; (master)&lt;/li&gt;
&lt;li&gt;Nova: &lt;code&gt;de42f9e983cb4d4e94977697f86abf0a05e61cb4&lt;/code&gt; (master) (before checking
out in-progress vTPM changes)&lt;/li&gt;
&lt;li&gt;Barbican: &lt;code&gt;1ad43597352b225b6f3a21ef6c4186330cadf660&lt;/code&gt; (master)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Using AMI Images in OpenStack</title>
      <link>https://that.guru/blog/upload-cirros-ami-images/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/upload-cirros-ami-images/</guid>
      <description>&lt;p&gt;I recently had to validate some interactions between the OpenStack Image
service, &lt;a href=&#34;https://docs.openstack.org/glance/latest/&#34;&gt;glance&lt;/a&gt;, and the Compute service, &lt;a href=&#34;https://docs.openstack.org/nova/latest/&#34;&gt;nova&lt;/a&gt;. For this, I needed
separate kernel and ramdisk images. Glance supports a variety of image formats,
which is required since different virtualization backends support different
image formats. For quite some time, the &lt;a href=&#34;https://docs.openstack.org/devstack/latest/&#34;&gt;DevStack&lt;/a&gt; installer defaulted to
using AMI images, so if you&amp;rsquo;d run &lt;code&gt;openstack image list&lt;/code&gt; on a fresh
DevStack-based deployment, you&amp;rsquo;d have seen three &lt;a href=&#34;https://download.cirros-cloud.net/&#34;&gt;CirrOS&lt;/a&gt; &amp;ldquo;images&amp;rdquo; with
differing suffixes: &lt;code&gt;-uec&lt;/code&gt;, &lt;code&gt;-uec-ramdisk&lt;/code&gt;, and &lt;code&gt;-uec-kernel&lt;/code&gt;. This has since
&lt;a href=&#34;https://github.com/openstack/devstack/commit/6fc332d85279865c32f50b081efb25ba7b671a9a&#34;&gt;changed&lt;/a&gt;, but there&amp;rsquo;s no reason we can&amp;rsquo;t create these types of image still.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s get the image. We&amp;rsquo;re going to want the Ubuntu Enterprise Cloud
(UEC) CirrOS images:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ wget https://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-uec.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Extract this tarball, which will yield three files: a &lt;code&gt;vmlinuz&lt;/code&gt; kernel image,
a &lt;code&gt;initrd&lt;/code&gt; ramdisk image, and a empty mkfs&amp;rsquo;d &lt;code&gt;blank&lt;/code&gt; image:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ tar -xvzf cirros-0.5.1-x86_64-uec.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With those created, we can now create the three images in glance. First, the
kernel image:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack image create cirros-0.5.1-x86_64-uec-kernel \
    --public --disk-format aki --container-format aki \
    --file cirros-0.5.1-x86_64-vmlinuz
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then the ramdisk image:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack image create cirros-0.5.1-x86_64-uec-ramdisk \
    --public --disk-format ari --container-format ari \
    --file cirros-0.5.1-x86_64-initrd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And finally the &amp;ldquo;machine&amp;rdquo; image, which requires references to the kernel and
ramdisk image by way of image metadata properties:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack image create cirros-0.5.1-x86_64-uec \
    --property ramdisk_id=52ab2881-3f0e-4d0b-8824-d6c144eb872a \
    --property kernel_id=b582cf17-1785-4915-9b89-dc31c1794757 \
    --public --disk-format ami --container-format ami \
    --file cirros-0.5.1-x86_64-blank.img
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once done, you should be able to boot an instance using the machine image:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ openstack server create cirros-server \
    --flavor m1.tiny --image cirros-0.5.1-x86_64-uec
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Why You Can&#39;t Schedule to Host NUMA Nodes in Nova?</title>
      <link>https://that.guru/blog/the-numa-scheduling-story-in-nova/</link>
      <pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/the-numa-scheduling-story-in-nova/</guid>
      <description>&lt;p&gt;If I had a euro for every time someone had asked me or someone else working on
nova for the ability to schedule an instance to a specific host NUMA node, I
might never have to leave the pub (back in halcyon days pre-COVID-19 when pubs
were still a thing, that is).&lt;/p&gt;
&lt;p&gt;Below is an edited version of a response one of my friends and colleagues,
&lt;a href=&#34;https://github.com/SeanMooney&#34;&gt;Sean Mooney&lt;/a&gt;, provided to a Red Hat partner asking just this question
recently. This information is accurate as of the OpenStack 21.0.0 (Ussuri)
release but is subject to change in future releases.&lt;/p&gt;
&lt;h2 id=&#34;whats-wrong-with-choosing-a-host-numa-node&#34;&gt;What&amp;rsquo;s wrong with choosing a host NUMA node?&lt;/h2&gt;
&lt;p&gt;In almost all cases when we discuss the motivation for this request with
people, we discover that selecting host CPUs or NUMA nodes via the flavor is
not actually what they wanted to do. Rather, it is seen as a means to an end
recommended by people familiar with virtualisation technologies but not with
cloud platforms. There are a number of reasons this is not considered an
acceptable solution in a cloud context. To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is seen as a potential security concern for public clouds.&lt;/p&gt;
&lt;p&gt;To correctly understand which flavor to use when flavors can map to host
resources like CPUs or NUMA nodes would require knowledge of the underlying
hardware. This information can be used by a malicious user as a DDOS vector
as they could intentionally place their instance on the same NUMA node
(opening the opportunity to exhaust memory bandwidth of a NUMA node) and
host as their victim. It also exposes information about what hardware a
cloud is running indirectly that many clouds would prefer not to share.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is a violation of the cloud abstraction.&lt;/p&gt;
&lt;p&gt;OpenStack is a cloud platform intended to provide a consistent API across
multiple backend implementations of services. In fact, cloud abstraction is
a key element of the &lt;a href=&#34;https://docs.openstack.org/nova/latest/contributor/project-scope.html#driver-parity&#34;&gt;nova project scope&lt;/a&gt;. Virtual NUMA topologies are
supported by two main drivers today, Libvirt and HyperV, and while the
Libvirt driver and Libvirt in general are much more flexible than other
virt drivers in many aspects, nova does strive to keep the differences to a
minimum. If we were to encode the semantics of how virtual resources map to
physical resources, we would exclude the possibility of achieving
interoperability between different drivers or different clouds as well as
create barriers to adopting features across multiple drivers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It makes our clouds much smaller and less useful.&lt;/p&gt;
&lt;p&gt;Encoding host specific resource assignment information severely limits the
available hosts that can be used to create an instance. It complicates move
operations and makes the maintainability and extensibility of the scheduler
harder over time. The operational overhead of having to create different
flavors for a VM that runs on node 5 vs node 6 and the additional cognitive
load that this puts on the end user is a failure in API design. Instead of
expressing an abstract policy, the end user and operator needs to
understand the intricate mechanics of how the workload compute context is
created.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are additional reasons to those listed above, but these alone should
serve to illustrate that this is not oversight in how nova currently functions
but rather a very deliberate design choice that we do not want to remove.&lt;/p&gt;
&lt;h2 id=&#34;but-we-really-need-this-so-why-cant-you-special-case-it-for-us&#34;&gt;But we really need this, so why can&amp;rsquo;t you special case it for us?&lt;/h2&gt;
&lt;p&gt;If you were willing to ignore all of the above, and the many other reasons for
not doing this, you might tempted to think that you could maybe hack this in
anyway. After all, extra specs have long been one of the untamed corners of
nova (at least, they were until &lt;a href=&#34;https://specs.openstack.org/openstack/nova-specs/specs/ussuri/implemented/flavor-extra-spec-validators.html&#34;&gt;Ussuri&lt;/a&gt;). Not so fast. Long-term, it&amp;rsquo;s
unlikely that this will even be an option for reasons to do with how we&amp;rsquo;ve
handled scheduling in nova in the past and how we&amp;rsquo;re planning to evolve it in
the future.&lt;/p&gt;
&lt;p&gt;As you may or may not be aware, there has been a multi-year effort to evolve
the tracking of resources in OpenStack. This effort predates the creation of
the placement service but was the primary reason for its existence. Nova has a
multi-layer scheduling approach, the first step of which is delegated to the
placement service, followed by filtering and then weighing. Following
scheduling (that is, the act of selecting a host) there is then a resource
assignment phase that is performed by the virt driver.&lt;/p&gt;
&lt;p&gt;While the scheduling and resource assignment steps are heavily linked, they are
&lt;strong&gt;independent&lt;/strong&gt; operations. To put this in concrete terms, while the NUMA
topology filter has input into selecting a host, by determining whether a given
NUMA topology can be created on a given host, it has no input into selecting
which NUMA nodes on the host will be used for the VM. That decision is made
entirely by the virt driver during the assignment phase. The three phases of
nova scheduling today are summarised as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The placement service is queried for a set of allocation candidates
(potential hosts your instance can be scheduled to) that represent resource
allocation on hosts that can fulfil the quantitative and qualitative
requirement of the resources requested for an instance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The hosts represented by those allocation candidates are filtered based
primarily on non-resource related attributes such as server group affinity
or anti-affinity constraints and resources and topologies that are not yet
modeled in placement, such as PCI devices and hugepages and various NUMA
affinity metrics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The filtered hosts are weighed to select an optimal host.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Up until now, placement, which has a global hierarchical view of resources from
multiple services, has largely operated on the basis of tracking a simple tally
count of resources on a given host. When it was originally created, placement
modeled each compute host as a single resource provider containing multiple
inventories of different resources classes. Each resource provider can only
have one instance of an inventory per resource class, so when memory is
represented as &lt;code&gt;MEMORY_MB&lt;/code&gt; it tracks the total available system memory on the
host. Placement also tracks allocation from resource providers to instances so
it can know how much of each resource is still available. It can then use this
capacity information in addition to some qualitative traits (e.g. this host
has SSD storage while this host does not) to produce a set of potential
allocation candidates.&lt;/p&gt;
&lt;p&gt;As we have evolved from this simple view, we have extended the placement API to
support nested resource providers, allowing us to convert a flat view of a
host&amp;rsquo;s resources into a tree data structure. There were many reasons for this
effort, but the chief motivator was that it allows us to begin modelling of
&lt;a href=&#34;https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/numa-topology-with-rps.html&#34;&gt;NUMA topologies in placement&lt;/a&gt;. This will allow us to track hugepages as
inventory in placement as well as consider NUMA affinity for things like vGPUs
(note: we also plan to model generic PCI devices in placement, though this
effort will likely not begin until the NUMA-in-placement effort is complete).
There are two implications of this work that would prevent a &amp;ldquo;boot on NUMA node
N&amp;rdquo; feature in the future.&lt;/p&gt;
&lt;p&gt;Firstly, doing this work will allow us to remove the NUMA topology filter in
the future, increasing scheduler performance among other things but you do not
get this performance for free. In the case of placement, the trade-off is in
the freedom to select resources on the host during the assignment phase. In
order to maintain placement as the single source or truth with regard to
capacity and availability of resources, it is vitally important that, when a
resource class on a host exists on multiple separate providers, the virt driver
will &lt;strong&gt;only&lt;/strong&gt; allocate resources from the hardware corresponding to the
resource provider chosen during scheduling. For example, memory tracked per
NUMA node or vGPUs tracked per physical GPU must be assigned from the NUMA node
or pGPU that the allocation is made against in placement. This means that in a
world where all resources are tracked in placement, the assignment done by a
virt driver is constrained to only looking at a subset of hardware that
correlates with the placement allocation and it will no longer be possible to,
for example, ensure that a given instance is pinned to CPUs from a specific
NUMA node.&lt;/p&gt;
&lt;p&gt;Secondly, it&amp;rsquo;s important to realize that the resource topology reported to
placement will be determined by the virt driver that reports them.  For
example, ironic hosts are represented using a single custom resource class
rather than reporting &lt;code&gt;VCPU&lt;/code&gt;, &lt;code&gt;MEMORY_MB&lt;/code&gt; and &lt;code&gt;DISK_GB&lt;/code&gt; inventories in
order to model their consumption as a single unit. This means that at the time
of making the placement query, since we have not yet selected a host we also
have not selected a compute context (hypervisor, baremetal/composable server or
container runtime) and as such cannot make assumptions about how the virt
driver that manages that host will model its resource in placement. Nova
supports using multiple compute contexts concurrently and it&amp;rsquo;s not uncommon for
operators to use image-based filters to map Windows images to HyperV hosts and
Linux images to VMWare or Libvirt hosts. Similarly, in a multi-architecture
cloud, they may offer PowerVM hosts. Since each of these compute contexts may
support assigning host resources to instances in different ways we cannot make
it an API requirement to pin the instance to NUMA node N based on a flavor
extra spec.&lt;/p&gt;
&lt;p&gt;Taken together, the continued effort to move tracking of all resource to
placement means any effort to map a given instance to a specific host NUMA node
is dead on arrival. We can achieve what&amp;rsquo;s necessary, but it can and should be
done in a better way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VCPUs, PCPUs and Placement</title>
      <link>https://that.guru/blog/cpu-resources-redux/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/cpu-resources-redux/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://that.guru/blog/cpu-resources&#34;&gt;previous blog post&lt;/a&gt;, I&amp;rsquo;d described how instance NUMA
topologies and CPU pinning worked in the OpenStack Compute service (nova).
Starting with the 20.0.0 (Train) release, things have changed somewhat. This
post serves to explain how things have changed and what impact that will have
on a typical deployment.&lt;/p&gt;
&lt;h2 id=&#34;the-pre-train-world&#34;&gt;The pre-Train world&lt;/h2&gt;
&lt;p&gt;As noted previously, a NUMA topology could be added to an instance either
explicitly, using the &lt;code&gt;hw:numa_nodes=N&lt;/code&gt; flavor extra spec, or implicitly, by
requesting a specific mempage size (&lt;code&gt;hw:mem_page_size=N&lt;/code&gt;) or CPU pinning
(&lt;code&gt;hw:cpu_policy=dedicated&lt;/code&gt;). For historical reasons, it is not possible to
request memory pages or CPU pinning without getting a NUMA topology meaning
every pinned instance or instance with hugepages (common when using something
like Open vSwitch with DPDK) has a NUMA topology associated with it.&lt;/p&gt;
&lt;p&gt;The presence of a NUMA topology implies a couple of things. The most
beneficial of them is that each instance NUMA node is mapped to a unique host
NUMA node and will only consume CPUs and RAM from that host node. The NUMA
topology of the instance is exposed to the guest OS meaning well engineered
applications running in the guest OS are able to able to tune themselves for
this topology and avoid cross-NUMA node memory accesses and the performance
penalties these brings. Unfortunately, there have also been some downsides, of
which two were rather significant. The most impactful of these was the
inability to correctly live migrate such instances, as noted in
&lt;a href=&#34;https://bugs.launchpad.net/nova/+bug/1417667&#34;&gt;bug #1417667&lt;/a&gt;. In short, that bug noted that nova was not recalculating any
of its CPU or mempage pinning information on a migration, resulting in a
failure to live migrate to hosts with different NUMA topologies or, worse,
individual instance NUMA nodes being spread across multiple NUMA nodes or the
pinned CPUs of pinned instances overlapping with those of other pinned
instances. The other issue stemmed from nova&amp;rsquo;s schizophrenic model for tracking
resource utilization, where it used different models for tracking pinned CPUs
from unpinned or &amp;ldquo;floating&amp;rdquo; CPUs, along with different models for tracking
standard memory from explicit small and huge page requests. Combined, these led
to a scenario where operators had to divide their datacenters up into &lt;a href=&#34;https://docs.openstack.org/nova/latest/admin/aggregates.html&#34;&gt;host
aggregates&lt;/a&gt; in order to separate normal, unpinned instances from both pinned
instances and unpinned instances with a NUMA topology.&lt;/p&gt;
&lt;h2 id=&#34;train-to-the-rescue&#34;&gt;Train to the rescue&lt;/h2&gt;
&lt;p&gt;Train changes things. Not only does it resolve the live migration issue through
the completion of the &lt;a href=&#34;https://specs.openstack.org/openstack/nova-specs/specs/train/approved/numa-aware-live-migration.html&#34;&gt;NUMA-aware live migration spec&lt;/a&gt; but it introduces an
&lt;a href=&#34;https://specs.openstack.org/openstack/nova-specs/specs/train/approved/cpu-resources.html&#34;&gt;entirely new model for tracking CPU resources&lt;/a&gt; that prevents the need to
shard your datacenter using host aggregates. This is made possible by the
reporting of a new resource type, &lt;code&gt;PCPU&lt;/code&gt;, for host CPUs intended to host pinned
instance CPUs. This is described in the &lt;a href=&#34;https://docs.openstack.org/releasenotes/nova/train.html#relnotes-20-0-0-stable-train&#34;&gt;Train release notes&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Compute nodes using the libvirt driver can now report &lt;code&gt;PCPU&lt;/code&gt; inventory. This
is consumed by instances with dedicated (pinned) CPUs. This can be configured
using the &lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt; config option. The scheduler will
automatically translate the legacy &lt;code&gt;hw:cpu_policy&lt;/code&gt; flavor extra spec or
&lt;code&gt;hw_cpu_policy&lt;/code&gt; image metadata property to &lt;code&gt;PCPU&lt;/code&gt; requests, falling back to
&lt;code&gt;VCPU&lt;/code&gt; requests only if no &lt;code&gt;PCPU&lt;/code&gt; candidates are found. Refer to the help
text of the &lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt;, &lt;code&gt;[compute] cpu_shared_set&lt;/code&gt; and
&lt;code&gt;vcpu_pin_set&lt;/code&gt; config options for more information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We&amp;rsquo;ll explore how this manifests itself in a bit, but before that we should
look at how one can migrate from an existing pre-Train deployment using
&lt;code&gt;[DEFAULT] vcpu_pin_set&lt;/code&gt; (or not using it, as the case may be) to these new
configuration options.&lt;/p&gt;
&lt;h2 id=&#34;migrating-to-train&#34;&gt;Migrating to Train&lt;/h2&gt;
&lt;p&gt;The migration from Stein to Train is tricky. In short, we need to migrate from
&lt;code&gt;[DEFAULT] vcpu_pin_set&lt;/code&gt; to a combination of &lt;code&gt;[compute] cpu_shared_set&lt;/code&gt; and
&lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt; and unset &lt;code&gt;[DEFAULT] reserved_host_cpus&lt;/code&gt;. How you
do this is touched upon in the help text for the &lt;a href=&#34;https://docs.openstack.org/nova/train/configuration/config.html#DEFAULT.vcpu_pin_set&#34;&gt;&lt;code&gt;[DEFAULT] vcpu_pin_set&lt;/code&gt;&lt;/a&gt;,
&lt;a href=&#34;https://docs.openstack.org/nova/train/configuration/config.html#compute.cpu_shared_set&#34;&gt;&lt;code&gt;[compute] cpu_shared_set&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://docs.openstack.org/nova/train/configuration/config.html#compute.cpu_dedicated_set&#34;&gt;&lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt;&lt;/a&gt; options.
As always, a diagram will help:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources-redux_migration.png&#34; alt=&#34;Migrating nova.conf&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once this migration is complete, restarting the &lt;code&gt;nova-compute&lt;/code&gt; service will
result in nova automatically &amp;ldquo;reshaping&amp;rdquo; the inventory for the compute node
stored in placement. Any host CPUs listed in the &lt;code&gt;[compute] cpu_shared_set&lt;/code&gt;
config option will continue to be reported as &lt;code&gt;VCPU&lt;/code&gt; inventory, but host CPUs
listed in the &lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt; config option will be reported as
&lt;code&gt;PCU&lt;/code&gt; inventory.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s look at some examples of how this would be reflected in the real world.
For all these examples, consider a host with two sockets and two CPUs with four
cores and no hyperthreading (so eight CPUs).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_host-topology.png&#34; alt=&#34;The basic host&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;hosts-with-unpinned-workloads&#34;&gt;Hosts with unpinned workloads&lt;/h3&gt;
&lt;p&gt;If your host is only intended for unpinned workloads, you don&amp;rsquo;t need to do
anything! If neither &lt;code&gt;[compute] cpu_shared_set&lt;/code&gt; nor &lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt; are configured, the former will default to all host cores.&lt;/p&gt;
&lt;p&gt;We can see this in practice by examining the placement records for the given
compute node. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 8     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 8     |
+------------------+-------+

$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 PCPU
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Optionally, we might decide to exclude a certain number of cores, perhaps
setting aside some for the host. For example, to reserve core 0 from each host
NUMA node for the host, configure the following in &lt;code&gt;nova.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[compute]
cpu_shared_set = 1-3,5-7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we now query placement again, we&amp;rsquo;ll see the number of available &lt;code&gt;VCPU&lt;/code&gt;
inventory has dropped.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 6     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 6     |
+------------------+-------+

$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 PCPU
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hosts-with-pinned-workloads&#34;&gt;Hosts with pinned workloads&lt;/h3&gt;
&lt;p&gt;Next, let&amp;rsquo;s consider a host that&amp;rsquo;s only intended for pinned workloads.
Previously, we highly recommended configuring &lt;code&gt;[DEFAULT] vcpu_pin_set&lt;/code&gt; as not
setting this could result in impacted performance for some workloads due to
contention from the host. The new &lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt; option is
mandatory because, as noted above, not configuring any option will result in
all host cores being reported as &lt;code&gt;VCPU&lt;/code&gt; inventory. Let&amp;rsquo;s once again reserve
core 0 from each host NUMA node for the host by configuring our &lt;code&gt;nova.conf&lt;/code&gt;
like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[compute]
cpu_dedicated_set = 1-3,5-7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we query placement, we&amp;rsquo;ll no longer see &lt;code&gt;VCPU&lt;/code&gt; inventory but rather &lt;code&gt;PCPU&lt;/code&gt;
inventory.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU

$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 PCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 1.0   |
| max_unit         | 6     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 6     |
+------------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;&lt;code&gt;PCPU&lt;/code&gt; inventory will always have an &lt;code&gt;allocation_ratio&lt;/code&gt; or &lt;code&gt;1.0&lt;/code&gt;. This is
because pinned CPUs cannot be oversubscribed.&lt;/div&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;hosts-with-mixed-pinned-and-unpinned-workloads&#34;&gt;Hosts with mixed pinned and unpinned workloads&lt;/h2&gt;
&lt;p&gt;Finally, let&amp;rsquo;s consider a host with both pinned and unpinned workloads. As
discussed earlier, this was not previously possible. To do this, we must simple
configure both &lt;code&gt;[compute] cpu_shared_set&lt;/code&gt; and &lt;code&gt;[compute] cpu_dedicated_set&lt;/code&gt; on
the host. Given that we have two host NUMA nodes with for cores per node, let&amp;rsquo;s
reserve two cores from each node for both pinned and unpinned workloads by
configuring our &lt;code&gt;nova.conf&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[compute]
cpu_shared_set = 0,1,4,5
cpu_dedicated_set = 2,3,6,7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we query placement, we&amp;rsquo;ll now see both &lt;code&gt;VCPU&lt;/code&gt; and &lt;code&gt;PCPU&lt;/code&gt; inventory reported
alongside each other.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 4     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 4     |
+------------------+-------+

$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 PCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 1.0   |
| max_unit         | 4     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 4     |
+------------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The ability to place both pinned and unpinned instances on the same compute
node should lead to higher resource utilization and avoid the need to shard
your compute, both of which are very useful features for smaller deployments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Upgrading DD-WRT to OpenWRT on the TP-Link TL-WR1043ND</title>
      <link>https://that.guru/blog/updating-tp-link-tl-wr1043nd/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/updating-tp-link-tl-wr1043nd/</guid>
      <description>&lt;p&gt;I had an old &lt;a href=&#34;https://www.tp-link.com/uk/home-networking/wifi-router/tl-wr1043nd/&#34;&gt;TP-Link TL-WR1043ND&lt;/a&gt; that was running DD-WRT firmware from
2013. Needless to say, this was pretty ancient and probably should be remedied
but, unfortunately, I&amp;rsquo;d been unable to figure out how to do this via the web
UI. It turns this can&amp;rsquo;t actually be done and instead you need to use a
terminal. The below are my notes from doing just this.&lt;/p&gt;
&lt;p&gt;First, you need to download the original firmware and strip the bootloader from
it. You can source these firmwares from the TP-Link site but unfortunately,
things aren&amp;rsquo;t as simple as that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you want to revert back to stock TP-link firmware from custom firmware,
most of the time you can not flash the TP-Link firmware directly from the
official TP-Link website.&lt;/p&gt;
&lt;p&gt;The reason for this is that most of the downloadable firmware from the
TP-Link website contains a so called bootloader section in front of the
actual firmware.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thankfully, a kind individual on the internet has done this work for us and you
can download the firmware with the bootloader stripped from
&lt;a href=&#34;https://www.friedzombie.com/tplink-stripped-firmware/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once downloaded, you need to flash this firmware. To do that, you need SSH
access. Login in to the router and go to the &amp;ldquo;Services&amp;rdquo; tab. Once there, scroll
down and enable SSH access. After that, go to the &amp;ldquo;Administration&amp;rdquo; and toggle
the radio button for &amp;ldquo;SSH Managment&amp;rdquo;. Reboot the router.&lt;/p&gt;
&lt;p&gt;After reboot, extract and transfer the stripped firmware to the router:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ unzip TL-WR1043ND-V1-stripped.zip
$ cd TL-WR1043ND-V1-stripped
$ scp TL-WR1043ND-V1-FW0.0.3-stripped.bin root@192.168.1.1:/tmp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(use your admin password to authenticate).&lt;/p&gt;
&lt;p&gt;Once this has been uploaded, SSH into the device to perform the restore:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ssh root@192.168.1.1
$ cd /tmp
$ mtd -r write TL-WR1043ND-V1-FW0.0.3-stripped.bin linux
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the restore is complete, you can download the new firmware for the device
from &lt;a href=&#34;http://downloads.openwrt.org/releases/18.06.2/targets/ar71xx/generic/&#34;&gt;openwrt.org&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ wget http://downloads.openwrt.org/releases/18.06.2/targets/ar71xx/generic/openwrt-ar71xx-generic-tl-wr1043nd-v1-squashfs-factory.bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upload this through the web UI.&lt;/p&gt;
&lt;h2 id=&#34;bonus-restoring-to-stock-from-openwrt&#34;&gt;Bonus: Restoring to stock from OpenWRT&lt;/h2&gt;
&lt;p&gt;This is pretty simple too. Follow the above but instead of flashing to &lt;code&gt;linux&lt;/code&gt;,
flash to &lt;code&gt;firmware&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ mtd -r write TL-WR1043ND-V1-FW0.0.3-stripped.bin firmware
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ediy.com.my/index.php/blog/item/9-tp-link-tl-wr1043nd-resotre-from-dd-wrt-to-original-firmware&#34;&gt;http://ediy.com.my/index.php/blog/item/9-tp-link-tl-wr1043nd-resotre-from-dd-wrt-to-original-firmware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.friedzombie.com/tplink-stripped-firmware/&#34;&gt;https://www.friedzombie.com/tplink-stripped-firmware/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://forum.openwrt.org/t/solved-re-installing-the-original-tp-link-firmware-a-second-time/9249/5&#34;&gt;https://forum.openwrt.org/t/solved-re-installing-the-original-tp-link-firmware-a-second-time/9249/5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NUMA, CPU Pinning and &#39;vcpu_pin_set&#39;</title>
      <link>https://that.guru/blog/cpu-resources/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/cpu-resources/</guid>
      <description>&lt;p&gt;The libvirt driver in the OpenStack Compute service (nova) has supported
instance NUMA topologies for a number of releases. A NUMA topology can be added
to an instance either explicitly, using the &lt;code&gt;hw:numa_nodes=N&lt;/code&gt; flavor extra
spec, or implicitly, by requesting a specific mempage size
(&lt;code&gt;hw:mem_page_size=N&lt;/code&gt;) or CPU pinning (&lt;code&gt;hw:cpu_policy=dedicated&lt;/code&gt;). For
historical reasons, it is not possible to request memory pages or CPU pinning
without getting a NUMA topology meaning every pinned instance or instance with
hugepages (common when using something like Open vSwitch with DPDK) has a NUMA
topology associated with it.&lt;/p&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;For most of the extra specs mentioned here and below, there exists an
equivalent image metadata property. These are omitted here for brevity.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;For yet more historical reasons, nova has gained a number of configuration
options that only apply to these instances with NUMA topologies or those
without. This article aims to discuss the implications of one of these,
&lt;code&gt;vcpu_pin_set&lt;/code&gt;, through a number of relevant examples.&lt;/p&gt;
&lt;h2 id=&#34;overview-of-vcpu_pin_set&#34;&gt;Overview of &lt;code&gt;vcpu_pin_set&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;vcpu_pin_set&lt;/code&gt; option has existed in nova for quite some time and describes
itself as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Defines which physical CPUs (pCPUs) can be used by instance
virtual CPUs (vCPUs).&lt;/p&gt;
&lt;p&gt;Possible values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A comma-separated list of physical CPU numbers that virtual CPUs can be
allocated to by default. Each element should be either a single CPU number,
a range of CPU numbers, or a caret followed by a CPU number to be
excluded from a previous range. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  vcpu_pin_set = &amp;quot;4-12,^8,15&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;This config option has two purposes. Firstly, the placement service uses it to
generate the amount of &lt;code&gt;VCPU&lt;/code&gt; resources available on a given host using the
following formula:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(SUM(CONF.vcpu_pin_set) * CONF.cpu_allocation_ratio) - CONF.reserved_host_cpus
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(where &lt;code&gt;SUM&lt;/code&gt; is the sum of CPUs expressed by the CPU mask).&lt;/p&gt;
&lt;p&gt;How we do this can be seen at [&lt;a href=&#34;https://github.com/openstack/nova/blob/19.0.0/nova/virt/libvirt/driver.py#L5714-L5754&#34;&gt;1&lt;/a&gt;]&lt;a href=&#34;https://github.com/openstack/nova/blob/19.0.0/nova/virt/libvirt/driver.py#L5714-L5754&#34;&gt;1&lt;/a&gt;, [&lt;a href=&#34;https://github.com/openstack/nova/blob/19.0.0/nova/virt/libvirt/driver.py#L6645-L6661&#34;&gt;2&lt;/a&gt;]&lt;a href=&#34;https://github.com/openstack/nova/blob/19.0.0/nova/virt/libvirt/driver.py#L6645-L6661&#34;&gt;2&lt;/a&gt;, [&lt;a href=&#34;https://github.com/openstack/nova/blob/19.0.0/nova/virt/driver.py#L903-L935&#34;&gt;3&lt;/a&gt;]&lt;a href=&#34;https://github.com/openstack/nova/blob/19.0.0/nova/virt/driver.py#L903-L935&#34;&gt;3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The number of &lt;code&gt;VCPU&lt;/code&gt; resources impacts instances regardless of whether they
have a NUMA topology or not since potential NUMA/non-NUMA&amp;rsquo;ness is not
considered at this early stage of scheduling. However, once allocation
candidates have been provided by placement, we see the original purpose of this
option emerge: for instances with a NUMA topology, it is used by
&lt;code&gt;nova-scheduler&lt;/code&gt; (specifically by the &lt;code&gt;NUMATopologyFilter&lt;/code&gt;) and by
&lt;code&gt;nova-compute&lt;/code&gt; (when building instance XML). NUMA instances either map their
entire range of instance cores to a range of host cores (for non-pinned
instances) or each individual instance core to a specific host core (for pinned
instances), and this mapping is calculated by both &lt;code&gt;nova-scheduler&lt;/code&gt;&amp;rsquo;s
&lt;code&gt;NUMATopologyFilter&lt;/code&gt; filter and &lt;code&gt;nova-compute&lt;/code&gt; &lt;code&gt;vcpu_pin_set&lt;/code&gt; is used to limit
which of these host cores can be used and allows you to do things like exclude
every core from a host NUMA node. However, since instances without a NUMA
topology are entirely floating and are not limited to any host NUMA node, this
option is totally ignored.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s look at some examples of how this would be reflected in the real world.
For all these examples, consider a host with two sockets and two CPUs with four
cores and no hyperthreading (so eight CPUs).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_host-topology.png&#34; alt=&#34;Non-NUMA instances without vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The host NUMA topology with two sockets and four cores per socket&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We can see the resources that this reports like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 8     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 8     |
+------------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;non-numa&#34;&gt;Non-NUMA&lt;/h3&gt;
&lt;p&gt;First, consider an instance without a NUMA topology:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack flavor create --vcpus 2 --ram 512 --disk 0 test-flavor
$ openstack server create --flavor test-flavor ... test-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As this instance does not have a NUMA topology, the instance will float across
all host cores with no regard for NUMA affinity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_non-numa.png&#34; alt=&#34;Non-NUMA instances without vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The host NUMA topology is ignored for instances without a NUMA topology.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we look at what placement is reporting, we can see that our inventory has
changed accordingly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider usage show \
    6a969900-bbf7-4725-959b-2db3092933b0
+----------------+-------+
| resource_class | usage |
+----------------+-------+
| VCPU           |     2 |
| MEMORY_MB      |   512 |
| DISK_GB        |     0 |
+----------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s use &lt;code&gt;vcpu_pin_set&lt;/code&gt; to exclude the cores from host NUMA node &lt;code&gt;0&lt;/code&gt; as
seen in the sample &lt;code&gt;nova.conf&lt;/code&gt; below, then restart the service:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[DEFAULT]
vcpu_pin_set = 4-7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we examine the number of resources available in placement, we can see that
it has changed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 4     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 4     |
+------------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately though, because the instance does not have a NUMA topology, this
option is completely ignored when actually booting the instance. As above, the
instance continues to run across the entire range of host cores.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_non-numa.png&#34; alt=&#34;Non-NUMA instances with vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The &lt;code&gt;vcpu_pin_set&lt;/code&gt; option is also ignored for instances without a NUMA
topology.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;numa-no-pinning&#34;&gt;NUMA, no pinning&lt;/h3&gt;
&lt;p&gt;Next, consider an instance with a NUMA topology. We can create such an instance
like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack flavor create --vcpus 2 --ram 512 --disk 0 test-flavor
$ openstack flavor set --property hw:numa_nodes=1 test-flavor
$ openstack server create --flavor test-flavor ... test-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because this instance has a NUMA topology, the instance will be confined to
cores from a single host NUMA node.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_numa_wo_vcpu_pin_set.png&#34; alt=&#34;NUMA instances without vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The host NUMA topology is considered for instances with a NUMA topology.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now, once again we&amp;rsquo;ll use &lt;code&gt;vcpu_pin_set&lt;/code&gt; to exclude the cores from host NUMA
node &lt;code&gt;0&lt;/code&gt; via &lt;code&gt;nova.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[DEFAULT]
vcpu_pin_set = 4-7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And again we&amp;rsquo;ll see this change in what&amp;rsquo;s reported to placement:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 4     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 4     |
+------------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time &lt;code&gt;vcpu_pin_set&lt;/code&gt; will actually be respected and we&amp;rsquo;ll see it reflected
in the host cores used by the instance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_numa_w_vcpu_pin_set.png&#34; alt=&#34;NUMA instances with vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The &lt;code&gt;vcpu_pin_set&lt;/code&gt; option is respected for instances with a NUMA topology, so
cores &lt;code&gt;0-3&lt;/code&gt; are excluded.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;numa-with-pinning&#34;&gt;NUMA, with pinning&lt;/h3&gt;
&lt;p&gt;Finally, let&amp;rsquo;s consider pinned instances. We can create such an instance like
so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack flavor create --vcpus 2 --ram 512 --disk 0 test-flavor
$ openstack flavor set --property hw:cpu_policy=dedicated test-flavor
$ openstack server create --flavor test-flavor ... test-server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As noted previously, these have an implicit NUMA topology but whereas every
core of an unpinned instance is mapped to the same range of host cores, the
cores of pinned instances are mapped to their own individual host cores.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_pinning_wo_vcpu_pin_set.png&#34; alt=&#34;Pinned instances without vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;Because they have a NUMA topology, pinned instances also respect
&lt;code&gt;vcpu_pin_set&lt;/code&gt;. As always, we can use &lt;code&gt;vcpu_pin_set&lt;/code&gt; to exclude the cores from
host NUMA node &lt;code&gt;0&lt;/code&gt; via &lt;code&gt;nova.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[DEFAULT]
vcpu_pin_set = 4-7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As always, we&amp;rsquo;ll see this reflected in placement:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack --os-placement-api-version 1.18 resource provider inventory show \
    6a969900-bbf7-4725-959b-2db3092933b0 VCPU
+------------------+-------+
| Field            | Value |
+------------------+-------+
| allocation_ratio | 16.0  |
| max_unit         | 4     |
| reserved         | 0     |
| step_size        | 1     |
| min_unit         | 1     |
| total            | 4     |
+------------------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And because pinned instances also have a NUMA topology, we&amp;rsquo;ll also see this
reflected in the host cores used by the instance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://that.guru/media/cpu-resources_pinning_w_vcpu_pin_set.png&#34; alt=&#34;Pinned instances with vcpu_pin_set&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The &lt;code&gt;vcpu_pin_set&lt;/code&gt; option is also respected for instances with CPU pinning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;However, it&amp;rsquo;s worth noting here that pinned instances cannot be overcommited.
Despite the fact that we have an &lt;code&gt;allocation_ratio&lt;/code&gt; of 16.0, we can only
schedule &lt;code&gt;total&lt;/code&gt; instances cores. These cores also can&amp;rsquo;t be spread across host
NUMA nodes unless you&amp;rsquo;ve specifically said otherwise (via the
&lt;code&gt;hw:numa_nodes=N&lt;/code&gt; flavor extra spec).&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;vcpu_pin_set&lt;/code&gt; option is used to generate the amount of &lt;code&gt;VCPU&lt;/code&gt; resources
available in placement but it otherwise has no effect on instances without a
NUMA topology. For instances with a NUMA topology, it also controls the host
cores that the instance can schedule to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Direct vs Hostdev Interfaces in Nova</title>
      <link>https://that.guru/blog/direct-vs-hostdev-interfaces/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/direct-vs-hostdev-interfaces/</guid>
      <description>&lt;p&gt;Mostly a note for myself. There are two types of SR-IOV&amp;rsquo;y networks supported in
nova: &lt;code&gt;direct&lt;/code&gt; and &lt;code&gt;hostdev&lt;/code&gt;. Confusingly, the latter corresponds to passthrough
of the virtual function (VF), while the former corresponds to macvtap. The
difference between these is described rather succinctly in an Oracle whitepaper
titled &lt;a href=&#34;https://cloud.oracle.com/iaas/whitepapers/installing_kvm_multi_vnics.pdf&#34;&gt;&amp;ldquo;Installing and Configuring KVM on Bare Metal Instances with
Multi-VNIC&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;hostdev&#34;&gt;Hostdev&lt;/h2&gt;
&lt;p&gt;First, the hostdev VIF type:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The hostdev method is preferred for both performance and guest isolation
reasons. It provides the guest with direct access to the PCI device, created
as part of the configuration of SR-IOV on the hypervisor. A PCI device is
known as a virtual function (VF)and represents an actual interface into the
hardware of the hypervisor (bare metal instance). This allows the guest to
have both maximum throughput and maximum isolation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Maximum throughput because there is no operating system between the guest
and the network&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximum isolation because the hypervisor operating system is not involved
beyond providing the hardware interface (the overhead is minimal)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The disadvantage of the hostdev method is that it isn&amp;rsquo;t possible to emulate
a different device type. So, the guest operating system must have a driver
available that matches the hardware type provided by the hypervisor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a user, you are likely to encounter the driver issues outlined above when
using something like CirrOS image deployed by DevStack.&lt;/p&gt;
&lt;h2 id=&#34;direct&#34;&gt;Direct&lt;/h2&gt;
&lt;p&gt;Then the direct VIF type which, again, is not really &amp;ldquo;direct&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The direct method relies on hypervisor-configured network interfaces to
provide connectivity to the guest operating systems. However, the network
configuration provided by the hypervisor is minimal: the guest operating
system still issues all the DHCP and related higher-level networking
management, while the hypervisor simply provides an interface for the guest
to operate on.&lt;/p&gt;
&lt;p&gt;The direct method allows KVM to natively emulate some common network
interface types that are typically found in most current and legacy
operating systems. The following emulations have been observed to work: the
e1000 (Intel FastEthernet driver) and the virtio (KVM native) device types,
although the virtio driver might still require you to inject a driver into a
Windows operating system. This is useful for prepackaged, virtual machines
because their configurations are typically static and are looking for
specific hardware types.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Poon Hill Trek</title>
      <link>https://that.guru/blog/poon-hill-trek/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/poon-hill-trek/</guid>
      <description>&lt;p&gt;So, there I was, in Nepal for two weeks with a friend, wanting to try some of
the world-renowned trekking along with exploring the cities of Kathmandu and
Pokhara.  I only had two weeks, which unfortunately ruled out both the famous
&lt;a href=&#34;https://en.wikivoyage.org/wiki/Everest_Base_Camp_Trek&#34;&gt;Everest Base Camp&lt;/a&gt; and &lt;a href=&#34;https://en.wikivoyage.org/wiki/Annapurna_Circuit&#34;&gt;Annapurna Circuit&lt;/a&gt; treks, along with treks through
the less well known but apparently stunning &lt;a href=&#34;https://en.wikivoyage.org/wiki/Langtang_Valley_Trek&#34;&gt;Langtang Valley&lt;/a&gt;, given that all
take a minimum of nine days to complete. That left, according to the wisdom of
the internet, a trek to Poon Hill (Ghorepani) in the Annapurna Valley as my
best option and this is roughly what we did.&lt;/p&gt;
&lt;p&gt;Below are my rough notes from that trek, which took place in November 2018.
With any luck, I&amp;rsquo;ll get most of this published on &lt;a href=&#34;https://en.wikivoyage.org/&#34;&gt;Wikivoyage&lt;/a&gt; before long.&lt;/p&gt;
&lt;h2 id=&#34;getting-there&#34;&gt;Getting there&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;d flown into and was staying in Kathmandu, the capital, but pretty much all
treks in the Annapurna region start from Pokhara, the second biggest city in
Nepal. As such, our first step was to get from Kathmandu to Pokhara. How to do
this is pretty well documented but, in summary, you can either fly or take a
tourist bus. Flying is much faster (30 minutes vs. 6-7 hours) but significantly
more expensive (~120 USD vs. 500-800 NPR) so we ended up taking the bus. This
itself was an interesting experience, given that seemed to spend as much time
driving alongside the road, which was closed for resurfacing, as we did on the
road.&lt;/p&gt;
&lt;p&gt;From Pokhara, you need to get to Nayapul. You can take either a bus from
&lt;a href=&#34;https://www.lonelyplanet.com/nepal/pokhara/transport/getting-there-away/land&#34;&gt;Baglung bus station&lt;/a&gt; or do what we did and get a taxi. Buses apparently cost
~200 Rs. per person while a taxi will be ~2000 Rs. total. Once you arrive in
Nayapul, you can start walking towards Birethanti, where the various park
checkpoints are located.&lt;/p&gt;
&lt;h2 id=&#34;itinerary&#34;&gt;Itinerary&lt;/h2&gt;
&lt;h3 id=&#34;day-1&#34;&gt;Day 1&lt;/h3&gt;
&lt;p&gt;We started from Pokhara rather late (~11am) and arrived in Neyapul, near
Neyapul Trekking Starting Point, around 1pm. From here, we walked down to
Birethanti where we passed two checkpoints: first for the TIMS, the trekking
permit (2000 Rs. if purchased in Pohkara, same onsite), and the second for the
conservation area pass (3000 Rs. if purchased in Pokhara, 6000 Rs. if purchased
onsite). From here, you can choose to do the loop two ways. We went to
Ghorepani, which is the easier, nicer route.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_1_a.jpg&#34;
         alt=&#34;The taxi ride to Nayapul was&amp;amp;hellip;interesting.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;The taxi ride to Nayapul was&amp;hellip;interesting.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We stopped at Ulleri, which was just below our targeted destination of
Banthanti, as it was starting to get dark by 5pm (sundown around 5:30pm). As
there were there just two of us, we were able to get a room (with attached hot
shower!) for free once we bought food. Total came to 3400 Rs. which included
dinner and breakfast - both with beverages - along with a bottle of water in
the morning. This is the only place we stayed at that included a hot shower in
the room and we were certainly glad to have it.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_1_b.jpg&#34;
         alt=&#34;We finally made it to Ulleri.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;We finally made it to Ulleri.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_1_c.jpg&#34;
         alt=&#34;Warm bed? Check. Hot shower? Check.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Warm bed? Check. Hot shower? Check.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;day-2&#34;&gt;Day 2&lt;/h3&gt;
&lt;p&gt;We rose after sunrise and left about 8:30am. Arrived in Ghorepani about 11:30am
and got ourselves some coffee and baked goods before heading onto Poon Hill
(side note: the machine in the coffee shop here apparently weighs 70+ kg and
was carried up on someone&amp;rsquo;s back. As you do). If we had been able to make it
here the day before (ambitious though doable), we could have headed up to Poon
Hill for sunrise (expect at 4:30am rise as it&amp;rsquo;s a 45min - 1 hour climb - you
have to pay an entry fee at this time too), which is supposed to be amazing. We
didn&amp;rsquo;t fancy hanging around Ghorepani until the next day so we went up straight
after our coffee. Even at this time, the view is still outstanding and we hung
around here for a while.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_2_a.jpg&#34;
         alt=&#34;The sort of famous Poon Hill sign.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;The sort of famous Poon Hill sign.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_2_b.jpg&#34;
         alt=&#34;The view from the top.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;The view from the top.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Once we were done, we headed back down to Ghorepani, leaving here at 2:30pm,
arriving in Deurali at 3:40pm, followed by Ban Thanti at 4:30pm, and finished
up in Tadapani at 5:40pm. This was a bit late, given that it was essentially
dark by then, but it wasn&amp;rsquo;t anything too challenging physically.&lt;/p&gt;
&lt;h3 id=&#34;day-3&#34;&gt;Day 3&lt;/h3&gt;
&lt;p&gt;We decided we&amp;rsquo;d leave Tadapani a little earlier than we&amp;rsquo;d left Ulleri the day
before and headed off at 8:10am. Got to Ghandruk about 10:10, where we grabbed
some tea here before heading onto Jhinu Danda and the nearby Jhinu hot springs.&lt;/p&gt;
&lt;p&gt;This initially wasn&amp;rsquo;t too bad, as we simply doubled back in ourselves and
walked towards a place called Little Paradise. From Little Paradise though, we
got totally lost. We were told to &amp;ldquo;follow the electricity lines&amp;rdquo; and the blue
and white markers along the trail. We did the former and it wasn&amp;rsquo;t long before
we realized we were totally lost and ended up walking through farms and &amp;ldquo;local&amp;rdquo;
tracks. Google Maps was all but useless in the area so we very much relied on
the map we&amp;rsquo;d brought with us. We were very happy to see the suspension bridge
before Jhinu Danda and ended up arriving about 3:30pm, having expected to
arrive about 2pm. We had a quick lunch in Jhinu Danda before heading down to
the hot springs for a soak. After this, we went back to Jhinu Danda where we
stayed for the night.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_3_a.jpg&#34;
         alt=&#34;One of the many steel &amp;amp;ldquo;rope bridges&amp;amp;rdquo; we crossed on our way. Most of these were provided by the Gurkhas.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;One of the many steel &amp;ldquo;rope bridges&amp;rdquo; we crossed on our way. Most of these were provided by the Gurkhas.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_3_b.jpg&#34;
         alt=&#34;The walk down to the hot springs.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;The walk down to the hot springs.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;day-4&#34;&gt;Day 4&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;d planned to make our way back Birethanti along the west side of the river,
instead of crossing the &amp;ldquo;New Bridge&amp;rdquo; and travelling via Landruk. This turned
out to be a terrible idea. We&amp;rsquo;re still not sure what we did wrong but we got
totally lost on the way and ended up walking through fields. We were fortuitous
enough to meet a local farmer passing through one of this fields, who put us on
the right path (which happened to also take us via his family&amp;rsquo;s restaurant,
heh). From here, it was a long, rather dull walk down into the valley, where we
were able to get a taxi straight back to Pokhara.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/poon_hill_day_4.jpg&#34;
         alt=&#34;Day 4 was mostly spent walking by (and through) farms like this one.&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Day 4 was mostly spent walking by (and through) farms like this one.&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;additional-notes&#34;&gt;Additional Notes&lt;/h2&gt;
&lt;h3 id=&#34;agencies-vs-guides-vs-solo-travel&#34;&gt;Agencies vs. guides vs. solo travel&lt;/h3&gt;
&lt;p&gt;There are three ways to organize a trek in Nepal. Firstly, you can use an
agency. These can be booked online or in Kathmandu or Pokhara as soon as you
arrive. They will organize everything for you, including guides, porters, food
and accommodation but they are by far the most expensive option, particularly
when booked online. Unless you&amp;rsquo;ve a good reason not to, you&amp;rsquo;re almost always
going to be better off organizing a guide yourself or, if possible (some areas
require a guide or may be considered too challenging to do solo), by travelling
solo (as in, just you and your friends). Given that the Poon Hill trek is a
relatively easy, short trek, we opted for the latter and the below guide is
based on this. If you do want a guide though, I can&amp;rsquo;t stress enough &lt;em&gt;(a)&lt;/em&gt; the
importance of meeting your guide before you travel (you&amp;rsquo;re going to be spending
days with this person - make sure you like each other) and &lt;em&gt;(b)&lt;/em&gt; the value of
shopping around.&lt;/p&gt;
&lt;h3 id=&#34;permits&#34;&gt;Permits&lt;/h3&gt;
&lt;p&gt;To trek in the Annapurna Conservation Area, you require both a &amp;ldquo;&lt;a href=&#34;https://www.welcomenepal.com/plan-your-trip/tims-card.html&#34;&gt;Trekkers&#39;
Information Management Systems&lt;/a&gt;&amp;rdquo; (TIMS) card and a conservation area pass.
These cost 2000 Rs. and 3000 Rs. respectively. Both of these should be
purchased in Pokhara or Kathmandu before you travel. If you forget (or didn&amp;rsquo;t
know, as was the issue for us), these can be purchased at the checkpoints to
the park in Birethanti but for a higher price (2000 Rs. for the TIMS card but
6000 Rs. for the conservation area pass).&lt;/p&gt;
&lt;h3 id=&#34;food-drink-and-accommodation&#34;&gt;Food, drink and accommodation&lt;/h3&gt;
&lt;p&gt;We ended up spending roughly 3500-4000 Rs. a day between two of us and in all
but one village, we were able to negotiate free accommodation in exchange for
purchasing two meals. This isn&amp;rsquo;t the case for everyone and I suspect it very
much depends on whether you&amp;rsquo;re travelling solo or not, the time of year, and
how busy the particular tea house you wish to stay in is.&lt;/p&gt;
&lt;h3 id=&#34;when-to-visit&#34;&gt;When to visit&lt;/h3&gt;
&lt;p&gt;We visited in November, which marks the end of peak tourist season. The weather
was decent, raining only once during our time there, and it&amp;rsquo;s a lot warmer than
December, when it can be -7C during day, dropping to as low as -20C during
the night. We were told it&amp;rsquo;s visiting in March or April can also be a good idea
as the forest is in full bloom. Note that it&amp;rsquo;s a lot warmer at this time of
year, however.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Updating Mellanox ConnectX-3 NIC Firmware</title>
      <link>https://that.guru/blog/updating-mellanox-connectx-3/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/updating-mellanox-connectx-3/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://that.guru/blog/sriov-mellanox-connectx-3&#34;&gt;previous post&lt;/a&gt;, I provided a guide on configuring SR-IOV for a Mellanox
ConnectX-3 NIC. I&amp;rsquo;ve since picked up a second one of these and was attempting
to follow through on the same guide. However, when I attempted to &amp;ldquo;&lt;em&gt;query&lt;/em&gt;&amp;rdquo; the
device, I saw the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mstconfig -d 02:00.0 query

Device #1:
----------

Device type:    ConnectX3
PCI device:     02:00.0

-E- Failed to query device: 02:00.0. Unsupported FW (version 2.31.5000 or above required for CX3/PRO)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Clearly we need to update something here, hence, here&amp;rsquo;s my guide to updating
that firmware and making &lt;code&gt;mstconfig&lt;/code&gt; happy again.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;As with the other post, you&amp;rsquo;re going to need a Mellanox ConnectX-3 card for
this to be of any use. Once again, I&amp;rsquo;m using Ubuntu 16.04 because that&amp;rsquo;s what
that OpenStack gate uses but I think most of this stuff is packaged on Fedora
too.&lt;/p&gt;
&lt;h2 id=&#34;identify-and-download-firmware&#34;&gt;Identify and Download Firmware&lt;/h2&gt;
&lt;p&gt;As you can see on the &lt;a href=&#34;http://www.mellanox.com/page/firmware_table_ConnectX3EN&#34;&gt;firmware download page&lt;/a&gt;, there are a variety of
ConnectX-3 models available, each with different feature sets (Ethernet vs.
Infiniband, two ports vs. four). You need to figure out which one you have.
These models are identified by their &lt;strong&gt;OPN&lt;/strong&gt; and &lt;strong&gt;PSID&lt;/strong&gt;. I&amp;rsquo;ve still to figure
out how to identify the former, but the latter is easily identified using the
&lt;code&gt;mstflint&lt;/code&gt; tool. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mstflint -d 02:00.0 q
Image type:      FS2
FW Version:      2.30.8000
Rom Info:        type=PXE version=3.4.151 devid=4099 proto=ETH
Device ID:       4099
Description:     Node             Port1            Port2            Sys image
GUIDs:           0002c90300056aa8 0002c90300056aa9 0002c90300056aaa 0002c90300056aab
MACs:                                 0002c921d6d0     0002c921d6d1
VSD:
PSID:            MT_108012002
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So with our PSID to hand - &lt;code&gt;MT_108012002&lt;/code&gt; in this case - we can proceed to
downloading the correct firmware from the firmware download page. There doesn&amp;rsquo;t
seem to be anyway to download by just PSID so I had to click through the list
of OPNs until I found the OPN with a PSID that matches. In my case, this was
&lt;code&gt;MCX312A-XCBT&lt;/code&gt; for firmware version &lt;code&gt;2.42.5000&lt;/code&gt;. Download the relevant
firmware to your host. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ wget http://www.mellanox.com/downloads/firmware/fw-ConnectX3-rel-2_42_5000-MCX312A-XCB_A2-A6-FlexBoot-3.4.752.bin.zip
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;update-firmware&#34;&gt;Update Firmware&lt;/h2&gt;
&lt;p&gt;With our firmware update to hand, it&amp;rsquo;s time to actually update the NIC. For
this section, I initially followed the &lt;a href=&#34;http://www.mellanox.com/page/firmware_NIC_FW_update&#34;&gt;updating guide&lt;/a&gt; provided by Mellanox.
However, as before, I didn&amp;rsquo;t want to install anything that wasn&amp;rsquo;t already
provided with my distro. As such the instructions provided below are a suitable
alternative.&lt;/p&gt;
&lt;p&gt;Firstly, extract the firmware you downloaded previously. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ unzip fw-ConnectX3-rel-2_42_5000-MCX312A-XCB_A2-A6-FlexBoot-3.4.752.bin.zip
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once done, it&amp;rsquo;s time to flash the firmware. To do this, we once again use the
&lt;code&gt;mstflint&lt;/code&gt; tool::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mstflint -d 02:00.0 -i fw-ConnectX3-rel-2_42_5000-MCX312A-XCB_A2-A6-FlexBoot-3.4.752.bin burn

Current FW version on flash:  2.30.8000
New FW version:               2.42.5000

Burning FS2 FW image without signatures - OK
Restoring signature                     - OK
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now reboot your machine so the new firmware is loaded::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo reboot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And with that, we have flashed out new firmware and can query the NIC
configuration::&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mstconfig -d 02:00.0 query

Device #1:
----------

Device type:    ConnectX3
PCI device:     02:00.0

Configurations:                              Current
         SRIOV_EN                            0
         NUM_OF_VFS                          8
         LOG_BAR_SIZE                        3
         BOOT_OPTION_ROM_EN_P1               1
         BOOT_VLAN_EN_P1                     0
         BOOT_RETRY_CNT_P1                   0
         LEGACY_BOOT_PROTOCOL_P1             1
         BOOT_VLAN_P1                        1
         BOOT_OPTION_ROM_EN_P2               1
         BOOT_VLAN_EN_P2                     0
         BOOT_RETRY_CNT_P2                   0
         LEGACY_BOOT_PROTOCOL_P2             1
         BOOT_VLAN_P2                        1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Easy peasy. Now back to &lt;a href=&#34;https://that.guru/blog/sriov-mellanox-connectx-3&#34;&gt;configuring SR-IOV&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Configuring SR-IOV for a Mellanox ConnectX-3 NIC</title>
      <link>https://that.guru/blog/sriov-mellanox-connectx-3/</link>
      <pubDate>Tue, 25 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/sriov-mellanox-connectx-3/</guid>
      <description>&lt;p&gt;So &lt;a href=&#34;https://en.wikipedia.org/wiki/Single-root_input/output_virtualization&#34;&gt;SR-IOV&lt;/a&gt; support in my &lt;a href=&#34;https://ark.intel.com/products/84804/Intel-Ethernet-Server-Adapter-I350-T2V2&#34;&gt;Intel I350-T2V2&lt;/a&gt; decided to stop working recently
(or maybe it never worked - I can&amp;rsquo;t be sure), meaning it was time to pick up a
new SR-IOV NIC for testing/development. I settled on a used
&lt;a href=&#34;http://www.mellanox.com/page/products_dyn?product_family=127&#34;&gt;Mellanox ConnectX-3&lt;/a&gt; from eBay because it supported SR-IOV and other cool
things like RDMA over Ethernet (&lt;a href=&#34;https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet&#34;&gt;RoCE&lt;/a&gt;), the Mellanox guys I&amp;rsquo;ve dealt with in
Nova have been a great bunch and, most crucially at the time, it was cheap. For
what it&amp;rsquo;s worth, I also got a pair of &lt;a href=&#34;https://en.wikipedia.org/wiki/Small_form-factor_pluggable_transceiver&#34;&gt;SFP&lt;/a&gt; cables&lt;/p&gt;
&lt;p&gt;The first time I configured this, I followed the instructions from the
&lt;a href=&#34;https://community.mellanox.com/docs/DOC-2365&#34;&gt;Mellanox website&lt;/a&gt;. This mandated downloading a tarball and
using their custom installer script, &lt;code&gt;mlnxofedinstall&lt;/code&gt;, to install the
drivers and various tools. It was only when I later reinstalled the OS on this
machine that I discovered this was wholly unnecessary: Ubuntu 16.04 (and
presumably 18.04) already include everything you need to configure and use
these NICs. As such, here is &amp;ldquo;Stephen&amp;rsquo;s Guide to Using Mellanox ConnectX-3
Cards Without All That C***&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;It should go without saying, but you need a Mellanox ConnectX-3 card for this
to be of any use. In addition, I&amp;rsquo;m using Ubuntu 16.04 because that&amp;rsquo;s what that
OpenStack gate uses, but I think most of this stuff is packaged on Fedora too.&lt;/p&gt;
&lt;h2 id=&#34;enable-sr-iov-in-the-firmware&#34;&gt;Enable SR-IOV in the firmware&lt;/h2&gt;
&lt;p&gt;The ConnectX-3 allows you to configure the amount of VFs available on the
device. To do this, the &lt;a href=&#34;https://community.mellanox.com/docs/DOC-2365&#34;&gt;official guide&lt;/a&gt; would have you run the &lt;code&gt;mlxconfig&lt;/code&gt;
tool, which is installed by the aforementioned &lt;code&gt;mlnxofedinstall&lt;/code&gt; tool.
However, Mellanox have an open source version of this tool, &lt;code&gt;mstconfig&lt;/code&gt;,
which fulfils the same purpose as is available as part of the
&lt;a href=&#34;https://github.com/Mellanox/mstflint&#34;&gt;&lt;code&gt;mstflint&lt;/code&gt;&lt;/a&gt; package. Install this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo apt install mstflint
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once installed, inspect the current configuration of the device. To do this,
you need to find the PCI address of the device, which is pretty easy when you
only have one such device in your system:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ lspci | grep Mellanox
02:00.0 Ethernet controller: Mellanox Technologies MT27500 Family [ConnectX-3]
$ sudo mstconfig -d 02:00.0 query

Device #1:
----------

Device type:    ConnectX3
PCI device:     02:00.0

Configurations:                              Current
         SRIOV_EN                            1
         NUM_OF_VFS                          8
         LOG_BAR_SIZE                        3
         BOOT_OPTION_ROM_EN_P1               1
         BOOT_VLAN_EN_P1                     0
         BOOT_RETRY_CNT_P1                   0
         LEGACY_BOOT_PROTOCOL_P1             1
         BOOT_VLAN_P1                        1
         BOOT_OPTION_ROM_EN_P2               1
         BOOT_VLAN_EN_P2                     0
         BOOT_RETRY_CNT_P2                   0
         LEGACY_BOOT_PROTOCOL_P2             1
         BOOT_VLAN_P2                        1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;As you can see, I already have SR-IOV enabled (&lt;code&gt;SRIOV_EN=1&lt;/code&gt;) and have enabled
eight VFs (&lt;code&gt;NUM_OF_VFS=8&lt;/code&gt;). If this wasn&amp;rsquo;t the case though, you&amp;rsquo;d need to
configure these attributes. You can do so using the &lt;code&gt;mstconfig&lt;/code&gt; tool again.
For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mstconfig -d 02:00.0 set SRIOV_EN=1 NUM_OF_VFS=8

Device #1:
----------

Device type:    ConnectX3
PCI device:     02:00.0

Configurations:                              Current         New
         SRIOV_EN                            1               1
         NUM_OF_VFS                          8               8
         LOG_BAR_SIZE                        3               3
         BOOT_OPTION_ROM_EN_P1               1               1
         BOOT_VLAN_EN_P1                     0               0
         BOOT_RETRY_CNT_P1                   0               0
         LEGACY_BOOT_PROTOCOL_P1             1               1
         BOOT_VLAN_P1                        1               1
         BOOT_OPTION_ROM_EN_P2               1               1
         BOOT_VLAN_EN_P2                     0               0
         BOOT_RETRY_CNT_P2                   0               0
         LEGACY_BOOT_PROTOCOL_P2             1               1
         BOOT_VLAN_P2                        1               1

 Apply new Configuration? ? (y/n) [n] :
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If applying configuration, you should now reboot and then inspect the
configuration to ensure it has persisted:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo mstconfig -d 02:00.0 query
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the device&amp;rsquo;s firmware is configured, we can move on to configuring the
driver.&lt;/p&gt;
&lt;h2 id=&#34;enable-sr-iov-in-the-driver&#34;&gt;Enable SR-IOV in the driver&lt;/h2&gt;
&lt;p&gt;As with &lt;code&gt;mstconfig&lt;/code&gt; above, Ubuntu 16.04 also provides in-tree alternatives to
the drivers provided in the tarball o&#39; doom. Better yet, these drivers are
provided and enabled by default: all we need to do is configure them.&lt;/p&gt;
&lt;p&gt;As noted in the &lt;a href=&#34;https://community.mellanox.com/docs/DOC-2365&#34;&gt;original guide&lt;/a&gt;, this can be done by
creating (or editing) the &lt;code&gt;/etc/modprobe.d/mlx4_core.conf&lt;/code&gt; file. Add the
following to that file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;options mlx4_core num_vfs=4,4,0 port_type_array=2,2 probe_vf=4,4,0 probe_vf=4,4,0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Reproducing (in slightly modified form) from the guide, this means:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;num_vfs&lt;/strong&gt; - is the number of VF required for this server, in this
example 8 VFs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;port_type_array&lt;/strong&gt; - is the port type of the interface, 1 is for
infiniBand, 2 for Ethernet. In this example, both ports are Ethernet.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;probe_vf&lt;/strong&gt; - is the number of VF to be probed in the hypervisor. Probed
in the hypervisor means that the VF will also have interface in the
hypervisor (e.g. can be seen using the command &lt;code&gt;ifconfig&lt;/code&gt;).  In this
example there are no probed VFs. When running &lt;code&gt;ifconfig&lt;/code&gt;, no new
interfaces will be added (per VF). In case, &lt;code&gt;probe_vf&lt;/code&gt; was equal to 1
for example, we would get 2 new interfaces in the hypervisor (check
&lt;code&gt;ifconfig -a&lt;/code&gt;), one each port.&lt;/p&gt;
&lt;p&gt;Probed VFs can be used by the IT administrator to monitor the traffic on
that hypervisor without the need of doing that via logging to the VM
itself.&lt;/p&gt;
&lt;p&gt;In this example, we will have 4 VFs on the first physical port and 4 on
the other. The 0 indicates that you don&amp;rsquo;t want VF to be probed on both
port. Refer to the &lt;a href=&#34;https://community.mellanox.com/docs/DOC-1484#jive_content_id_Configuring_8_VFs_on_a_dual_port_NIC_while_4_VFs_are_probed_on_port_1_and_4_VFs_are_probed_on_port_2&#34;&gt;Mellanox docs&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of these, the &lt;code&gt;probe_vf&lt;/code&gt; one is particularly important. Without this, you&amp;rsquo;ll
see the VFs listed under their parent PF with &lt;code&gt;ip link&lt;/code&gt; but each VF will not
have its own netdev. Nova requires that these devices do have their own netdev
so this is a necessity.&lt;/p&gt;
&lt;p&gt;Once this is configured, save the file and reload the driver.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo modprobe -r mlx4_en mlx4_ib
$ sudo modprobe mlx4_en
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should now see the devices listed in &lt;code&gt;ip link&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ip link
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp5s0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 0c:c4:7a:d8:bd:72 brd ff:ff:ff:ff:ff:ff
3: enp6s0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 0c:c4:7a:d8:bd:73 brd ff:ff:ff:ff:ff:ff
6: enp2s0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop portid e41d2d03004c47c0 state DOWN mode DEFAULT group default qlen 1000
    link/ether e4:1d:2d:4c:47:c0 brd ff:ff:ff:ff:ff:ff
    vf 0 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 1 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 2 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 3 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 4 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 5 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 6 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 7 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
7: enp2s0d1: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop portid e41d2d03004c47c1 state DOWN mode DEFAULT group default qlen 1000
    link/ether e4:1d:2d:4c:47:c1 brd ff:ff:ff:ff:ff:ff
    vf 0 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 1 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 2 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 3 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 4 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 5 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 6 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
    vf 7 MAC 00:00:00:00:00:00, vlan 4095, spoof checking off, link-state auto
8: enp2s0f1: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop portid e41d2d03004c47c0 state DOWN mode DEFAULT group default qlen 1000
    link/ether ce:c9:04:d2:00:a4 brd ff:ff:ff:ff:ff:ff
9: enp2s0f1d1: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop portid e41d2d03004c47c1 state DOWN mode DEFAULT group default qlen 1000
    link/ether ce:20:d7:8b:38:6c brd ff:ff:ff:ff:ff:ff
10: enp2s0f2: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop portid e41d2d03004c47c0 state DOWN mode DEFAULT group default qlen 1000
    link/ether fe:a0:dc:21:1f:4c brd ff:ff:ff:ff:ff:ff
11: enp2s0f2d1: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop portid e41d2d03004c47c1 state DOWN mode DEFAULT group default qlen 1000
    link/ether 46:a5:f9:9c:ee:27 brd ff:ff:ff:ff:ff:ff
....
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;Now that everything is configured, it&amp;rsquo;s time to start using it. I dove straight
in with OpenStack. Feel free to use the &lt;a href=&#34;https://gist.github.com/stephenfin/a13fb36bb1afb6fcd770cc24ff125a33&#34;&gt;&lt;code&gt;local.conf&lt;/code&gt;&lt;/a&gt; I used to
deploy this with &lt;a href=&#34;https://docs.openstack.org/devstack/rocky/&#34;&gt;DevStack&lt;/a&gt;. The &lt;a href=&#34;https://docs.openstack.org/neutron/rocky/admin/config-sriov&#34;&gt;neutron SR-IOV docs&lt;/a&gt; are probably worth a look
too.  These are based on the Rocky release (August 2018) so they probably won&amp;rsquo;t
age well, but they are a starting point.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatically Set The Name of tmux Windows</title>
      <link>https://that.guru/blog/automatically-set-tmux-window-name/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/automatically-set-tmux-window-name/</guid>
      <description>&lt;p&gt;I recently updated to Fedora 28 which includes a updated version of &lt;em&gt;tmux&lt;/em&gt; -
&lt;em&gt;tmux 2.7&lt;/em&gt;. I quickly noted that my window title was no longer being set as
before and eventually identified the issue: a breaking change in 2.7. Below is
my guide on ensuring &lt;em&gt;tmux&lt;/em&gt; displays a useful title for each window.&lt;/p&gt;
&lt;p&gt;What I&amp;rsquo;m looking to achieve is a prompt like the following:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/automatically-set-tmux-window-name.png&#34;
         alt=&#34;tmux prompt&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;More specifically, I want &lt;code&gt;$(hostname): $(basename $PWD)&lt;/code&gt;. &lt;em&gt;tmux&lt;/em&gt; provides
two ways to do this.&lt;/p&gt;
&lt;h2 id=&#34;the-automatic-rename-options&#34;&gt;The &lt;code&gt;automatic-rename&lt;/code&gt; options&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;http://man7.org/linux/man-pages/man1/tmux.1.html&#34;&gt;&lt;code&gt;tmux(1)&lt;/code&gt;&lt;/a&gt; man page describes the following options:&lt;/p&gt;
&lt;blockquote&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;automatic-rename&lt;/strong&gt; &lt;em&gt;[on | off]&lt;/em&gt;&lt;/dt&gt;
&lt;dd&gt;Control automatic window renaming.  When this setting is enabled, tmux will
rename the window automatically using the format specified by
&lt;code&gt;automatic-rename-format&lt;/code&gt;. This flag is automatically disabled for an
individual window when a name is specified at creation with &lt;code&gt;new-window&lt;/code&gt; or
&lt;code&gt;new-session&lt;/code&gt;, or later with &lt;code&gt;rename-window&lt;/code&gt;, or with a terminal escape
sequence.  It may be switched off globally with:
&lt;pre&gt;&lt;code&gt;set-window-option -g automatic-rename off
&lt;/code&gt;&lt;/pre&gt;&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;automatic-rename-format&lt;/strong&gt; &lt;em&gt;format&lt;/em&gt;&lt;/dt&gt;
&lt;dd&gt;The format (see &lt;code&gt;FORMATS&lt;/code&gt;) used when the &lt;code&gt;automatic-rename&lt;/code&gt; option is
enabled.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;status-interval&lt;/strong&gt; &lt;em&gt;interval&lt;/em&gt;&lt;/dt&gt;
&lt;dd&gt;Update the status line every interval seconds.  By default, updates will
occur every 15 seconds.  A setting of zero disables redrawing at interval.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can use a combination of these to configure the status line by adding the
following to our &lt;code&gt;.tmux.conf&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set-option -g status-interval 1
set-option -g automatic-rename on
set-option -g automatic-rename-format &#39;#{b:pane_current_path}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More detail can be found in a related &lt;a href=&#34;https://stackoverflow.com/questions/28376611&#34;&gt;StackOverflow question&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately this is not ideal as there will still be some lag between the
window name being set and it being reflected in the UI. This brings us to the
alternative.&lt;/p&gt;
&lt;h2 id=&#34;terminal-escape-sequences&#34;&gt;Terminal escape sequences&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;tmux&lt;/em&gt; can also take advantage of terminal escape sequences. By emitting these
from our shell, we can ensure the changes to the window name happen
automatically. To this end, &lt;a href=&#34;http://man7.org/linux/man-pages/man1/bash.1.html&#34;&gt;&lt;code&gt;bash(1)&lt;/code&gt;&lt;/a&gt; provides the &lt;code&gt;PROMPT_COMMAND&lt;/code&gt; shell
variable:&lt;/p&gt;
&lt;blockquote&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;PROMPT_COMMAND&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;If set, the value is executed as a command prior to issuing each primary
prompt.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can configure this in our &lt;code&gt;.bashrc&lt;/code&gt; file like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;case &amp;quot;$TERM&amp;quot; in
linux|xterm*|rxvt*)
  export PROMPT_COMMAND=&#39;echo -ne &amp;quot;\033]0;${HOSTNAME%%.*}: ${PWD##*/}\007&amp;quot;&#39;
  ;;
screen*)
  export PROMPT_COMMAND=&#39;echo -ne &amp;quot;\033k${HOSTNAME%%.*}: ${PWD##*/}\033\\&amp;quot; &#39;
  ;;
*)
  ;;
esac
&lt;/code&gt;&lt;/pre&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;This is actually a little more complicated that necessary but it with ensure
we get the format requested both when we&amp;rsquo;re using &lt;em&gt;tmux&lt;/em&gt; (the latter case)
and when we are not (the former).&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;You also need to configure the following in your &lt;code&gt;.tmux.conf&lt;/code&gt; in order to
work around the breaking change mentioned above:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;set -g allow-rename on
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once done, restart &lt;em&gt;tmux&lt;/em&gt; as you&amp;rsquo;ll see your changes propagated.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolution Calendar Issues</title>
      <link>https://that.guru/blog/evolution-calendar-issues/</link>
      <pubDate>Fri, 08 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/evolution-calendar-issues/</guid>
      <description>&lt;p&gt;I started Evolution this morning and noticed half of my calendar events were
missing. Attempts to refresh said calendar resulted in the following errors
message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The calendar backend servicing &amp;ldquo;XXX&amp;rdquo; encountered an error.&lt;/p&gt;
&lt;p&gt;The reported error was SQLite error code &amp;lsquo;11&amp;rsquo;: database disk image is
malformed (statement:SELECT * FROM ECacheObjects WHERE ECacheState!=0).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just the start I wanted to my Friday morning. Unfortunately the Evolution
documentation didn&amp;rsquo;t provided any guidelines on fixing a corrupted database and
the best advice I found, aside from deleting and recreating the account in
Evolution, was to run an integrity check on the offending database &lt;a href=&#34;https://ubuntuforums.org/showthread.php?t=2215232#post12978225&#34;&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I knew from this guide that my main Evolution configuration was stored in
&lt;code&gt;.local/share/evolution&lt;/code&gt;, but a look through the &lt;code&gt;calendar&lt;/code&gt; subdirectory yielded
nothing. Googling &lt;code&gt;ECacheObjects&lt;/code&gt; curiously didn&amp;rsquo;t bring anything up, but a
search on GitHub did identify the offending service, &lt;code&gt;evolution-data-server&lt;/code&gt;
&lt;a href=&#34;https://github.com/GNOME/evolution-data-server/blob/3abbcce2ea/src/libebackend/e-cache.h&#34;&gt;[2]&lt;/a&gt;. Unfortunately while I was able to find something that look like
directory containing &amp;ldquo;private data&amp;rdquo; &lt;a href=&#34;https://github.com/GNOME/evolution-data-server/blob/3abbcce2ea/CMakeLists.txt#L178&#34;&gt;[3]&lt;/a&gt;, I didn&amp;rsquo;t know what build
configuration had been used and couldn&amp;rsquo;t find the folder locally.&lt;/p&gt;
&lt;p&gt;Now Linux provides a very helpful feature where it show all files currently
marked as open by a given process. To use this, I first needed to find the
process responsible for maintaining the calendar.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ps aux | grep evolution
sfinucan  2552  0.0  0.3 1717928 59912 ?       SLsl Dec07   0:02 /usr/libexec/evolution-source-registry
sfinucan  2616  0.0  0.2 1242008 50640 ?       Ssl  Dec07   0:00 /usr/libexec/evolution-calendar-factory
sfinucan  2696  0.0  0.9 2803208 182632 ?      SLl  Dec07   0:19 /usr/libexec/evolution-calendar-factory-subprocess --factory caldav [...]
sfinucan  2738  0.0  0.2 1255468 48452 ?       Sl   Dec07   0:00 /usr/libexec/evolution-calendar-factory-subprocess --factory contacts [...]
sfinucan  2754  0.0  0.2 1152708 44496 ?       Ssl  Dec07   0:00 /usr/libexec/evolution-addressbook-factory
sfinucan  2766  0.0  0.2 1329236 47984 ?       Sl   Dec07   0:00 /usr/libexec/evolution-calendar-factory-subprocess --factory local [...]
sfinucan  2787  0.0  0.2 1440644 46152 ?       Sl   Dec07   0:00 /usr/libexec/evolution-addressbook-factory-subprocess --factory local [...]
sfinucan  2925  0.0  0.2 1536780 54080 tty2    Sl+  Dec07   0:00 /usr/libexec/evolution/evolution-alarm-notify
sfinucan  9496  0.0  0.2 1443748 59028 ?       SLl  10:27   0:00 /usr/libexec/evolution-addressbook-factory-subprocess --factory google [...]
sfinucan  9503  2.0  1.6 4220880 332836 tty2   SLl+ 10:27   0:32 evolution
sfinucan 10611  0.0  0.0 119728   972 pts/1    S+   10:55   0:00 grep --color=auto evolution
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This looked promising and I took the &lt;code&gt;evolution-calendar-factory-subprocess&lt;/code&gt;
process with the &lt;code&gt;caldav&lt;/code&gt; &lt;em&gt;factory&lt;/em&gt; to be the most likely candidate. Let&amp;rsquo;s
see what this has open.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ls -l /proc/2696/fd | grep *.db
lrwx------. 1 sfinucan sfinucan 64 Dec  7 21:39 12 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/fd3d04f3a29f36ce66c87bca8ef0b4d1d0dc3577/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 13 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/853c325e65384d811be1d53e0c6d21706d810a5e/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 14 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/9ff6cfa62a76324ab004c9c4a09ecec0a96c0956/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 15 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/41464062e9943c630c2bb3171b67d4e1a2cf8a93/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 16 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/6e9502d1c38772667d06ed809e1012bb0178a62d/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 17 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/f22562ff5b1e02106f69e957a7a18513bec94cab/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 18 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/6d11aa1cdaf7e1a1c7ff83b464f319b8bf0b8b08/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 22 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/f90f25baabe8d65bb2d1d8197dac7a450bcb46e7/cache.db
lrwx------. 1 sfinucan sfinucan 64 Dec  8 10:27 23 -&amp;gt; /home/sfinucan/.cache/evolution/calendar/fd8b197130da0ca054ab698175e0b3dd16e1b52d/cache.db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That looks promising. Time to kill the various &lt;code&gt;evolution&lt;/code&gt; processes and go
fix those databases.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo pkill evolution
$ sudo pkill -9 evolution-*
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;$ $ for i in $(find . -path &amp;quot;./trash&amp;quot; -prune -o -name &amp;quot;cache.db&amp;quot; -print); do
 echo &amp;quot;$i&amp;quot;;
 sqlite3 &amp;quot;$i&amp;quot; &amp;quot;pragma integrity_check;&amp;quot;;
 done
./41464062e9943c630c2bb3171b67d4e1a2cf8a93/cache.db
ok
./9ff6cfa62a76324ab004c9c4a09ecec0a96c0956/cache.db
ok
./f22562ff5b1e02106f69e957a7a18513bec94cab/cache.db
ok
./f90f25baabe8d65bb2d1d8197dac7a450bcb46e7/cache.db
ok
./fd8b197130da0ca054ab698175e0b3dd16e1b52d/cache.db
ok
./6d11aa1cdaf7e1a1c7ff83b464f319b8bf0b8b08/cache.db
ok
./fd3d04f3a29f36ce66c87bca8ef0b4d1d0dc3577/cache.db
*** in database main ***
On tree page 2935 cell 492: Rowid 3396 out of order
On tree page 2935 cell 491: Rowid 3394 out of order
On tree page 2935 cell 490: Rowid 3392 out of order
On tree page 2935 cell 489: Rowid 3390 out of order
Page 1635: btreeInitPage() returns error code 11
On tree page 2935 cell 487: Rowid 3386 out of order
Page 1634: btreeInitPage() returns error code 11
Page 1762: btreeInitPage() returns error code 11
On tree page 2935 cell 419: Rowid 3289 out of order
Page 1243 is never used
Page 1255 is never used
Page 1263 is never used
row 1934 missing from index IDX_SUMMARY
row 1934 missing from index IDX_COMPLETED
row 1934 missing from index IDX_DUE
row 1934 missing from index IDX_OCCUREND
row 1934 missing from index IDX_OCCURSTART
row 1934 missing from index sqlite_autoindex_ECacheObjects_1
row 1938 missing from index IDX_SUMMARY
row 1938 missing from index IDX_COMPLETED
row 1938 missing from index IDX_DUE
row 1938 missing from index sqlite_autoindex_ECacheObjects_1
row 1939 missing from index IDX_SUMMARY
row 1939 missing from index IDX_COMPLETED
row 1939 missing from index IDX_DUE
row 1941 missing from index IDX_SUMMARY
row 1941 missing from index IDX_COMPLETED
row 1941 missing from index IDX_DUE
row 1941 missing from index sqlite_autoindex_ECacheObjects_1
Error: database disk image is malformed
./853c325e65384d811be1d53e0c6d21706d810a5e/cache.db
ok
./6e9502d1c38772667d06ed809e1012bb0178a62d/cache.db
ok
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We have our offending database. Now, we could simply remove this and be done
but, to be honest, I don&amp;rsquo;t really trust the rest of them now. Seeing as
everything is already stored in the cloud, I can simply delete these caches.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ rm -f .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Problem solved.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building PDFs for OpenStack documentation</title>
      <link>https://that.guru/blog/create-pdfs-for-openstack/</link>
      <pubDate>Wed, 06 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/create-pdfs-for-openstack/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve only ever really worked with HTML and man page builds for the documentation
various of &lt;a href=&#34;https://docs.openstack.org/&#34;&gt;various OpenStack projects&lt;/a&gt;. However, OpenStack uses Sphinx
across the board and Sphinx, being the awesome tool that it is, supports many
other output formats. In this instance, I was interested in &lt;em&gt;PDF&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sphinx doesn&amp;rsquo;t actually provide a native PDF builder (&lt;a href=&#34;http://www.sphinx-doc.org/en/stable/builders.html#sphinx.builders.latex.LaTeXBuilder&#34;&gt;although other packages
do&lt;/a&gt;). Instead, you have to generate LaTeX sources and then generate a PDF for
this. This means you need to have a pretty well fleshed out TeX installation.
As such, the first step is to install the dependencies.&lt;/p&gt;
&lt;h2 id=&#34;install-dependencies&#34;&gt;Install dependencies&lt;/h2&gt;
&lt;p&gt;We need a number of packages to build the LaTeX sources and convert them into a
PDF. I&amp;rsquo;m using Fedora 27 which means package names may need to be adjusted
accordingly. Do note, however, that these names should work equally well on RHEL
and RHEL derivatives like CentOS).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;texlive&lt;/code&gt; (TeX Live)&lt;/p&gt;
&lt;p&gt;Per the description of this package in the Fedora repos.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The TeX Live software distribution offers a complete TeX system for a
variety of Unix, Macintosh, Windows and other platforms. It encompasses
programs for editing, typesetting, previewing and printing of TeX
documents in many different languages, and a large collection of TeX
macros and font libraries.&lt;/p&gt;
&lt;p&gt;The distribution includes extensive general documentation about TeX, as
well as the documentation for the included software packages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the basic thing we need to actually build LaTeX documents and
convert them to PDF. The package itself includes multiple other
dependencies, either because they are necessary for a minimal install or
they form a standard library of sorts. I&amp;rsquo;m not sure which.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;latexmk&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Per the description of this package in the Fedora repos.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;latexmk is a perl script for running LaTeX the correct number of times to
resolve cross references, etc.; it also runs auxiliary programs (bibtex,
makeindex if necessary, and dvips and/or a previewer as requested).  It
has a number of other useful capabilities, for example to start a
previewer and then run latex whenever the source files are updated, so
that the previewer gives an up-to-date view of the document.  The script
runs on both UNIX and MS-WINDOWS (95, ME, XP, etc).  This script is a
corrected and improved version of the original version of latexmk.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a tool that Sphinx uses, via some makefiles it includes with the
built sources to ease converting the built LaTeX sources into PDFs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You also need a variety of other packages which aren&amp;rsquo;t included as part of the
&lt;code&gt;texlive&lt;/code&gt; installation. The base Sphinx template has the following requirements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;texlive-fncychap&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-titlesec&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-tabulary&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-framed&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-wrapfig&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-upquote&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-capt-of&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;texlive-needspace&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, &lt;code&gt;openstackdocstheme&lt;/code&gt;, the theme used for all official OpenStack
projects, provides a LaTeX template with the following dependencies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;texlive-polyglossia&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;I built this list manually by running &lt;code&gt;make&lt;/code&gt; and then installing whatever
dependency &lt;code&gt;latexmk&lt;/code&gt; complained about. There&amp;rsquo;s probably easier ways to do this,
such as installing a larger package but I&amp;rsquo;m not aware of it at this time.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;Installation of all the above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo dnf install texlive latexmk texlive-fncychap texlive-titlesec \
    texlive-tabulary texlive-framed texlive-wrapfig texlive-upquote \
    texlive-capt-of texlive-needspace texlive-polyglossia
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;get-some-documentation&#34;&gt;Get some documentation&lt;/h2&gt;
&lt;p&gt;I didn&amp;rsquo;t have specific project in mind, so I&amp;rsquo;ve decided to build the project
documentation for the &lt;code&gt;openstackdocstheme&lt;/code&gt; project. As mentioned previously,
this project provides &lt;a href=&#34;https://github.com/openstack/openstackdocstheme&#34;&gt;the theme used for all Official OpenStack projects&lt;/a&gt;.
Given that this is the base for most projects&#39; documentation, it figures that
it contains all the necessary configuration to build PDFs. If, however, you are
planning to build docs for some other project, you should ensure that this is
project is &lt;a href=&#34;https://docs.openstack.org/openstackdocstheme/latest/&#34;&gt;configured correctly for PDF builds&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To begin, clone the repo locally.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/openstack/openstackdocstheme
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once cloned, create a virtual environment and install the dependencies.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ virtualenv .venv
$ source .venv
$ pip install -e .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the case of &lt;code&gt;openstackdocstheme&lt;/code&gt;, Sphinx isn&amp;rsquo;t installed by default.
Let&amp;rsquo;s install that too.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ pip install Sphinx
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;build-the-docs&#34;&gt;Build the docs&lt;/h2&gt;
&lt;p&gt;Now that we have both our dependencies and source documentation in place, it&amp;rsquo;s
time to actually build some PDFs. Sphinx actually provides a number of helpful
targets in the Makefile generated by &lt;code&gt;sphinx-quickstart&lt;/code&gt;. However, OpenStack
projects don&amp;rsquo;t tend to use these so we&amp;rsquo;re going to take the (still exceedingly
simple) two-step process. Firstly, build the LaTeX sources. Seeing as this is a
Python project, we&amp;rsquo;re going to use the &lt;a href=&#34;http://www.sphinx-doc.org/en/stable/setuptools.html&#34;&gt;setuptools integration&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python setup.py build_sphinx -b latex
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All OpenStack projects place their documentation source and builds in
&lt;code&gt;doc/source&lt;/code&gt; and &lt;code&gt;doc/build&lt;/code&gt; respectively. The above command should generate a
number of files in &lt;code&gt;doc/build/latex&lt;/code&gt; including a very helpful makefile. We&amp;rsquo;re
going to use that now to complete the process of generating a PDF.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ make -C doc/build/latex
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should emit a &lt;strong&gt;lot&lt;/strong&gt; of text, followed by something that looks like this.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Output written on os-doc-demo.pdf (23 pages).
Transcript written on os-doc-demo.log.
=== TeX engine is &#39;XeTeX&#39;
Latexmk: Index file &#39;os-doc-demo.idx&#39; was written
Latexmk: Log file says output to &#39;os-doc-demo.pdf&#39;
Latexmk: All targets (os-doc-demo.pdf) are up-to-date
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once complete, you can find your compiled PDF in the same build directory (in
this case, &lt;code&gt;doc/build/latex/os-doc-demo.pdf&lt;/code&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying Real Time Openstack</title>
      <link>https://that.guru/blog/deploying-real-time-openstack/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/deploying-real-time-openstack/</guid>
      <description>&lt;p&gt;Recent versions of OpenStack nova have added support for real-time instances,
that is, instances that provide the determinism and performance guarantees
required by real-time applications. While this work was finally marked complete
in the OpenStack Ocata release, it built upon lots of features added in
previously releases.&lt;/p&gt;
&lt;p&gt;The below is a guide that covers a basic, single-node deployment of OpenStack
suitable for evaluating basic real-time instance functionality. We use CentOS
7, but the same instructions can be modified for RHEL 7 or Fedora, and any
CentOS-specific aspects are called out. Also note that we&amp;rsquo;re using DevStack:
you obviously shouldn&amp;rsquo;t be using this in production (I hear &lt;a href=&#34;https://www.redhat.com/en/technologies/linux-platforms/openstack-platform&#34;&gt;Red Hat OpenStack
Platform&lt;/a&gt; is pretty swell!).&lt;/p&gt;
&lt;h2 id=&#34;host-bios-configuration&#34;&gt;Host BIOS configuration&lt;/h2&gt;
&lt;p&gt;Configure your BIOS as recommended in the &lt;a href=&#34;https://rt.wiki.kernel.org/index.php/HOWTO:_Build_an_RT-application&#34;&gt;rt-wiki&lt;/a&gt; page. The most important
steps are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Disable power management, including CPU sleep states&lt;/li&gt;
&lt;li&gt;Disable hyper-threading or any option related to logical processors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are standard steps used in benchmarking as both sets of features can
result in non-deterministic behavior.&lt;/p&gt;
&lt;h2 id=&#34;host-os-configuration&#34;&gt;Host OS configuration&lt;/h2&gt;
&lt;aside class=&#34;admonition important&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;
Most of the commands below must be run with root privileges
&lt;/div&gt;
&lt;/aside&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download and install CentOS 7.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Log in as &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Most of the following steps require root privileges. While you can do this
with &lt;code&gt;sudo&lt;/code&gt;, it&amp;rsquo;s generally easier to log in as the &lt;code&gt;root&lt;/code&gt; user. Do this
now.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ su -
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enable the &lt;code&gt;rt&lt;/code&gt; repo.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/yum.repos.d/CentOS-RT.repo
# CentOS-RT.repo
#
# The Real Time (RT) repository.
#

[rt]
name=CentOS-$releasever - rt
baseurl=http://mirror.centos.org/centos/$releasever/rt/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
enabled=1
EOF

$ sudo yum update -y
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Most online guides will point you to a CERN repo for these packages. I
had no success with this as some packages were missing. However, the
steps to do this are below, just in case they&amp;rsquo;re helpful.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ wget http://linuxsoft.cern.ch/cern/centos/7/rt/CentOS-RT.repo /etc/yum.repos.d/CentOS-RT.repo
$ wget http://linuxsoft.cern.ch/cern/centos/7/os/x86_64/RPM-GPG-KEY-cern /etc/pki/rpm-gpg/RPM-GPG-KEY-cern
$ sudo yum groupinstall RT
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install dependencies.&lt;/p&gt;
&lt;p&gt;The most critical of these are &lt;code&gt;kernel-rt&lt;/code&gt; and &lt;code&gt;kernel-rt-kvm&lt;/code&gt;, but
these have dependencies of their own. When I was installing this, there was
a conflict between the version installed by default (&lt;code&gt;@anaconda&lt;/code&gt;) and the
one provided by the &lt;code&gt;rt&lt;/code&gt; repo. To resolve this, I simply removed the
conflicting version and installed the one provided by the &lt;code&gt;rt&lt;/code&gt; repo.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum remove tuned
$ yum install -y tuned-2.7.1-5.el7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this, install the aforementioned dependencies along with some required
by CentOS specifically.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum install -y centos-release-qemu-ev
$ yum install -y tuned-profiles-realtime tuned-profiles-nfv
$ yum install -y kernel-rt.x86_64 kernel-rt-kvm.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the realtime profile.&lt;/p&gt;
&lt;p&gt;We want to isolate some cores from the kernel and will use the &lt;code&gt;tuned&lt;/code&gt;
application with the profiles installed above to do this.&lt;/p&gt;
&lt;p&gt;First, dump info about your NUMA topology.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ lscpu | grep ^NUMA
NUMA node(s):          2
NUMA node0 CPU(s):     0,2,4,6,8,10
NUMA node1 CPU(s):     1,3,5,7,9,11
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This processor, an &lt;a href=&#34;https://ark.intel.com/products/81897/Intel-Xeon-Processor-E5-2609-v3-15M-Cache-1_90-GHz&#34;&gt;Intel Xeon E5-2609-v3&lt;/a&gt;, has six cores and we&amp;rsquo;ve got
two of them. We want to isolate some of these cores. CPU0 should be excluded
from the possible cores as it handles console interrupts while a second core
should be kept free for other host overhead processes. Let&amp;rsquo;s take a highly
scientific approach and isolate four of the six cores from each socket
because why not?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;isolated_cores=4-11&amp;quot; &amp;gt;&amp;gt; /etc/tuned/realtime-virtual-host-variables.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load the realtime profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ systemctl enable tuned
$ systemctl start tuned
$ tuned-adm profile realtime-virtual-host
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should confirm that the profile has been applied.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ grep tuned_params= /boot/grub2/grub.cfg
set tuned_params=&amp;quot;isolcpus=4-11 nohz=on nohz_full=4-11 intel_pstate=disable nosoftlockup&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure huge pages.&lt;/p&gt;
&lt;p&gt;First, add the following to &lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt; in &lt;code&gt;/etc/default/grub&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;default_hugepagesz=1G
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Save this configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.10.0-327.13.1.el7.x86_64
done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Because we&amp;rsquo;re using a number of CPUs from each NUMA node, we want to assign
a number hugepages to each node. We&amp;rsquo;re going to assign four per node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo 4 &amp;gt; /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages
$ echo 4 &amp;gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We want to make this persistent. While you can configure persistent
hugepages via the &lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt; option, you cannot do this on a
per-NUMA node basis. We&amp;rsquo;re going to use our own &lt;code&gt;systemd&lt;/code&gt; unit files to
solve this problem until such a time as &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=1232350&#34;&gt;bug #1232350&lt;/a&gt; is resolved. This
solution is taken from that bug.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cat &amp;lt;&amp;lt; EOF &amp;gt; /usr/lib/systemd/system/hugetlb-gigantic-pages.service
[Unit]
Description=HugeTLB Gigantic Pages Reservation
DefaultDependencies=no
Before=dev-hugepages.mount
ConditionPathExists=/sys/devices/system/node
ConditionKernelCommandLine=hugepagesz=1G

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=/usr/lib/systemd/hugetlb-reserve-pages

[Install]
WantedBy=sysinit.target
EOF

$ cat &amp;lt;&amp;lt; EOF &amp;gt; /usr/lib/systemd/hugetlb-reserve-pages
#!/bin/bash

nodes_path=/sys/devices/system/node/
if [ ! -d $nodes_path ]; then
  echo &amp;quot;ERROR: $nodes_path does not exist&amp;quot;
  exit 1
fi

reserve_pages()
{
  echo $1 &amp;gt; $nodes_path/$2/hugepages/hugepages-1048576kB/nr_hugepages
}

reserve_pages 4 node0
reserve_pages 4 node1
EOF

$ chmod +x /usr/lib/systemd/hugetlb-reserve-pages
$ systemctl enable hugetlb-gigantic-pages
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reboot the host to apply changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify that changes have been applied.&lt;/p&gt;
&lt;p&gt;You want to ensure the &lt;code&gt;tuned&lt;/code&gt; profile is loaded and the changes it has made
have taken effect, such as adding &lt;code&gt;isolcpus&lt;/code&gt; and related parameters to the
boot command. In addition, you want to make sure your own hugepage
configuration has been applied.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tuned-adm active
Current active profile: realtime-virtual-host

$ cat /proc/cmdline
BOOT_IMAGE=/vmlinuz-3.10.0-327.18.2.rt56.223.el7_2.x86_64 root=/dev/mapper/rhel_virtlab502-root ro crashkernel=auto rd.lvm.lv=rhel_virtlab502/root rd.lvm.lv=rhel_virtlab502/swap console=ttyS1,115200 default_hugepagesz=1G isolcpus=3,5,7 nohz=on nohz_full=3,5,7 intel_pstate=disable nosoftlockup

$ cat /sys/module/kvm/parameters/lapic_timer_advance_ns
1000  # this should be a non-0 value

$ cat /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages
4
$ cat /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages
4
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify that system interrupts are disabled.&lt;/p&gt;
&lt;p&gt;You should install the &lt;code&gt;rt-tests&lt;/code&gt; package, then run the &lt;code&gt;hwlatdetect&lt;/code&gt;
utility it provides to validate correct behavior.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum install -y rt-tests

$ hwlatdetect
      hwlatdetect:  test duration 120 seconds
   parameters:
        Latency threshold: 10us
        Sample window:     1000000us
        Sample width:      500000us
     Non-sampling period:  500000us
        Output File:       None

Starting test
test finished
Max Latency: 0us
Samples recorded: 0
Samples exceeding threshold: 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If this shows any samples exceeding threshold, something is wrong and you
should retrace your steps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify &amp;ldquo;real-time readiness&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rteval&lt;/code&gt; utility can be used to evaluate system suitability for RT
Linux. It must be run for a long duration, so you should put this running
and come back to it later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum install rteval

$ rteval --onlyload --duration=4h --verbose
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;guest-image-configuration&#34;&gt;Guest image configuration&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re going to need a real-time image for the guest too. I did this manually on
another machine using &lt;code&gt;virt-install&lt;/code&gt;. Much of the configuration is duplicated
from the host.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Boot the guest and configure it using a &lt;code&gt;root&lt;/code&gt; user.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t actually care about most of the configuration here wrt to RAM and
CPU count since we&amp;rsquo;ll be changing this later. The only things to note are
that we&amp;rsquo;re using the same OS as the host (CentOS) for ease-of-use and we
have both network connectivity (so we can install packages) and a serial
connection (so we can interact with the guest).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo virt-install \
    --name centos7 \
    --ram 4096 \
    --disk path=./centos7.qcow2,size=8 \
    --vcpus 4 \
    --os-type linux \
    --os-variant centos7.0 \
    --network bridge=virbr0 \
    --graphics none \
    --console pty,target_type=serial \
    --location &#39;http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1708.iso&#39; \
    --extra-args &#39;console=ttyS0,115200n8 serial&#39;

# ... follow prompts
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enable the &lt;code&gt;rt&lt;/code&gt; repo.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo &amp;lt;&amp;lt; EOF &amp;gt; /etc/yum.repos.d/CentOS-RT.repo
# CentOS-RT.repo
#
# The Real Time (RT) repository.
#

[rt]
name=CentOS-$releasever - rt
baseurl=http://mirror.centos.org/centos/$releasever/rt/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
enabled=1
EOF

$ sudo yum update -y
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install dependencies.&lt;/p&gt;
&lt;p&gt;We naturally don&amp;rsquo;t need the &lt;code&gt;kernel-rt-kvm&lt;/code&gt; module, but we do need the
&lt;code&gt;kernel-rt&lt;/code&gt; package and some other dependencies. Seeing as we&amp;rsquo;re using
CentOS for the guest too, we have to deal with the same &lt;code&gt;tuned&lt;/code&gt; dependency
conflict.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum remove tuned
$ yum install -y tuned-2.7.1-5.el7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this, install the aforementioned dependencies along with some required
by CentOS specifically.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum install -y centos-release-qemu-ev
$ yum install -y tuned-profiles-realtime tuned-profiles-nfv
$ yum install -y kernel-rt.x86_64
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the realtime profile.&lt;/p&gt;
&lt;p&gt;Configure the &lt;code&gt;tuned&lt;/code&gt; profile toisolate the two CPUs we reserved for
real-time in the flavour (i.e. &lt;code&gt;^0-1&lt;/code&gt;, so &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo &amp;quot;isolated_cores=2,3&amp;quot; &amp;gt;&amp;gt; /etc/tuned/realtime-virtual-guest-variables.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load the realtime profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ systemctl enable tuned
$ systemctl start tuned
$ tuned-adm profile realtime-virtual-guest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note that we&amp;rsquo;re using the &lt;em&gt;guest&lt;/em&gt; profile here - not the &lt;em&gt;host&lt;/em&gt; one.&lt;/p&gt;
&lt;p&gt;You should confirm that the profile has been applied.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ grep tuned_params= /boot/grub2/grub.cfg
set tuned_params=&amp;quot;isolcpus=2,3 nohz=on nohz_full=2,3 rcu_nocbs=2,3 intel_pstate=disable nosoftlockup&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure hugepages.&lt;/p&gt;
&lt;p&gt;First, add the following to &lt;code&gt;GRUB_CMDLINE_LINUX&lt;/code&gt; in &lt;code&gt;/etc/default/grub&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;default_hugepagesz=1G
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Save this configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ grub2-mkconfig -o /boot/grub2/grub.cfg
Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.10.0-327.13.1.el7.x86_64
done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We don&amp;rsquo;t need to enable these as this will be done from the OpenStack side.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install testing dependencies.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re going to be doing some testing later. Best to install these
dependencies now.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum install -y epel-release
$ yum install -y rt-tests stress
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reboot the guest to apply changes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify the changes have been applied.&lt;/p&gt;
&lt;p&gt;Once again, you want to ensure the &lt;code&gt;tuned&lt;/code&gt; profile is loaded and applied,
and that the hugepages have been configured.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tuned-adm active
Current active profile: realtime-virtual-guest

$ uname -a
Linux guest.localdomain 3.10.0-693.2.2.rt56.623.el7.x86_64 #1 SMP PREEMPT RT Sun Jan 01 00:00:00 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

$ cat /proc/cmdline
BOOT_IMAGE=/vmlinuz-3.10.0-693.2.2.rt56.623.el7.x86_64 root=/dev/mapper/centos-root ro rd.lvm.lv=centos/root rd.lvm.lv=centos/swap console=ttyS0,115200n8 default_hugepagesz=1G isolcpus=2,3 nohz=on nohz_full=2,3 rcu_nocbs=2,3 intel_pstate=disable nosoftlockup
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install OpenStack-specific dependencies.&lt;/p&gt;
&lt;p&gt;We want to use &lt;code&gt;cloud-init&lt;/code&gt; to configure stuff in OpenStack, so let&amp;rsquo;s
install the various dependencies required. This is taken from &lt;a href=&#34;https://docs.openstack.org/image-guide/centos-image.html&#34;&gt;the OpenStack
docs&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ yum install -y acpid
$ systemctl enable acpid
$ yum install -y cloud-init cloud-utils-growpart
$ echo &amp;quot;NOZEROCONF=yes&amp;quot; &amp;gt;&amp;gt; /etc/sysconfig/network
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We don&amp;rsquo;t need to configure a console interface as &lt;code&gt;virt-install&lt;/code&gt; has already
done this for us.&lt;/p&gt;
&lt;p&gt;Once this is done, you can shutdown the guest.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ poweroff
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clean up the image.&lt;/p&gt;
&lt;p&gt;We want to strip stuff like MAC addresses from the guest. This should be
done wherever you ran &lt;code&gt;virt-install&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo virt-sysprep -d centos7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If this is successful, you can undefine and shrink the image. It&amp;rsquo;s now ready
for use later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo virsh undefine centos7
$ sudo qemu-img convert -O qcow2 -c centos7.qcow2 centos7-small.qcow2
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;nova-configuration&#34;&gt;Nova configuration&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Log back into your standard user.&lt;/p&gt;
&lt;p&gt;We no longer need to run as root and DevStack, which I&amp;rsquo;m using here, will
refuse to run this way.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install and configure OpenStack.&lt;/p&gt;
&lt;p&gt;I used DevStack for this, though you can use anything you want. This feature
relies on features first included in the Pike release so you should deploy a
suitable version. Given that I&amp;rsquo;m using DevStack, I&amp;rsquo;m simply going to use the
&lt;code&gt;stable/pike&lt;/code&gt; variant of DevStack and all dependencies.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/openstack-dev/devstack/
$ cd devstack
$ git checkout stable/pike

$ echo &amp;lt;&amp;lt; EOF &amp;gt; local.conf
[[local|localrc]]
GLANCE_V1_ENABLED=False

CINDER_BRANCH=stable/pike
GLANCE_BRANCH=stable/pike
HORIZON_BRANCH=stable/pike
KEYSTONE_BRANCH=stable/pike
NEUTRON_BRANCH=stable/pike
NEUTRON_FWAAS_BRANCH=stable/pike
NOVA_BRANCH=stable/pike
SWIFT_BRANCH=stable/pike

ADMIN_PASSWORD=password
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
HORIZON_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD

[[post-config|$NOVA_CONF]]
[DEFAULT]
firewall_driver=nova.virt.firewall.NoopFirewallDriver
scheduler_default_filters=RamFilter,ComputeFilter,AvailabilityZoneFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,PciPassthroughFilter,NUMATopologyFilter
vcpu_pin_set=4-11
EOF

$ ./stack.sh  # wait for successful deployment

$ . openrc admin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can use a mostly stock configuration with the exception of one
configuration option: &lt;code&gt;[DEFAULT] vcpu_pin_set&lt;/code&gt;. This should be configured
for the &lt;code&gt;nova-compute&lt;/code&gt; service and should be set to the mask configured by
&lt;code&gt;tuned&lt;/code&gt; earlier.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate deployment.&lt;/p&gt;
&lt;p&gt;Once this has deployed, you can check the logs of the &lt;code&gt;nova-compute&lt;/code&gt; service
to make sure the &lt;code&gt;vcpu_pin_set&lt;/code&gt; configuration has been successful.  If
deploying using &lt;code&gt;stable/pike&lt;/code&gt; DevStack, you can do this using &lt;code&gt;journalctl&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo journalctl -u devstack@n-cpu.service | grep &#39;vcpu_pin_set&#39; | tail -1
vcpu_pin_set = 4-11
$ sudo journalctl -u devstack@n-cpu.service | grep &#39;Total usable vcpus&#39; | tail -1
Total usable vcpus: 8, total allocated vcpus: 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is as expected, given that we we were using a &lt;code&gt;4-11&lt;/code&gt; mask and have not
yet deployed any instances.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m sure there&amp;rsquo;s a better way to do this filtering with &lt;code&gt;journalctl&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure flavor.&lt;/p&gt;
&lt;p&gt;Once you&amp;rsquo;ve verified everything, you can create your custom real-time
flavor. To do this, first configure your environment variables.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack flavor create --vcpus 4 --ram 4096 --disk 20 rt1.small
$ openstack flavor set rt1.small \
    --property &#39;hw:cpu_policy=dedicated&#39; \
    --property &#39;hw:cpu_realtime=yes&#39; \
    --property &#39;hw:cpu_realtime_mask=^0-1&#39; \
    --property &#39;hw:mem_page_size=1GB&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;By way of an explanation, these various properties correspond to the
following.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hw:cpu_policy=dedicated&lt;/code&gt;: This indicates that instances must have
exclusive pCPUs assigned to them.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hw:cpu_realtime=yes&lt;/code&gt;: This indicates that instances will have a real-time
policy.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hw:cpu_realtime_mask=&amp;quot;^0-1&amp;quot;&lt;/code&gt;: This indicates that all instance vCPUs
&lt;em&gt;except&lt;/em&gt; vCPUs 0 and 1 will have a real-time policy.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hw:mem_page_size=1GB&lt;/code&gt;: This indicates that instances will have a sole 1
GB huge page assigned to them.&lt;/p&gt;
&lt;p&gt;For more information, refer to &lt;a href=&#34;https://docs.openstack.org/nova/latest/user/flavors&#34;&gt;the nova docs&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure image.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re going to use the &lt;code&gt;centos7-small.qcow2&lt;/code&gt; created previously. Upload this
to &lt;code&gt;glance&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack image create --disk-format qcow2 --container-format bare \
    --public --file ./centos-small.qcow2 centos-rt
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Optional) Configure security groups and keypairs.&lt;/p&gt;
&lt;p&gt;We want to ensure we can both ping the instance and SSH into it. This
requires ICMP and TCP port 22 rules in the security group for the project.
This is necessary because I installed using DevStack but may not be
necessary using other deployment tools.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo OS_PROJECT_NAME
demo
$ openstack project list | grep -w demo
| f5a2496e6edf4ef4b5ffe62b01a8bf4b | demo               |
$ openstack security group list | grep -w f5a2496e6edf4ef4b5ffe62b01a8bf4b
| 466ffc5e-114d-43a4-8854-db490c6b4571 | default | Default security group | f5a2496e6edf4ef4b5ffe62b01a8bf4b |

$ openstack security group rule create --proto icmp \
    466ffc5e-114d-43a4-8854-db490c6b4571
$ openstack security group rule create --proto tcp --dst-port 22 \
    466ffc5e-114d-43a4-8854-db490c6b4571
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In addition, we want to create a keypair so we can ssh into the instance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack keypair create --public-key .ssh/id_rsa.pub default-key
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;
&lt;p&gt;Now we have everything configured, we&amp;rsquo;re going to create an instance and run
our tests.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Boot instance&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack server create --flavor rt1.small --image centos-rt \
    --key-name default-key rt-server
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This initially failed for me with the following error message:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Could not access KVM kernel module: Permission denied
failed to initialize KVM: Permission denied
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I was able to resolve this with the following commands, taken from &lt;a href=&#34;https://bugzilla.redhat.com/show_bug.cgi?id=950436&#34;&gt;a
related bugzilla&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo rmmod kvm_intel
$ sudo rmmod kvm
$ sudo modprobe kvm
$ sudo modprobe kvm_intel
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Connect floating IP.&lt;/p&gt;
&lt;p&gt;This is necessary so we can SSH into the instance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack floating ip create public
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| created_at          | 2017-01-01T00:00:00Z                 |
| description         |                                      |
| fixed_ip_address    | None                                 |
| floating_ip_address | 172.24.4.9                           |
| floating_network_id | 5e123439-bbe8-479b-ab32-cc66d1a34ae2 |
| id                  | cb62400c-983f-4468-949c-a64fb6b47827 |
| name                | 172.24.4.9                           |
| port_id             | None                                 |
| project_id          | f5a2496e6edf4ef4b5ffe62b01a8bf4b     |
| revision_number     | 0                                    |
| router_id           | None                                 |
| status              | DOWN                                 |
| updated_at          | 2017-01-01T00:00:00Z                 |
+---------------------+--------------------------------------+

$ openstack server add floating ip rt-server 172.24.4.9
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SSH to guest.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openstack server ssh rt-server --login centos
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run &lt;code&gt;cyclictest&lt;/code&gt; to confirm expected latencies.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re going to run a intensive process, &lt;code&gt;stress&lt;/code&gt;, and then use &lt;code&gt;cyclictest&lt;/code&gt;
to confirm that guest latencies are within expected limits.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ taskset -c 2 stress --cpu 4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will result in four processes running on vCPU 2. Once this is running,
start &lt;code&gt;cyclictest&lt;/code&gt; in another tab.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ taskset -c 2 cyclictest -m -n -q -p95 -D 24h -h100 -i 200 &amp;gt; cyclictest.out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will run for 24 hours (&lt;code&gt;-D 24h&lt;/code&gt;). Once done, you can check the output
(in &lt;code&gt;cyclictest.out&lt;/code&gt;) to see if it&amp;rsquo;s within expected tolerances. The RT Wiki
&lt;a href=&#34;https://rt.wiki.kernel.org/index.php/CONFIG_PREEMPT_RT_Patch#Platforms_Tested_and_in_Use_with_CONFIG_PREEMPT_RT&#34;&gt;lists some example latencies so you can get an idea of what you can
expect&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://events.linuxfoundation.org/sites/events/files/slides/cyclictest.pdf&#34;&gt;http://events.linuxfoundation.org/sites/events/files/slides/cyclictest.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://events.static.linuxfound.org/images/stories/slides/elc2013_rostedt.pdf&#34;&gt;https://events.static.linuxfound.org/images/stories/slides/elc2013_rostedt.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://linuxgizmos.com/adding-real-time-to-linux-with-preempt-rt/&#34;&gt;http://linuxgizmos.com/adding-real-time-to-linux-with-preempt-rt/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/q/26311757/&#34;&gt;https://stackoverflow.com/q/26311757/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Patchwork and CI in a Tree</title>
      <link>https://that.guru/blog/patchwork-and-ci-in-a-tree/</link>
      <pubDate>Sun, 12 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/patchwork-and-ci-in-a-tree/</guid>
      <description>&lt;p&gt;This has been a long time in the works.&lt;/p&gt;
&lt;p&gt;With the upcoming release of &lt;a href=&#34;https://github.com/getpatchwork/patchwork/&#34;&gt;Patchwork&lt;/a&gt; 2.0, Patchwork will provides first
class support for series, or collections of patches, and expose these (and much
more besides) over a new REST API. Coupled with the &lt;a href=&#34;https://github.com/getpatchwork/patchwork/releases/tag/v1.1.0&#34;&gt;Check support added in
1.1&lt;/a&gt;, we will be able to use Patchwork with continuous integration and
automated testing tools like Jenkins to validate projects using the mailing
list workflow.&lt;/p&gt;
&lt;p&gt;Below is the result of my own experiments coupling Patchwork with Jenkins.  Be
aware that this represents but the very basics of what one can do with this
functionality. However, I aim to build upon this later and this should still
serve to illustrate most of the key concepts. An even simpler guide will be
available in the Patchwork documentation shortly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This guide has been updated since publication to reflect the changes
found in Patchwork 2.0.&lt;/p&gt;
&lt;h2 id=&#34;initializing-services&#34;&gt;Initializing Services&lt;/h2&gt;
&lt;p&gt;Before we begin, we need to install both Patchwork and Jenkins. I chose to use
Docker for this, though Patchwork can also be installed manually.&lt;/p&gt;
&lt;h3 id=&#34;patchwork&#34;&gt;Patchwork&lt;/h3&gt;
&lt;p&gt;First up, Patchwork. Clone the Patchwork repo and bring up the project, as
described in the &lt;a href=&#34;https://patchwork.readthedocs.io/en/latest/development/installation/&#34;&gt;docs&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ git clone https://github.com/getpatchwork/patchwork

$ cd patchwork
$ docker-compose build
$ docker-compose up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Leave this running and jump to another tab.&lt;/p&gt;
&lt;p&gt;You should also create an superuser that you can use manage the instance. I
used &lt;code&gt;admin&lt;/code&gt; and &lt;code&gt;tester&lt;/code&gt; for username and password, respectively:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker-compose run --rm web python manage.py createsuperuser
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once initialized, you should be able to browse Patchwork at &lt;code&gt;localhost:8000&lt;/code&gt;.
A default &lt;code&gt;Patchwork&lt;/code&gt; project will have been created, which we will use for
this demo.&lt;/p&gt;
&lt;h3 id=&#34;jenkins&#34;&gt;Jenkins&lt;/h3&gt;
&lt;p&gt;Now for Jenkins. We&amp;rsquo;ll follow the instructions provided on the the &lt;a href=&#34;https://hub.docker.com/_/jenkins/&#34;&gt;Docker
Hub&lt;/a&gt; page. Run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker pull jenkins
$ docker run --name myjenkins -p 8080:8080 -p 50000:50000 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -v /var/jenkins_home jenkins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Browse to the Jenkins URL (&lt;code&gt;localhost:8080&lt;/code&gt;), where you will be asked to
authenticate, create a user, and select which plugins to install. I used
&lt;code&gt;admin&lt;/code&gt; and &lt;code&gt;tester&lt;/code&gt; for username and password once again, then chose the
following plugins:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Git plugin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parameterized Trigger plugin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Timestamper&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that you can install additional plugins if necessary but these are the
bare minimum.&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Once this step is completed, both services should be accessible: the Patchwork
instance at &lt;code&gt;localhost:8000&lt;/code&gt; and the Jenkins instance at &lt;code&gt;localhost:8080&lt;/code&gt;.
Both instances should have a user account configured, using the &lt;code&gt;admin&lt;/code&gt;
username and &lt;code&gt;tester&lt;/code&gt; password. Finally, the Patchwork instance should have the
default &lt;code&gt;Patchwork&lt;/code&gt; project. Record all of these details for future steps.&lt;/p&gt;
&lt;h2 id=&#34;configuring-services&#34;&gt;Configuring Services&lt;/h2&gt;
&lt;p&gt;Once the services are initialized, we need to configure them.&lt;/p&gt;
&lt;h3 id=&#34;patchwork-1&#34;&gt;Patchwork&lt;/h3&gt;
&lt;p&gt;There isn&amp;rsquo;t really much initial configuration necessary for Patchwork. You
should already have a user account created and Patchwork will use the default
default &lt;code&gt;Patchwork&lt;/code&gt; project. One step that &lt;em&gt;is&lt;/em&gt; necessary is to assign your
user, &lt;code&gt;admin&lt;/code&gt; in this case, as a maintainer of the &lt;code&gt;Patchwork&lt;/code&gt; project. This is
necessary to ensure we can upload test results, change the state of patches
etc. You can do this using the admin console:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8000/admin/auth/user/1/change/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to extract the IP address of the Docker container running the
Patchwork instance. This is necessary so Jenkins can communicate with the
instance. You can extract the IP address using &lt;code&gt;docker inspect&lt;/code&gt;, per &lt;a href=&#34;http://stackoverflow.com/a/20686101/613428&#34;&gt;this
StackOverflow answer&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker inspect &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -f &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    &amp;lt;CONTAINER_NAME_OR_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where &lt;code&gt;&amp;lt;CONTAINER_NAME_OR_ID&amp;gt;&lt;/code&gt; refers to the active &lt;code&gt;patchwork_web&lt;/code&gt; container.
This will return an IP address, in my case &lt;code&gt;172.17.0.3&lt;/code&gt;. Ensure you can access
the Patchwork instance via this address (including port) - for example at
&lt;code&gt;172.17.0.3:8000&lt;/code&gt; - then store this IP for later.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-1.png&#34;
         alt=&#34;Screenshot of newly configured Patchwork instance&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;A Patchwork instance fresh out of the oven&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&#34;jenkins-1&#34;&gt;Jenkins&lt;/h2&gt;
&lt;p&gt;Before beginning, you should extract your user&amp;rsquo;s &amp;ldquo;token&amp;rdquo;. We will use this to
talk to the Jenkins API in a later step. Browse to your user settings to obtain
this and copy it down somewhere:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8080/user/admin/configure
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you have this token stored, we can move onto configuring the Jenkins job.
We&amp;rsquo;re going to make heavy use of &lt;a href=&#34;https://wiki.jenkins-ci.org/display/JENKINS/Parameterized+Build&#34;&gt;Parameterized Builds&lt;/a&gt; and the &lt;a href=&#34;https://wiki.jenkins-ci.org/display/JENKINS/Remote+access+API&#34;&gt;Remote
Access API&lt;/a&gt;, both of which are part of the default Jenkins install.&lt;/p&gt;
&lt;p&gt;Create a new &amp;ldquo;freestyle&amp;rdquo; project, setting the project name and description to
something useful. You also need to define two parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;MBOX_URL&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The URL to download the patch from.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;CHECK_URL&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The URL to report the test result to.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While we could expand the available parameters to avoid hard-coding variables
and make the job more flexible, we&amp;rsquo;re going to keep it simple for now.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-2.png&#34;
         alt=&#34;Screenshot of General configuration&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;General&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Next, configure Jenkins to check out the code from the project of choice in the
&lt;em&gt;Source Code Management&lt;/em&gt; section. We&amp;rsquo;re testing Patchwork, so we&amp;rsquo;re going to
use Git and the GitHub repo. For reference, the Patchwork repo URL is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://github.com/getpatchwork/patchwork.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You don&amp;rsquo;t need to specify credentials, nor should you change the default branch
specifier. You should, however, enable the &lt;em&gt;Clean before checkout&lt;/em&gt; behavior, to
ensure old tests don&amp;rsquo;t corrupt newer tests.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-3.png&#34;
         alt=&#34;Screenshot of Source Code Management configuration&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Source Code Management&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;For the &lt;em&gt;Build Triggers&lt;/em&gt; section, we&amp;rsquo;re going to trigger this remotely using
another script. Simply tick the relevant box and set an &amp;ldquo;authentication token&amp;rdquo;.
Use &lt;code&gt;hello-world&lt;/code&gt; as a token.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-4.png&#34;
         alt=&#34;Screenshot of Build Triggers configuration&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Build Triggers&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The &lt;em&gt;Build Environment&lt;/em&gt; section can be mostly ignored. Simply enable timestamps
in the build output.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-5.png&#34;
         alt=&#34;Screenshot of Build Environment configuration&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Build Environment&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Now for the juicy bit: &lt;em&gt;Build&lt;/em&gt;. You&amp;rsquo;ll want to execute four steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Tell Patchwork (via the API) that we&amp;rsquo;re kicking off the build&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download and apply the patch&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the test(s)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Report the end result to Patchwork (again, via the API)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A script provided below will do the job for you. This script will use the
username, password and IP address of the Patchwork instance, which we got
previously. Add a new &amp;ldquo;Execute Shell&amp;rdquo; build step to the &lt;em&gt;Build&lt;/em&gt; section. Paste
the script provided below there, updating the IP address or credentials where
necessary:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/env bash
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Constants&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Patchwork credentials&lt;/span&gt;

readonly PATCHWORK_URL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.17.0.3:8000&amp;#34;&lt;/span&gt;
readonly PATCHWORK_USER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
readonly PATCHWORK_PASS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tester&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Functions&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; submit_check&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# Submit a check to Patchwork&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# Args:&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#   $1 - the state to register&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;#   $2 - an optional description&lt;/span&gt;
  state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$1&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
  description&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$2&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;

  curl -X POST &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -u &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$PATCHWORK_USER&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;$PATCHWORK_PASS&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -F &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;state=&lt;/span&gt;$state&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -F &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;target_url=&lt;/span&gt;$BUILD_URL&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -F &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;context=&lt;/span&gt;$JOB_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -F &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;description=&lt;/span&gt;$description&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$CHECK_URL&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Main&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Sanity checks&lt;/span&gt;

echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MBOX_URL=&lt;/span&gt;$MBOX_URL&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;CHECK_URL=&lt;/span&gt;$CHECK_URL&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;JOB_NAME=&lt;/span&gt;$JOB_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BUILD_URL=&lt;/span&gt;$BUILD_URL&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Notify the user that the build is starting&lt;/span&gt;
submit_check &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pending&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Navigate to the Patchwork directory&lt;/span&gt;
cd patchwork &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; exit

&lt;span style=&#34;color:#75715e&#34;&gt;# Configure fake git credentials so we can use git-am&lt;/span&gt;
git config user.name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;John Doe&amp;#39;&lt;/span&gt;
git config user.email &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;john.doe@example.org&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Simply download and apply the mbox to ensure it still applies&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ! curl &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$MBOX_URL&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; | git am -3; &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt;
  submit_check &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;
  exit &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
  submit_check &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;success&amp;#34;&lt;/span&gt;
  exit &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You&amp;rsquo;ll notice we&amp;rsquo;re really not taking advantage of Jenkins&#39; power here but
bear with me: these are baby steps.&lt;/p&gt;
&lt;p&gt;Once done, you should have something like the following:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-6.png&#34;
         alt=&#34;Screenshot of Build configuration&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Build&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Finally, the &lt;em&gt;Post-build Actions&lt;/em&gt;. I didn&amp;rsquo;t make use of this section, though
I&amp;rsquo;m sure the above script could be broken up to make use of this. Some other
time, perhaps.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://that.guru/media/patchwork-and-ci-in-a-tree-7.png&#34;
         alt=&#34;Screenshot of Post-build Actions&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Post-build Actions&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;summary-1&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;Once this step is complete, you should have configured a basic Jenkins job that
can be kicked off using the API. This job uses the credentials and IP of the
Patchwork instance. You&amp;rsquo;ll also have a token for your Jenkins user, which we
will use to talk to the Jenkins API.&lt;/p&gt;
&lt;h2 id=&#34;poll-patchwork-and-kick-of-jenkins-builds&#34;&gt;Poll Patchwork and Kick of Jenkins Builds&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re going to use a rather simple Bash script to kick of the Jenkins build. We
could do the exact same thing in Python (it would likely be more robust), but
Bash makes for a good first pass. The script consists of a number of different
parts: all should be combined to produce the final scripts.&lt;/p&gt;
&lt;p&gt;Note that the below script(s) makes use of the &lt;code&gt;jq&lt;/code&gt; tool to parse JSON. This is
not part of the default install on many distros, and may need to be installed
manually. You should do this now. For example, on Fedora 25 run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo dnf install jq
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First up in the script: Jenkins and Patchwork credentials. We&amp;rsquo;re going to
hard-code these for now, using the Jenkins username and token and Patchwork
username and password we configured/collected earlier. We&amp;rsquo;re also going to
create some function stubs, which we will populate later.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Constants&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Patchwork credentials&lt;/span&gt;

readonly PATCHWORK_URL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.17.0.3:8000&amp;#34;&lt;/span&gt;
readonly PATCHWORK_USER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
readonly PATCHWORK_PASS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tester&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Jenkins credentials&lt;/span&gt;

readonly JENKINS_URL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;localhost:8080&amp;#34;&lt;/span&gt;
readonly JENKINS_USER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
readonly JENKINS_TOKEN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;962ccd82ce467804ec7e465602381d12&amp;#34;&lt;/span&gt;
readonly JENKINS_CRUMB&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;curl -s &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;JENKINS_USER&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;JENKINS_TOKEN&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;@&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;JENKINS_URL&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\&amp;#34;:\&amp;#34;,//crumb)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Job configuration&lt;/span&gt;

readonly JOB_NAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;checkstyle&amp;#34;&lt;/span&gt;
readonly JOB_TOKEN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hello-world&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Functions&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; submit_job&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# TODO&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that Jenkins requires that you supply a &amp;ldquo;crumb&amp;rdquo; with all requests to
prevent CSRF, so we gather that now. More information is provided in &lt;a href=&#34;http://stackoverflow.com/a/38314369/613428&#34;&gt;this
StackOverflow question&lt;/a&gt; and in the &lt;a href=&#34;https://wiki.jenkins-ci.org/display/JENKINS/Remote+access+API&#34;&gt;remote access API wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next, the polling of &lt;code&gt;/events&lt;/code&gt;. This is an endpoint in the Patchwork REST API
that reports events related to elements like patches, series or checks. A list
of all supported events is &lt;a href=&#34;#&#34;&gt;provided in the docs&lt;/a&gt; but there are two that we
care about here: &lt;code&gt;patch-created&lt;/code&gt; and &lt;code&gt;patch-complete&lt;/code&gt;. Per the docs, the first
of these occurs when a patch is added to Patchwork. The second, meanwhile,
occurs when all dependencies (if any) are met. This means that if a two patch
series is sent to Patchwork and the second patch is received first, only the
&lt;code&gt;patch-created&lt;/code&gt; event will be created for this patch. Only when the first patch
in that series (the second patch&amp;rsquo;s only dependency) is received will the
&lt;code&gt;patch-completed&lt;/code&gt; be raised for the second patch (the &lt;code&gt;patch-created&lt;/code&gt; and
&lt;code&gt;patch-completed&lt;/code&gt; events will be raised at the same time for the first patch,
given that it has no dependencies per se).&lt;/p&gt;
&lt;p&gt;The below section of the script deals with reading these events from the
Patchwork API.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Main&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;##################################################&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# Pull in latest events&lt;/span&gt;

response&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;curl -s -u &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$PATCHWORK_USER&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;$PATCHWORK_PASS&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Host: &lt;/span&gt;$PATCHWORK_URL&lt;span style=&#34;color:#e6db74&#34;&gt; \
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  http://&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;PATCHWORK_URL&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/api/1.0/events/?category=patch-completed)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The use of the &lt;code&gt;Host:&lt;/code&gt; header is important - without this, responses
will be returned using &lt;code&gt;localhost&lt;/code&gt; as the host name. This won&amp;rsquo;t be usable from
another container.&lt;/p&gt;
&lt;p&gt;The response from the &lt;code&gt;/events&lt;/code&gt; API will include links to the created patch
and, for the &lt;code&gt;patch-completed&lt;/code&gt; event, the patch series which is providing the
dependencies. We must retrieve the patch and series from the Patchwork API.
Note that, in a future version of the API, we should support an &lt;code&gt;embed&lt;/code&gt;
parameter that would allow us to embed the patch and series in the response and
avoid these additional requests.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Extract patch and series URLs from each event, and create jobs based on&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# these&lt;/span&gt;

patches_series&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$response&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; | jq -rc &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.[] | .payload.patch.url + &amp;#34;,&amp;#34; + .payload.series.id&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$patches_series&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; | &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; IFS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;, read -r patch series; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt;
  echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Submitting job&amp;#34;&lt;/span&gt;
  echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Patch URL: &lt;/span&gt;$patch&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
  echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Series ID: &lt;/span&gt;$series&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;

  submit_job &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$patch&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$series&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have the &lt;code&gt;patch_url&lt;/code&gt; and &lt;code&gt;series_id&lt;/code&gt;, we can submit the job to Jenkins.
We&amp;rsquo;re going to do this via a call to the &lt;code&gt;submit_job&lt;/code&gt; function, which we
already defined a stub for. Implementing this function is rather simple, given
that we have already configured the job and gathered all required parameters
and credentials. Replace the above stub with the below function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; submit_job&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
  local patch_url
  local series_id
  local mbox_url
  local build_url

  patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;curl &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$1&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
  series_id&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$2&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;

  mbox_url&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;echo $patch | jq -rc &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.mbox&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;?series=&lt;/span&gt;$series_id&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
  check_url&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;echo $patch | jq -rc &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.checks&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
  build_url&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://&lt;/span&gt;$JENKINS_USER&lt;span style=&#34;color:#e6db74&#34;&gt;:&lt;/span&gt;$JENKINS_TOKEN&lt;span style=&#34;color:#e6db74&#34;&gt;@&lt;/span&gt;$JENKINS_URL&lt;span style=&#34;color:#e6db74&#34;&gt;/job/&lt;/span&gt;$JOB_NAME&lt;span style=&#34;color:#e6db74&#34;&gt;/build&amp;#34;&lt;/span&gt;

  echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Submitting job: &lt;/span&gt;$build_url&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
  echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Job parameters: PATCH_MBOX_URL=&lt;/span&gt;$mbox_url&lt;span style=&#34;color:#e6db74&#34;&gt;, CHECK_URL=&lt;/span&gt;$check_url&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;

  curl -X POST &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$JENKINS_CRUMB&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --data token&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$JOB_TOKEN&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --data-urlencode json&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      &amp;#34;parameter&amp;#34;: [
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        {&amp;#34;name&amp;#34;:&amp;#34;MBOX_URL&amp;#34;, &amp;#34;value&amp;#34;:&amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$mbox_url&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#34;},
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        {&amp;#34;name&amp;#34;:&amp;#34;CHECK_URL&amp;#34;, &amp;#34;value&amp;#34;:&amp;#34;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$check_url&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#34;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      ]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    }&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$build_url&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;

  echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Job submitted&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are a two important points to note related to how the Jenkins API works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Jenkins API requires parameters be passed via a JSON body rather than
than individual fields. This differs from Patchwork.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Jenkins API requires the inclusion of a &lt;em&gt;crumb&lt;/em&gt; header, by way of the
&lt;code&gt;JENKINS_CRUMB&lt;/code&gt; that we stored earlier. This is intended to prevent CSRF
attacks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More information on the Jenkins API can be found in &lt;a href=&#34;https://wiki.jenkins-ci.org/display/JENKINS/Remote+access+API&#34;&gt;remote access API
wiki&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;add-patches-to-patchwork&#34;&gt;Add Patches to Patchwork&lt;/h2&gt;
&lt;p&gt;Once done, it&amp;rsquo;s time to load in some patches. In practice, this would happen
automatically but, as this is for testing only, we&amp;rsquo;re going to once again
follow the &lt;a href=&#34;https://patchwork.readthedocs.io/en/latest/development/installation/#import-mailing-list-archives&#34;&gt;Patchwork documentation&lt;/a&gt; and make use Mailman archives. I
downloaded the archives for January 2017, like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl https://gist.github.com/stephenfin/d13183dad40f8c472234cb26777355c3 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    | gunzip &amp;gt; sample-series.mbox
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that this &lt;em&gt;must&lt;/em&gt; be downloaded to the &lt;code&gt;patchwork&lt;/code&gt; directory, else it will
not be accessible from the Docker containers.&lt;/p&gt;
&lt;p&gt;Once downloaded, use the &lt;code&gt;parsearchive&lt;/code&gt; tool provided with Patchwork to load
the archive:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ docker-compose run web python manage.py parsearchive &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --list-id&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;patchwork.ozlabs.org &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    sample-series.mbox
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Keep an eye on the &lt;code&gt;/events&lt;/code&gt; URL - you should notice a few new patch events.&lt;/p&gt;
&lt;h2 id=&#34;go-time&#34;&gt;Go Time&lt;/h2&gt;
&lt;p&gt;Run the script on your host and watch the end result. That script will read the
events stream, filter the events we care about, and kick of Jenkins builds for
the patches you just added. Jenkins will then take over, reporting that it is
starting testing to Patchwork, actually running the tests, then reporting the
end result to Patchwork. You should be able to see the jobs running in Jenkins,
and the resulting checks reported in the API.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8080/job/checkstyle/
http://localhost:8000/api/1.0/events/?category=patch-completed
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;summary-2&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;And so completes our demo. It&amp;rsquo;s rather basic, involving a lot of &amp;ldquo;shortcuts&amp;rdquo;
and tinkering. However, it does demonstrate the coming together of many of the
features available in the next version of Patchwork, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Checks&lt;/em&gt;, available in &lt;a href=&#34;https://github.com/getpatchwork/patchwork/&#34;&gt;Patchwork 1.1&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;REST API&lt;/em&gt;, available in Patchwork 2.0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Series&lt;/em&gt;, available in Patchwork 2.0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Events&lt;/em&gt;, available in Patchwork 2.0&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not all of these features are required for building an automated testing
system. For example, if you didn&amp;rsquo;t want to test series or were happy testing
individual patches, you could skip the series support. Likewise, you don&amp;rsquo;t have
to report test results to Patchwork and could poll the &lt;code&gt;/patches&lt;/code&gt; or &lt;code&gt;/series&lt;/code&gt;
endpoints instead of the &lt;code&gt;/events&lt;/code&gt; endpoint to find patches to test.  In my
opinion though, this represents a great starting point for building an
sustainable, stable, and ultimately beneficial testing infrastructure for
projects using a mailing list workflow.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s Next&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A better permissions model for Patchwork users&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/HTTP_ETag&#34;&gt;ETag&lt;/a&gt; support, allowing us to minimize requests to the API&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Better upstream documentation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Open Python Paths with vim</title>
      <link>https://that.guru/blog/open-python-paths-with-vim/</link>
      <pubDate>Mon, 06 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/open-python-paths-with-vim/</guid>
      <description>&lt;p&gt;In OpenStack nova land, you run tests by specifying their Python paths, like
so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ tox -e py27 nova.tests.unit.virt.libvirt.test_vif.LibvirtVifTestCase
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also how tests are run and reported by the CI. If a tests fails when
run locally or in the CI, the failure will be reported with similar Python
module path-style references. These take a little cleanup to get a usable file
path that we can use to open the file, so I automated it.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/stephenfin/ea6bb2d650060d2a998a2dcd41f58291.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Download and copy this to somewhere on your path, then run it like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd $openstack_nova_dir
$ pyvi nova.tests.unit.virt.libvirt.test_vif.LibvirtVifTestCase
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
