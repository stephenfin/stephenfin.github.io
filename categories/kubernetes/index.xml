<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on Stephen Finucane (Fin-oo-can)</title>
    <link>https://that.guru/categories/kubernetes/</link>
    <description>Recent content in Kubernetes on Stephen Finucane (Fin-oo-can)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-IE</language>
    <lastBuildDate>Thu, 03 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://that.guru/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A closer look at the Cinder CSI Driver and the Topology feature</title>
      <link>https://that.guru/blog/csi-drivers-and-openstack/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/csi-drivers-and-openstack/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently found myself once again working on the OpenStack Cinder CSI Driver and the Operator that OpenShift uses to
deploy this. This work has inspired me to improve my knowledge of how the Cinder CSI Driver - and CSI drivers in
general - work. Below is my current high-level understanding of both as well as a quick summary of changes we are making
to the Cinder CSI Driver Operator in OpenShift 4.19.&lt;/p&gt;
&lt;h2 id=&#34;deployment-of-the-cinder-csi-driver&#34;&gt;Deployment of the Cinder CSI Driver&lt;/h2&gt;
&lt;p&gt;The Cinder CSI Driver Operator deploys the driver itself as two components: a controller component and a per-node
component, which is the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/deploying.html&#34;&gt;typical deployment model for CSI Drivers&lt;/a&gt;. The controller component is managed
via a Deployment which you can see &lt;a href=&#34;https://github.com/openshift/csi-operator/blob/release-4.18/assets/overlays/openstack-cinder/generated/standalone/controller.yaml&#34;&gt;here&lt;/a&gt;. It consists of the controller plugin and a number of sidecar
containers which interface between the controller and the Kubernetes controller manager (&lt;code&gt;kube-controller-manager&lt;/code&gt;) via
a Unix domain socket and handle different RPC calls. Breaking these down one-by-one:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The controller plugin container (&lt;code&gt;csi-driver&lt;/code&gt;) implements the Controller Service and Identity Service set of RPCs
described in the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#rpc-interface&#34;&gt;CSI spec&lt;/a&gt;. It is responsible for handling requests by calling the cloud provider&amp;rsquo;s APIs
(Cinder and Nova, this case).&lt;/p&gt;
&lt;p&gt;You can find the Cinder CSI implementation of the Controller Service &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/release-1.32/pkg/csi/cinder/controllerserver.go&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The attacher sidecar container (&lt;code&gt;csi-attacher&lt;/code&gt;) watches for attach and detach calls and calls
&lt;code&gt;ControllerPublishVolume&lt;/code&gt; and &lt;code&gt;ControllerUnpublishVolume&lt;/code&gt;, respectively. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The provisioner sidecar container (&lt;code&gt;csi-provisioner&lt;/code&gt;) watches for PVC creation and deletion and calls &lt;code&gt;CreateVolume&lt;/code&gt;
and &lt;code&gt;DeleteVolume&lt;/code&gt;, respectively. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The snapshotter sidecar container (&lt;code&gt;csi-snapshotter&lt;/code&gt;) does the same as the provisioner but for snapshots, calling
&lt;code&gt;CreateSnapshot&lt;/code&gt; and &lt;code&gt;DeleteSnapshot&lt;/code&gt;. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The resizer sidecar container (&lt;code&gt;csi-resizer&lt;/code&gt;) watches for changes to a PVC and calls &lt;code&gt;ControllerExpandVolume&lt;/code&gt; as
necessary. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-resizer&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The per-node component, by comparison, is deployed to each node using a DaemonSet. You can see the definition for this
&lt;a href=&#34;https://github.com/openshift/csi-operator/blob/release-4.18/assets/overlays/openstack-cinder/generated/standalone/node.yaml&#34;&gt;here&lt;/a&gt;. It consists of the node plugin and a single sidecar container:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The node plugin container (&lt;code&gt;csi-driver&lt;/code&gt;) implements the Node Service and Identity Service sets of RPCs described in
the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#rpc-interface&#34;&gt;CSI spec&lt;/a&gt;. It is responsible for reporting information about the node and for bind mounting volumes
once they are attached to the host. Specifically, it reports an ID of the node, the maximum number of volumes it
supports, and topology information. In the case of Cinder, both the ID and topology information are sourced from the
metadata service, while the volume limit is determined via a configuration option.&lt;/p&gt;
&lt;p&gt;You can find the Cinder CSI implementation of the Node Service &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/release-1.32/pkg/csi/cinder/nodeserver.go&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The node-driver-registrar sidecar container (&lt;code&gt;csi-node-driver-registrar&lt;/code&gt;) registers the CSI driver with kubelet,
allowing kubelet to call &lt;code&gt;NodeGetInfo&lt;/code&gt;, &lt;code&gt;NodeStageVolume&lt;/code&gt;, &lt;code&gt;NodePublishVolume&lt;/code&gt; etc. (&lt;a href=&#34;https://github.com/kubernetes-csi/node-driver-registrar&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;changes-to-topology-auto-configuration&#34;&gt;Changes to topology auto-configuration&lt;/h2&gt;
&lt;p&gt;Now that we understand the various components that make up the CSI Driver, let&amp;rsquo;s take a look at the changes we&amp;rsquo;ve been
working on in this area. As I&amp;rsquo;ve &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;previously discussed&lt;/a&gt;, the Cinder CSI Driver has support for
Availability Zones (or, in CSI parlance, the CSI Topology Feature) and since OpenShift 4.16 or so the Cinder CSI Driver
Operator has supported auto-configuration of this feature. Without getting too into the weeds, the way this is
determined is via a simple set comparison: the set of Compute AZs is compared to the set of Block Storage AZs, and if
the former isn&amp;rsquo;t a subset of the latter (e.g. if there was a Compute AZ called &lt;code&gt;foo&lt;/code&gt; but no equivalent Block Storage AZ
of the same name) then we determine that the feature should be disabled. Once we&amp;rsquo;ve determined this, we toggle the
&lt;code&gt;Topology&lt;/code&gt; feature gate of the CSI Provisioner sidecar container, thus ensuring that the &lt;code&gt;AccessibilityRequirements&lt;/code&gt;
field of the &lt;code&gt;CreateVolumeRequest&lt;/code&gt; struct generated by the provisioner (and fed to the controller plugin) &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner/blob/release-5.1/pkg/controller/controller.go#L682-L697&#34;&gt;would not be
populated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, things change and the Topology feature is now considered mature and is enabled by default. This means it is
likely that the feature flag will be removed at some point in the not-too-distant future, which in turn means we need to
find another way to enable and disable topology support from the operator. The solution we&amp;rsquo;ve arrived at is to copy what
was done in Manila and add support for a new &lt;code&gt;--with-topology&lt;/code&gt; option to both the controller plugin and node plugin
services. This new option has different effects depending on where it is set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For the controller plugin, the option determines whether (a) the calls to Cinder include a requested AZ and (b)
whether the &lt;code&gt;CreateVolumeResponse&lt;/code&gt; returned by the &lt;code&gt;CreateVolume&lt;/code&gt; call includes topology accessibility information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the node plugin, the option determines whether the node reports (a) the capability (as part of the
&lt;code&gt;GetPluginCapabilities&lt;/code&gt; RPC) and (b) a topology information (as part of the &lt;code&gt;NodeGetInfo&lt;/code&gt; call).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This work has been implemented in &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/pull/2743&#34;&gt;kubernetes/cloud-provider-openstack#2743&lt;/a&gt; (with some follow-ups in
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/pull/2862&#34;&gt;kubernetes/cloud-provider-openstack#2862&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/pull/2865&#34;&gt;kubernetes/cloud-provider-openstack#2865&lt;/a&gt;). With
the new option in place, we&amp;rsquo;ve been able to change how the Operator toggles the Topology feature. Now, instead of
enabling and disabling the feature gate on the &lt;code&gt;csi-provisioner&lt;/code&gt; container, it can enable and disable the feature on the
&lt;code&gt;csi-driver&lt;/code&gt; containers in the controller deployment and node daemonsets. &lt;em&gt;That&lt;/em&gt; work has been implemented in
&lt;a href=&#34;https://github.com/openshift/csi-operator/pull/345&#34;&gt;openshift/csi-operator#345&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m hoping this is last time I feel the need to write about the Cinder CSI Driver and its Operator. The work we&amp;rsquo;ve done
here should future proof both and ensure that, barring major changes to the CSI Spec itself, few other changes will be
needed for the foreseeable. I would however like to get a better understanding of how the equivalent feature in the
Manila CSI Driver works, so watch our for a possible post on that topic down the line.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling the switch to the Cloud Controller Manager (CCM) with OpenShift Operators</title>
      <link>https://that.guru/blog/user-managed-configuration-in-cccmo/</link>
      <pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/user-managed-configuration-in-cccmo/</guid>
      <description>&lt;p&gt;Recent versions of Kubernetes have begun moving functionality that previously
existed in the core project out to separate projects. One such set of
functionality is the cloud provider-specific code, which is now handled by the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/cloud-controller/&#34;&gt;Cloud Controller Manager&lt;/a&gt; project. This is well described in the
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/&#34;&gt;Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;Cloud providers implemented using Cloud Controller Manager may be referred to
as &lt;em&gt;external&lt;/em&gt; cloud providers (after the argument used to inform &lt;code&gt;kublet&lt;/code&gt; to
use CCM, &lt;code&gt;--cloud-provider=external&lt;/code&gt;) while the existing in-tree cloud
providers may be referred to as &lt;em&gt;internal&lt;/em&gt; or &lt;em&gt;legacy&lt;/em&gt; cloud providers. We use
these terms throughout.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;In the 4.12 release, we hope to switch OpenShift deployments running on
OpenStack clouds from the legacy OpenStack cloud provider to the external
OpenStack cloud provider, &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/using-openstack-cloud-controller-manager.md&#34;&gt;OpenStack Cloud Controller Manager (OCCM)&lt;/a&gt;.
There are a couple of steps needed to make this happen, one of which is taking
user-provided configuration for the legacy cloud provider and mapping it to
configuration for the shiny new external cloud provider. This is necessary to
ensure any user-provided configuration is retained and the upgrade doesn&amp;rsquo;t
break the deployment. In the case of the OpenStack provider, this configuration
is INI-style and thankfully quite similar for both the legacy and external
cloud provider implementations.&lt;/p&gt;
&lt;p&gt;To handle the migration of configuration in OpenShift deployments, we are
relying on the &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator&#34;&gt;Cluster Cloud Controller Manager Operator (CCCMO)&lt;/a&gt;. This
operator is already responsible for managing the lifecycle of CCM on OpenShift
deployments, including configuration of CCM, so naturally it is a good fit for
this kind of task. A detailed description of the changes we ultimately made,
along with motivation for same, can be found &lt;a href=&#34;https://github.com/openshift/enhancements/pull/1009&#34;&gt;in this enhancement&lt;/a&gt;
(the &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/pull/178&#34;&gt;pull request itself&lt;/a&gt; is probably quite helpful also, if you read Go)
but I hope to explain them at a high level here since the paradigms used are
similar to those found in other operators and are being used to manage other
complex upgrades, such as the switch from in-tree block storage drivers to
Cluster Storage Interface (CSI) drivers.&lt;/p&gt;
&lt;h2 id=&#34;how-cccmo-generates-configuration&#34;&gt;How CCCMO generates configuration&lt;/h2&gt;
&lt;p&gt;The first step in understanding how CCCMO can be used to manage the migration
of configuration is to examine how CCCMO sources configuration - specifically
user-provided configuration - and uses this to generate the configuration
actually used for CCM. Once we understand this, we can decide at what points to
hook in and customise or translate this user-provided configuration. We can
also use this model in other operators. Thankfully, in the case of CCCMO this
sourcing and generation of configuration is pretty simple.&lt;/p&gt;
&lt;p&gt;Firstly, the operator attempt to retrieve config from the
&lt;code&gt;openshift-config-managed / kube-cloud-config&lt;/code&gt; config map:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm/kube-cloud-config -n openshift-config-managed -o yaml
apiVersion: v1
data:
  cloud.conf: |
    [Global]
    secret-name = openstack-credentials
    secret-namespace = kube-system
    region = regionOne
    [LoadBalancer]
    use-octavia = True
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2022-02-25T17:01:58Z&amp;quot;
  name: kube-cloud-config
  namespace: openshift-config-managed
  resourceVersion: &amp;quot;3853&amp;quot;
  uid: c23c14b7-66db-431c-a723-59439f946f80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/pkg/controllers/cloud_config_sync_controller.go#L63-L68&#34;&gt;here&lt;/a&gt;.
The reason that it searches for this config map specifically is historical:
this is the config map generated by the &lt;a href=&#34;https://github.com/openshift/cluster-config-operator&#34;&gt;Cluster Config Operator (CCO)&lt;/a&gt;,
which is used to configure the legacy cloud provider (among other things). CCO
manipulates user-provided configuration for some cloud providers (specifically
AWS and Azure) so I guess the idea here was to avoid re-implementing this
transformation logic in CCCMO. Everything in the &lt;code&gt;openshift-config-managed&lt;/code&gt;
namespace is owned by CCO and is not intended to be modified by a user (in
fact, attempts to modify it will likely be futile and the operator will quickly
erase those changes).&lt;/p&gt;
&lt;p&gt;If the lookup of the &lt;code&gt;openshift-config-managed / kube-cloud-config&lt;/code&gt; config map
fails, we attempt to retrieve configuration from the &lt;code&gt;openshift-config / cloud-provider-config&lt;/code&gt; config map:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm/cloud-provider-config -n openshift-config -o yaml
apiVersion: v1
data:
  config: |
    [Global]
    secret-name = openstack-credentials
    secret-namespace = kube-system
    region = regionOne
    [LoadBalancer]
    use-octavia = True
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2022-02-25T17:00:15Z&amp;quot;
  name: cloud-provider-config
  namespace: openshift-config
  resourceVersion: &amp;quot;1802&amp;quot;
  uid: 45bda3c8-8866-4aea-92be-921502ff2055
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/pkg/controllers/cloud_config_sync_controller.go#L73-L87&#34;&gt;here&lt;/a&gt;.
Once again, the reason we use this config map is historical and is based on
what CCO uses. While things in the &lt;code&gt;openshift-config-managed&lt;/code&gt; namespace are
not user editable, the &lt;code&gt;openshift-config&lt;/code&gt; namespace is the namespace for
&amp;ldquo;user-managed&amp;rdquo; configuration or configuration that things like operators are
not allowed to modify.&lt;/p&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The &lt;strong&gt;name&lt;/strong&gt; of this config map (as opposed to the namespace) is actually
cloud/infrastructure dependent and this is simply the OpenStack name. It is
defined as an attribute on the &lt;code&gt;cluster&lt;/code&gt; infrastructure resource.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get infrastructure/cluster -o jsonpath=&amp;quot;{.spec.cloudConfig.name}&amp;quot;
cloud-provider-config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;If the both lookup fails, we error out. However, this is unlikely since the
installer should create it as seen
&lt;a href=&#34;https://github.com/openshift/installer/blob/fd00a659/pkg/asset/manifests/cloudproviderconfig.go#L78-L93&#34;&gt;here&lt;/a&gt;.
Assuming one of them does exist, we sync whatever we found to the
&lt;code&gt;openshift-cloud-controller-manager / cloud-conf&lt;/code&gt; config map:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm/cloud-conf -n openshift-cloud-controller-manager -o yaml
apiVersion: v1
data:
  cloud.conf: |
    [Global]
    secret-name = openstack-credentials
    secret-namespace = kube-system
    region = regionOne
    [LoadBalancer]
    use-octavia = True
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2022-02-25T17:01:08Z&amp;quot;
  name: cloud-conf
  namespace: openshift-cloud-controller-manager
  resourceVersion: &amp;quot;2519&amp;quot;
  uid: cbbeedaf-41ed-41c2-9f37-4885732d3677
&lt;/code&gt;&lt;/pre&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
	
	
	&lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;You can list all config maps in a namespace like this using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm -n openshift-cloud-controller-manager \
    -o jsonpath=&#39;{range .items[*]}{.metadata.name}{&amp;quot;\n&amp;quot;}{end}&#39;
ccm-trusted-ca
cloud-conf
kube-root-ca.crt
openshift-service-ca.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;This can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/pkg/controllers/cloud_config_sync_controller.go#L96-L100&#34;&gt;here&lt;/a&gt;.
In this instance, the namespace isn&amp;rsquo;t actually locked in. It is possible to
configure the &lt;code&gt;cluster-controller-manager-operator&lt;/code&gt; binary with a &lt;code&gt;--namespace&lt;/code&gt;
argument and this option defaults to &lt;code&gt;openshift-cloud-controller-manager&lt;/code&gt;, as
seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/cmd/cluster-cloud-controller-manager-operator/main.go#L85-L89&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-cccmo-handles-upgrades&#34;&gt;How CCCMO handles upgrades&lt;/h2&gt;
&lt;p&gt;(for OpenStack clouds on OpenShift 4.11 or later)&lt;/p&gt;
&lt;p&gt;So now that we understand how CCCMO sources user-provided configuration and
generates the resulting configuration used by Cloud Controller Manager, it&amp;rsquo;s
time to examine how we&amp;rsquo;ve decided to handle the migration of configuration for
legacy cloud providers to configuration suitable for external cloud provides.
As noted above, previously CCCMO took user-provided configuration from a config
map in one namespace and copied it to a config map in another namespace. It
should be pretty obvious that there&amp;rsquo;s no reason this copy has to be a
straightforward copy: we could modify the input config map before we dump it
back out. This is of course exactly what we did.&lt;/p&gt;
&lt;p&gt;Starting with the upcoming OpenShift 4.11 release, CCCMO provides configuration
&amp;ldquo;transformers&amp;rdquo;. Transformers simply load configuration provided by users, do
some basic validation, and then transform things by dropping options that are
no longer relevant, adding options that are now necessary, and renaming or
modifying options that have changed between the legacy. This idea isn&amp;rsquo;t
particularly novel - as noted previously, CCO was already doing something very
similar for AWS and Azure - but it works. Annoyingly these transformers must be
cloud-specific since the CCM binary used for each cloud provider expects a
radically different configuration files (in the case of the OpenStack cloud
provider this is an INI-style configuration file while Azure expects a
YAML-formatted file). As a result, we have only implemented the OpenStack
transformer for now. However, in the future we will likely implement additional
transformers for at least AWS and Azure since as noted previously CCO is
already doing some transformation here.&lt;/p&gt;
&lt;p&gt;Specifically, the transformer for OpenStack clouds in CCCMO currently does the
following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Drops the &lt;code&gt;[Global] secret-name&lt;/code&gt;, &lt;code&gt;[Global] secret-namespace&lt;/code&gt;, and &lt;code&gt;[Global] kubeconfig-path&lt;/code&gt; options, since these aren&amp;rsquo;t applicable for the external
cloud provider (the first two are OpenShift-only modifications). This inline
configuration has been replaced by configuration stored in a &lt;code&gt;clouds.yaml&lt;/code&gt;
file. Speaking of which&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adds the &lt;code&gt;[Global] use-clouds&lt;/code&gt;, &lt;code&gt;[Global] clouds-file&lt;/code&gt;, and &lt;code&gt;[Global] cloud&lt;/code&gt;
options.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Drops the entire &lt;code&gt;[BlockStorage]&lt;/code&gt; section since external cloud providers are
no longer responsible for anything storage&amp;rsquo;y (this is now handled by Cluster
Storage Interface (CSI) drivers, including the Manila CSI driver and Cinder
CSI driver)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adds or sets the &lt;code&gt;[LoadBalancer] use-octavia&lt;/code&gt; and &lt;code&gt;[LoadBalancer] enabled&lt;/code&gt;
options, depending on the specific deployment configuration (i.e. is Kuryr in
use?)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/13a37fe2/pkg/cloud/openstack/openstack.go#L136-L213&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this helps shine a little light on how CCCMO (and to a lesser degree,
CCM and CCO) works and operates, at least from a OpenStack perspective. For
most users, none of the above should matter: the OpenShift documentation
describes how configuration of the cloud provider, be it internal or external,
should happen via the &lt;code&gt;openshift-config / cloud-provider-config&lt;/code&gt; config map and
all of this transformation logic should be effectively invisible. However, when
things go wrong, it can be helpful to know in which dark corners to look 😄&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
