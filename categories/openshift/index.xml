<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Openshift on Stephen Finucane (Fin-oo-can)</title>
    <link>https://that.guru/categories/openshift/</link>
    <description>Recent content in Openshift on Stephen Finucane (Fin-oo-can)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-IE</language>
    <lastBuildDate>Sat, 18 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://that.guru/categories/openshift/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Manage Your OpenStack Resources From Kubernetes With ORC</title>
      <link>https://that.guru/talks/manage-your-openstack-resources-from-kubernetes-with-orc/</link>
      <pubDate>Sat, 18 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/talks/manage-your-openstack-resources-from-kubernetes-with-orc/</guid>
      <description>&lt;p&gt;This talk introduced OpenStack Resource Controller (ORC), a set of controllers that allows management of OpenStack
resources using Kubernetes CRDs. It was delivered during &lt;a href=&#34;https://summit2025.openinfra.org/a/schedule/#&#34;&gt;OpenInfra Summit Europe&lt;/a&gt; in Paris in October 2025.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Managing OpenStack resources efficiently can be complex, especially when striving for cloud-native agility and GitOps
workflows. In this talk, we&amp;rsquo;ll introduce the OpenStack Resource Controller (ORC), a powerful solution combining the
benefits of Kubernetes Custom Resource Definitions (CRDs) and controllers with OpenStack&amp;rsquo;s rich and mature APIs and
application ecosystem. We&amp;rsquo;ll explore ORC&amp;rsquo;s fundamental &amp;ldquo;raison d&amp;rsquo;√™tre,&amp;rdquo; highlighting how it directly addresses common
provisioning pain points many developers and operators face. We&amp;rsquo;ll compare ORC against ‚Äúthe competition‚Äù, exploring
both its strengths and weaknesses. Discover its core goals, design philosophy, and how ORC simplifies the declarative
management of your OpenStack infrastructure, enabling seamless resource orchestration for modern cloud-native
applications.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;More information is available on the &lt;a href=&#34;https://summit2025.openinfra.org/a/schedule/#&#34;&gt;OpenInfra Summit Europe 2025&lt;/a&gt; website.&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;fe689fb7df5f470bacc5f701b2ee968c&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;


</description>
    </item>
    
    <item>
      <title>A Closer Look at the Cinder CSI Driver and the Topology Feature</title>
      <link>https://that.guru/blog/csi-drivers-and-openstack/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/csi-drivers-and-openstack/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve recently found myself once again working on the OpenStack Cinder CSI Driver and the Operator that OpenShift uses to
deploy this. This work has inspired me to improve my knowledge of how the Cinder CSI Driver - and CSI drivers in
general - work. Below is my current high-level understanding of both as well as a quick summary of changes we are making
to the Cinder CSI Driver Operator in OpenShift 4.19.&lt;/p&gt;
&lt;h2 id=&#34;deployment-of-the-cinder-csi-driver&#34;&gt;Deployment of the Cinder CSI Driver&lt;/h2&gt;
&lt;p&gt;The Cinder CSI Driver Operator deploys the driver itself as two components: a controller component and a per-node
component, which is the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/deploying.html&#34;&gt;typical deployment model for CSI Drivers&lt;/a&gt;. The controller component is managed
via a Deployment which you can see &lt;a href=&#34;https://github.com/openshift/csi-operator/blob/release-4.18/assets/overlays/openstack-cinder/generated/standalone/controller.yaml&#34;&gt;here&lt;/a&gt;. It consists of the controller plugin and a number of sidecar
containers which interface between the controller and the Kubernetes controller manager (&lt;code&gt;kube-controller-manager&lt;/code&gt;) via
a Unix domain socket and handle different RPC calls. Breaking these down one-by-one:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The controller plugin container (&lt;code&gt;csi-driver&lt;/code&gt;) implements the Controller Service and Identity Service set of RPCs
described in the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#rpc-interface&#34;&gt;CSI spec&lt;/a&gt;. It is responsible for handling requests by calling the cloud provider&amp;rsquo;s APIs
(Cinder and Nova, this case).&lt;/p&gt;
&lt;p&gt;You can find the Cinder CSI implementation of the Controller Service &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/release-1.32/pkg/csi/cinder/controllerserver.go&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The attacher sidecar container (&lt;code&gt;csi-attacher&lt;/code&gt;) watches for attach and detach calls and calls
&lt;code&gt;ControllerPublishVolume&lt;/code&gt; and &lt;code&gt;ControllerUnpublishVolume&lt;/code&gt;, respectively. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The provisioner sidecar container (&lt;code&gt;csi-provisioner&lt;/code&gt;) watches for PVC creation and deletion and calls &lt;code&gt;CreateVolume&lt;/code&gt;
and &lt;code&gt;DeleteVolume&lt;/code&gt;, respectively. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The snapshotter sidecar container (&lt;code&gt;csi-snapshotter&lt;/code&gt;) does the same as the provisioner but for snapshots, calling
&lt;code&gt;CreateSnapshot&lt;/code&gt; and &lt;code&gt;DeleteSnapshot&lt;/code&gt;. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The resizer sidecar container (&lt;code&gt;csi-resizer&lt;/code&gt;) watches for changes to a PVC and calls &lt;code&gt;ControllerExpandVolume&lt;/code&gt; as
necessary. (&lt;a href=&#34;https://github.com/kubernetes-csi/external-resizer&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The per-node component, by comparison, is deployed to each node using a DaemonSet. You can see the definition for this
&lt;a href=&#34;https://github.com/openshift/csi-operator/blob/release-4.18/assets/overlays/openstack-cinder/generated/standalone/node.yaml&#34;&gt;here&lt;/a&gt;. It consists of the node plugin and a single sidecar container:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The node plugin container (&lt;code&gt;csi-driver&lt;/code&gt;) implements the Node Service and Identity Service sets of RPCs described in
the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#rpc-interface&#34;&gt;CSI spec&lt;/a&gt;. It is responsible for reporting information about the node and for bind mounting volumes
once they are attached to the host. Specifically, it reports an ID of the node, the maximum number of volumes it
supports, and topology information. In the case of Cinder, both the ID and topology information are sourced from the
metadata service, while the volume limit is determined via a configuration option.&lt;/p&gt;
&lt;p&gt;You can find the Cinder CSI implementation of the Node Service &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/release-1.32/pkg/csi/cinder/nodeserver.go&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The node-driver-registrar sidecar container (&lt;code&gt;csi-node-driver-registrar&lt;/code&gt;) registers the CSI driver with kubelet,
allowing kubelet to call &lt;code&gt;NodeGetInfo&lt;/code&gt;, &lt;code&gt;NodeStageVolume&lt;/code&gt;, &lt;code&gt;NodePublishVolume&lt;/code&gt; etc. (&lt;a href=&#34;https://github.com/kubernetes-csi/node-driver-registrar&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;changes-to-topology-auto-configuration&#34;&gt;Changes to topology auto-configuration&lt;/h2&gt;
&lt;p&gt;Now that we understand the various components that make up the CSI Driver, let&amp;rsquo;s take a look at the changes we&amp;rsquo;ve been
working on in this area. As I&amp;rsquo;ve &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;previously discussed&lt;/a&gt;, the Cinder CSI Driver has support for
Availability Zones (or, in CSI parlance, the CSI Topology Feature) and since OpenShift 4.16 or so the Cinder CSI Driver
Operator has supported auto-configuration of this feature. Without getting too into the weeds, the way this is
determined is via a simple set comparison: the set of Compute AZs is compared to the set of Block Storage AZs, and if
the former isn&amp;rsquo;t a subset of the latter (e.g. if there was a Compute AZ called &lt;code&gt;foo&lt;/code&gt; but no equivalent Block Storage AZ
of the same name) then we determine that the feature should be disabled. Once we&amp;rsquo;ve determined this, we toggle the
&lt;code&gt;Topology&lt;/code&gt; feature gate of the CSI Provisioner sidecar container, thus ensuring that the &lt;code&gt;AccessibilityRequirements&lt;/code&gt;
field of the &lt;code&gt;CreateVolumeRequest&lt;/code&gt; struct generated by the provisioner (and fed to the controller plugin) &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner/blob/release-5.1/pkg/controller/controller.go#L682-L697&#34;&gt;would not be
populated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, things change and the Topology feature is now considered mature and is enabled by default. This means it is
likely that the feature flag will be removed at some point in the not-too-distant future, which in turn means we need to
find another way to enable and disable topology support from the operator. The solution we&amp;rsquo;ve arrived at is to copy what
was done in Manila and add support for a new &lt;code&gt;--with-topology&lt;/code&gt; option to both the controller plugin and node plugin
services. This new option has different effects depending on where it is set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For the controller plugin, the option determines whether (a) the calls to Cinder include a requested AZ and (b)
whether the &lt;code&gt;CreateVolumeResponse&lt;/code&gt; returned by the &lt;code&gt;CreateVolume&lt;/code&gt; call includes topology accessibility information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the node plugin, the option determines whether the node reports (a) the capability (as part of the
&lt;code&gt;GetPluginCapabilities&lt;/code&gt; RPC) and (b) a topology information (as part of the &lt;code&gt;NodeGetInfo&lt;/code&gt; call).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This work has been implemented in &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/pull/2743&#34;&gt;kubernetes/cloud-provider-openstack#2743&lt;/a&gt; (with some follow-ups in
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/pull/2862&#34;&gt;kubernetes/cloud-provider-openstack#2862&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/pull/2865&#34;&gt;kubernetes/cloud-provider-openstack#2865&lt;/a&gt;). With
the new option in place, we&amp;rsquo;ve been able to change how the Operator toggles the Topology feature. Now, instead of
enabling and disabling the feature gate on the &lt;code&gt;csi-provisioner&lt;/code&gt; container, it can enable and disable the feature on the
&lt;code&gt;csi-driver&lt;/code&gt; containers in the controller deployment and node daemonsets. &lt;em&gt;That&lt;/em&gt; work has been implemented in
&lt;a href=&#34;https://github.com/openshift/csi-operator/pull/345&#34;&gt;openshift/csi-operator#345&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m hoping this is last time I feel the need to write about the Cinder CSI Driver and its Operator. The work we&amp;rsquo;ve done
here should future proof both and ensure that, barring major changes to the CSI Spec itself, few other changes will be
needed for the foreseeable. I would however like to get a better understanding of how the equivalent feature in the
Manila CSI Driver works, so watch our for a possible post on that topic down the line.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Availability Zones in Openstack and Openshift (Part 2)</title>
      <link>https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2/</link>
      <pubDate>Fri, 03 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2/</guid>
      <description>
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This is part two of two. If you&amp;rsquo;re looking for part one, you can find it
&lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-1&#34;&gt;here&lt;/a&gt;.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;After seeing a few too many availability zone-related issues popping up in OpenShift clusters of late, I&amp;rsquo;ve decided it
might make sense to document the situation with OpenStack AZs on OpenShift (and, by extension, Kubernetes). This is the
second part of two. &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-1&#34;&gt;The first part&lt;/a&gt; provided some background on what AZs are and how you can configure them,
while this part will examine how AZs affect OpenShift and Kubernetes components such as the OpenStack Machine API
Provider, the OpenStack Cluster API Provider, and the Cinder and Manila CSI drivers.&lt;/p&gt;
&lt;h2 id=&#34;the-line-up&#34;&gt;The line up&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s a couple of OpenStack-specific components we need to be aware of in a typical OpenShift-on-OpenStack (a.k.a.
ShiftStack) deployment. My former colleague Micha≈Ç Dulko &lt;a href=&#34;https://dulek.github.io/2022/07/14/capo-mapo-cloud-provider.html&#34;&gt;provided a good overview of many of these on his
blog&lt;/a&gt; but to (re-)summarise, you&amp;rsquo;ve got:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cloud Provider OpenStack (CPO)&lt;/li&gt;
&lt;li&gt;Machine API Provider OpenStack (MAPO)&lt;/li&gt;
&lt;li&gt;Cluster API Provider OpenStack (CAPO)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to these three components, there are two others to consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cinder CSI Driver&lt;/li&gt;
&lt;li&gt;Manila CSI Driver&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Today we&amp;rsquo;re going to take a look at three of these five components - CPO, MAPO, and the Cinder CSI Driver - and explore
how availability zones - both Compute and Block Storage - impact them.&lt;/p&gt;
&lt;h2 id=&#34;cloud-provider-openstack&#34;&gt;Cloud Provider OpenStack&lt;/h2&gt;
&lt;p&gt;In contrast to a favoured programming language of mine üêç, Kubernetes operates on very much batteries &lt;em&gt;not&lt;/em&gt; included
model. It does not provide out-of-the-box support for such important things as block storage, networking, or ingress. To
resolve this, you normally run Kubernetes on top of another platform - be that AWS, vSphere, GCE, or in our case
OpenStack - and add additional components that provide integration between your Kubernetes cluster and said platform and
its APIs. The cloud-provider interface provides one part of the integration puzzle here, managing the lifecycle of
&lt;code&gt;Node&lt;/code&gt;s (including their removal from the cluster if the underlying instance is deleted), &lt;code&gt;Service&lt;/code&gt;s of type
&lt;code&gt;LoadBalancer&lt;/code&gt;, and routes.&lt;/p&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;If you&amp;rsquo;re interested, &lt;a href=&#34;https://medium.com/@m.json/the-kubernetes-cloud-controller-manager-d440af0d2be5&#34;&gt;this blog from Mikael Johansson&lt;/a&gt; provides a far more thorough overview of this
component.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;The OpenStack CCM uses Compute AZ information for the underlying instance to label the corresponding &lt;code&gt;Node&lt;/code&gt;. It sets two
labels, the &lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone&#34;&gt;&lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;&lt;/a&gt; label and the legacy
&lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#failure-domainbetakubernetesiozone&#34;&gt;&lt;code&gt;failure-domain.beta.kubernetes.io/zone&lt;/code&gt;&lt;/a&gt; label. You can see this if you retrieve the labels
for a &lt;code&gt;Node&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get Node -o jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{.items[*].metadata.labels}&amp;#39;&lt;/span&gt; | jq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beta.kubernetes.io/arch&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;amd64&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beta.kubernetes.io/instance-type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ci.m1.xlarge&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beta.kubernetes.io/os&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;linux&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;failure-domain.beta.kubernetes.io/region&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;regionOne&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;failure-domain.beta.kubernetes.io/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;lt;--- !!! here !!!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/arch&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;amd64&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stephenfin-5ps6d-master-0&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/os&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;linux&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node-role.kubernetes.io/control-plane&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node-role.kubernetes.io/master&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node.kubernetes.io/instance-type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ci.m1.xlarge&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node.openshift.io/os_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rhcos&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.cinder.csi.openstack.org/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/region&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;regionOne&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;lt;--- !!! and here !!!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;It also sets two corresponding labels - the &lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesioregion&#34;&gt;&lt;code&gt;topology.kubernetes.io/region&lt;/code&gt;&lt;/a&gt; label and the legacy
&lt;a href=&#34;https://kubernetes.io/docs/reference/labels-annotations-taints/#failure-domainbetakubernetesioregion&#34;&gt;&lt;code&gt;failure-domain.beta.kubernetes.io/region&lt;/code&gt;&lt;/a&gt; label - but these are sourced from
Keystone-related information.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;To fetch the AZ information, OpenStack CCM queries the Nova API. You can see that happening
&lt;a href=&#34;https://github.com/openshift/cloud-provider-openstack/blob/release-4.15/pkg/openstack/openstack.go#L411-L463&#34;&gt;here&lt;/a&gt;,
and publishes this information via the &lt;code&gt;GetZoneByProviderID&lt;/code&gt; and &lt;code&gt;GetZoneByName&lt;/code&gt; functions, which form part of the
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider/blob/v0.30.0/cloud.go#L281-L289&#34;&gt;cloud-provider API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because the labels are defined on &lt;code&gt;Node&lt;/code&gt;s, they are useful for controlling the scheduling of pods, allowing users to
spread pods across multiple AZs. This is discussed in more details in the Kubernetes docs, in &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/&#34;&gt;Assigning Pods to
Nodes&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/&#34;&gt;Pod Topology Spread
Constraints&lt;/a&gt; for example. They&amp;rsquo;re
also used for other topology-related features, such as &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/&#34;&gt;Topology Aware
Routing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With regards to configuring these labels, there&amp;rsquo;s currently nothing to stop you modifying the labels in place. You
shouldn&amp;rsquo;t do this, however, since it doesn&amp;rsquo;t change the AZ of the underlying instance and pretty much kills whatever
advantage the topology feature has. If you want to change the AZ of the &lt;code&gt;Node&lt;/code&gt; then you&amp;rsquo;ll need to either migrate it (an
operation that comes with its own issues) or recreate it.&lt;/p&gt;
&lt;h2 id=&#34;machine-api-provider-openstack&#34;&gt;Machine API Provider OpenStack&lt;/h2&gt;
&lt;p&gt;The Machine API Provider OpenStack, or MAPO, is a Machine API provider for the OpenStack platform. The Machine API
allows you to scale up or scale down your cluster based on workload policies or other preferences and functions quite
similarly to the Cluster API, albeit with a different API. You can create &lt;code&gt;Machine&lt;/code&gt;s manually, but its more common to
instead create or modify &lt;code&gt;MachineSet&lt;/code&gt;s (for workers) or &lt;code&gt;ControlPlaneMachineSet&lt;/code&gt;s (for masters). You can find more
information about the Machine API in the &lt;a href=&#34;https://docs.openshift.com/container-platform/4.15/machine_management/index.html&#34;&gt;OpenShift
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Like OpenStack CCM, MAPO uses AZ information to label resources - this time setting the &lt;code&gt;machine.openshift.io/zone&lt;/code&gt;
label on &lt;code&gt;Machine&lt;/code&gt; resources:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -A Machine -o jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{.items[*].metadata.labels}&amp;#39;&lt;/span&gt; | jq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;machine.openshift.io/cluster-api-cluster&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stephenfin-5ps6d&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;machine.openshift.io/cluster-api-machine-role&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;master&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;machine.openshift.io/cluster-api-machine-type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;master&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;machine.openshift.io/instance-type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ci.m1.xlarge&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;machine.openshift.io/region&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;regionOne&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;machine.openshift.io/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;lt;-- !!! here !!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;It also sets an additional related labels - the &lt;code&gt;machine.openshift.io/region&lt;/code&gt; label - but this is sourced from Keystone
information.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;Also like OpenStack CCM, MAPO sources this AZ information from the Nova API, as you can see
&lt;a href=&#34;https://github.com/openshift/machine-api-provider-openstack/blob/release-4.15/pkg/machine/actuator.go#L226&#34;&gt;here&lt;/a&gt;
(&lt;code&gt;instanceStatus&lt;/code&gt; is a thin wrapper around a &lt;code&gt;ServerExt&lt;/code&gt; resource used by Gophercloud). They&amp;rsquo;re also editable but again,
editing them won&amp;rsquo;t actually change the AZ on the underlying instance and you&amp;rsquo;ll need to make changes elsewhere to do
this. However, unlike with &lt;code&gt;Node&lt;/code&gt;s, you can configure the AZ of a new or existing &lt;code&gt;Machine&lt;/code&gt; or &lt;code&gt;MachineSet&lt;/code&gt; /
&lt;code&gt;ControlPlaneMachineSet&lt;/code&gt; as part of the object definition and &lt;em&gt;this&lt;/em&gt; change will get reflected in the labels, both of
the &lt;code&gt;Machine&lt;/code&gt; and of the &lt;code&gt;Node&lt;/code&gt;. For example, to define a AZ when creating a new &lt;code&gt;MachineSet&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;machine.openshift.io/v1beta1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MachineSet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;infrastructure_id&amp;gt;-&amp;lt;role&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;openshift-machine-api&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;providerSpec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nova-az0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;rootVolume&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cinder-az0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This is described in more details in the &lt;a href=&#34;https://docs.openshift.com/container-platform/4.15/machine_management/creating_machinesets/creating-machineset-osp.html&#34;&gt;OpenShift
documentation&lt;/a&gt;.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;Alternatively, to define one for a &lt;code&gt;ControlPlaneMachineSet&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;machine.openshift.io/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ControlPlaneMachineSet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cluster&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;openshift-machine-api&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;machines_v1beta1_machine_openshift_io&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;failureDomains&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;platform&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OpenStack&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;openstack&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        - &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nova-az0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;rootVolume&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cinder-az0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        - &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nova-az1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;rootVolume&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cinder-az1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        - &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nova-az2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;rootVolume&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;availabilityZone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cinder-az2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This is described in more detail in the &lt;a href=&#34;https://docs.openshift.com/container-platform/4.15/machine_management/control_plane_machine_management/cpmso-configuration.html#cpmso-sample-yaml-openstack_cpmso-configuration&#34;&gt;OpenShift
documentation&lt;/a&gt;.&lt;/div&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;cinder-csi-driver&#34;&gt;Cinder CSI Driver&lt;/h2&gt;
&lt;p&gt;The last component we&amp;rsquo;re going to look at here is the Cinder CSI Driver. The Container Storage Interface (CSI) defines a
standardised way to expose arbitrary block and file storage systems to Kubernetes workloads, allowing us to plug in
storage backends for various cloud platforms or networked storage solutions like NFS or SMB. As you might suspect, the
Cinder CSI Driver allows us to plug in storage from the OpenStack Block Storage service, Cinder, and to create
&lt;code&gt;PersistentVolume&lt;/code&gt;s or &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;s that correspond to Cinder volumes.&lt;/p&gt;
&lt;p&gt;Once again, the Cinder CSI driver uses AZ information to label resources and once again it&amp;rsquo;s the &lt;code&gt;Node&lt;/code&gt;s that get the
resulting label. The Cinder CSI driver sets a single label on a node, the &lt;code&gt;topology.cinder.csi.openstack.org/zone&lt;/code&gt;
label:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get Node -o jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{.items[*].metadata.labels}&amp;#39;&lt;/span&gt; | jq
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beta.kubernetes.io/arch&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;amd64&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beta.kubernetes.io/instance-type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ci.m1.xlarge&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;beta.kubernetes.io/os&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;linux&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;failure-domain.beta.kubernetes.io/region&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;regionOne&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;failure-domain.beta.kubernetes.io/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/arch&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;amd64&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stephenfin-5ps6d-master-0&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/os&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;linux&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node-role.kubernetes.io/control-plane&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node-role.kubernetes.io/master&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node.kubernetes.io/instance-type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ci.m1.xlarge&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node.openshift.io/os_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rhcos&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.cinder.csi.openstack.org/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;,    &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;lt;--- !!! here !!!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/region&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;regionOne&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nova&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Unlike the other two components we&amp;rsquo;ve talked about though, the Cinder CSI Driver doesn&amp;rsquo;t fetch AZ information from the
API. Instead, it fetches it from the Metadata API, as you can see
&lt;a href=&#34;https://github.com/openshift/cloud-provider-openstack/blob/release-4.15/pkg/csi/cinder/nodeserver.go#L465-L469&#34;&gt;here&lt;/a&gt;.
That &lt;code&gt;NodeGetInfo&lt;/code&gt; function forms part of the CSI spec and is used by &lt;code&gt;kubelet&lt;/code&gt;, as detailed in the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/node-driver-registrar.html&#34;&gt;Kubernetes
documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now this use of the Metadata service is somewhat of an usual choice: the Metadata service is provided by Nova and the AZ
information it exposes is the AZ of the instance (derived from the host the instance is scheduled to). It&amp;rsquo;s therefore a
Compute AZ so why is being used for a storage-related component? The answer is that we&amp;rsquo;re using it because there&amp;rsquo;s
nothing else to use: as described in &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-1&#34;&gt;part 1&lt;/a&gt;, OpenStack doesn&amp;rsquo;t provide any mechanism to associate compute
hosts with storage AZs outside of using the same naming scheme across both compute and block storage AZs. In my
experience, this loose coupling is a very frequent source of bugs. To use the topology feature (which we&amp;rsquo;ll go into more
detail on shortly), you really need to have a common set of AZs for both the Compute and Block Storage services. Until
OCP 4.15, the OpenStack Cinder CSI Driver Operator (i.e. the operator that deploys and manages the lifecycle of the
Cinder CSI Driver in an OpenShift deployment) assumed this to be the case and always enabled the topology feature. This
has &lt;a href=&#34;https://github.com/openshift/openstack-cinder-csi-driver-operator/pull/127&#34;&gt;since changed&lt;/a&gt; but if you&amp;rsquo;re running an
older release then you&amp;rsquo;re likely to encounter this issue if e.g. using multiple Nova AZs and single Cinder AZ.&lt;/p&gt;
&lt;p&gt;Making things even more complicated, migration of the Nova instance corresponding to a &lt;code&gt;Node&lt;/code&gt; can result in an instance
moving between AZs, assuming the Nova instance in question was not created in a specific AZ initially. The CSI driver
will detect this change and will attempt to update the labels on the &lt;code&gt;Node&lt;/code&gt;, resulting in the following rather nasty
error that will require manual intervention to solve.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Received NotifyRegistrationStatus call: &amp;amp;RegistrationStatus{PluginRegistered:false,Error:RegisterPlugin error -- plugin registration failed with err: error updating Node object with CSI driver node info: error updating node: timed out waiting for the condition; caused by: detected topology value collision: driver reported &amp;#34;topology.cinder.csi.openstack.org/zone&amp;#34;:&amp;#34;nova&amp;#34; but existing label is &amp;#34;topology.cinder.csi.openstack.org/zone&amp;#34;:&amp;#34;nova-az3&amp;#34;,}
Registration process failed with error: RegisterPlugin error -- plugin registration failed with err: error updating Node object with CSI driver node info: error updating node: timed out waiting for the condition; caused by: detected topology value collision: driver reported &amp;#34;topology.cinder.csi.openstack.org/zone&amp;#34;:&amp;#34;nova&amp;#34; but existing label is &amp;#34;topology.cinder.csi.openstack.org/zone&amp;#34;:&amp;#34;nova-az3&amp;#34;, restarting registration container.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;However, assuming we know about these issues and work to avoid them, how does one actually use the topology features
provided by the Cinder CSI driver? There are actually two ways. The first is to configure a topology-aware
&lt;code&gt;StorageClass&lt;/code&gt; and use this for a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, as seen in the &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/v1.29.0/examples/cinder-csi-plugin/topology/example.yaml&#34;&gt;examples for the Cinder CSI
Driver&lt;/a&gt;.
For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;topology-aware-standard&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cinder.csi.openstack.org&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;WaitForFirstConsumer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;allowedTopologies&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- &lt;span style=&#34;color:#f92672&#34;&gt;matchLabelExpressions&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;topology.cinder.csi.openstack.org/zone&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#ae81ff&#34;&gt;az1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;demo-pvc-with-az&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1Gi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;topology-aware-standard&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;By using this storage class, we ensure that the Cinder volume created for the PVC will request an availability zone of
&lt;code&gt;az1&lt;/code&gt;. This is the standard mechanism promoted by Kubernetes and is also supported by other non-OpenStack CSI drivers
that provide topology support.&lt;/p&gt;
&lt;p&gt;The other mechanism is to specify a Cinder CSI driver-specific parameter, &lt;code&gt;availability&lt;/code&gt;, when creating the storage
class. This is effectively a legacy option that pre-dates topology support in CSI but it&amp;rsquo;s still available:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;legacy-az&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cinder.csi.openstack.org&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;WaitForFirstConsumer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;availability&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;az1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;demo-pvc-with-az&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1Gi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;legacy-az&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With regards to configuring these labels there&amp;rsquo;s currently nothing to stop you modifying the labels in place, just like
the labels OpenStack CCM sets on &lt;code&gt;Node&lt;/code&gt;s, and like that you don&amp;rsquo;t really want to do this since the Cinder CSI driver
effectively owns them. The only time you may wish to do this is if you&amp;rsquo;ve migrated your Nova instance and hit the issue
described above. In this case, you can opt to manually relabel the &lt;code&gt;Node&lt;/code&gt;, taking care to drain any workload from it
first to avoid topology mismatches.&lt;/p&gt;
&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap up&lt;/h2&gt;
&lt;p&gt;As so concludes this two part series looking at Availability Zones in OpenStack and OpenShift. As you&amp;rsquo;ve hopefully
ascertained, they can be a very useful feature, particularly in larger deployments, but there are more than a few
potential banana skins to be aware of when you start using them for storage. By way of recommendations, I would suggest
either using a single common AZ for you deployment (you can stick to the default of &lt;code&gt;nova&lt;/code&gt;) or a common set of AZs
across both the compute and block storage hosts (e.g. in a two-AZ deployment, you could use &lt;code&gt;az0&lt;/code&gt; and &lt;code&gt;az1&lt;/code&gt; for both
compute and block storage AZs, rather than &lt;code&gt;nova-az0&lt;/code&gt; and &lt;code&gt;nova-az1&lt;/code&gt; for compute AZs and &lt;code&gt;cinder-az0&lt;/code&gt; and &lt;code&gt;cinder-az1&lt;/code&gt;
for block storage AZs). If you insist on sticking with divergent sets of AZs, you should disable the topology feature
and rely on the legacy &lt;code&gt;availability&lt;/code&gt; parameter of the &lt;code&gt;StorageClass&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;I hope this has been useful to someone. If you spot any mistakes or identify things that I should have covered but
didn&amp;rsquo;t, feel free to send me an email and I&amp;rsquo;ll try get things sorted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Debugging Failed OpenShift-on-OpenStack Deployments</title>
      <link>https://that.guru/blog/debugging-failed-openshift-openstack-deployments/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/debugging-failed-openshift-openstack-deployments/</guid>
      <description>&lt;p&gt;I deploy OpenShift-on-OpenStack quite regularly these days. Some times these deployments fail and the most common
failure I usually see is a timeout during bootstrapping.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ openshift-install --log-level debug create cluster
DEBUG OpenShift Installer 4.15.10
DEBUG Built from commit 24a827900e76d8f9c79122307415b47a4921bbd7
DEBUG Fetching Metadata...
...
DEBUG Reusing previously-fetched Install Config
INFO Skipping VM console logs gather: no gather methods registered for &amp;#34;openstack&amp;#34;
INFO Pulling debug logs from the bootstrap machine
DEBUG Using SSH_AUTH_SOCK /run/user/1000/keyring/ssh to connect to an existing agent
ERROR Attempted to gather debug logs after installation failure: failed to connect to the bootstrap machine: dial tcp 10.0.212.9:22: connect: connection timed out
ERROR Attempted to gather ClusterOperator status after installation failure: listing ClusterOperator objects: Get &amp;#34;https://api.stephenfin.shiftstack-demo.com:6443/apis/config.openshift.io/v1/clusteroperators&amp;#34;: dial tcp 10.0.214.50:6443: i/o timeout
ERROR Bootstrap failed to complete: timed out waiting for the condition
ERROR Failed to wait for bootstrapping to complete. This error usually happens when there is a problem with control plane hosts that prevents the control plane operators from creating the control plane.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You&amp;rsquo;ve a couple of tools that you can use to validate this. The first of these is to check the serial console.
This will highlight the more egregious issues with your deployment. You can do this with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack console url show stephenfin-5ps6d-bootstrap  &lt;span style=&#34;color:#75715e&#34;&gt;# replace with your own bootstrap server&amp;#39;s name&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If this doesn&amp;rsquo;t show anything weird then the next step is to log in to the server and check the status of the &lt;code&gt;bootkube&lt;/code&gt;
service. As is custom with OpenStack, to SSH into a machine you need (a) a floating IP and (b) a security group (or more
accurately a security group rule) that allows SSH access. The Installer automatically assigns a floating IP to the
bootstrap machine so (a) is taken care of. That leaves (b). You like already have an &amp;ldquo;allow SSH&amp;rdquo; security group lying
around and if so, you can use that now:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack server add security group stephenfin-5ps6d-bootstrap allow_ssh  &lt;span style=&#34;color:#75715e&#34;&gt;# replace with your own server, SG names&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;If you don&amp;rsquo;t have such a group, creating one is easy. The following ought to do the trick:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack security group create allow_ssh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack security group rule create &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --protocol tcp --dst-port &lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt; --remote-ip 0.0.0.0/0 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    allow_ssh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack security group rule create &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    --protocol icmp --remote-ip 0.0.0.0/0 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    allow_ssh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;Once you&amp;rsquo;ve allowed SSH traffic you can SSH into the machine.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack server ssh stephenfin-5ps6d-bootstrap -- -l core
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;&lt;code&gt;core&lt;/code&gt; is the default username for Red Hat CoreOS.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;From here you can follow the directions given in the MOTD and check the &lt;code&gt;bootkube&lt;/code&gt; service first:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ journalctl -b -f -u release-image.service -u bootkube.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In my case it appeared the issue was the lack of access to the master nodes:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;s the base image from which all OpenShift Container Platform images inherit.)
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Check if API and API-Int URLs are reachable during bootstrap
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Checking if api.stephenfin.shiftstack-demo.com of type API_URL reachable
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Unable to reach API_URL&amp;#39;s https endpoint at https://api.stephenfin.shiftstack-demo.com:6443/version
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Unable to validate. https://api.stephenfin.shiftstack-demo.com:6443/version is currently unreachable.
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Checking if api-int.stephenfin.shiftstack-demo.com of type API_INT_URL reachable
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Unable to reach API_INT_URL&amp;#39;s https endpoint at https://api-int.stephenfin.shiftstack-demo.com:6443/version
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: Unable to validate. https://api-int.stephenfin.shiftstack-demo.com:6443/version is currently unreachable.
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap bootkube.sh[2449]: bootkube.service complete
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap systemd[1]: bootkube.service: Deactivated successfully.
Apr 22 14:01:09 stephenfin-5ps6d-bootstrap systemd[1]: bootkube.service: Consumed 1min 2.337s CPU time.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The same steps apply for debugging issues with master or worker nodes: add a floating IP, allow SSH access, then SSH
into the machine.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack server add floating ip stephenfin-5ps6d-master-0 10.0.214.101
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack server add security group stephenfin-5ps6d-master-0 allow_ssh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ openstack server ssh stephenfin-5ps6d-master-0 -- -l core
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Availability Zones in Openstack and Openshift (Part 1)</title>
      <link>https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-1/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-1/</guid>
      <description>
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This is part one of two. If you&amp;rsquo;re looking for part two, you can find it
&lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;here&lt;/a&gt;.&lt;/div&gt;
&lt;/aside&gt;


&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This post has been updated since publication to add additional information about implicit and explicit AZs.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;After seeing a few too many availability zone-related issues popping up in OpenShift clusters of late, I&amp;rsquo;ve decided it
might make sense to document the situation with OpenStack AZs on OpenShift (and, by extension, Kubernetes). This is the
first of two parts. This part provides some background on what AZs are and how you can configure them, while the &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;second
part&lt;/a&gt; examines how AZs affect OpenShift and Kubernetes components such as the OpenStack Machine API Provider,
the OpenStack Cluster API Provider, and the Cinder and Manila CSI drivers.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Both the Compute (Nova) and Block Storage (Cinder) services in OpenStack support the concept of Availability Zones (AZs)
and the envisioned use cases is very similar for both. Quoting from the &lt;a href=&#34;https://docs.openstack.org/nova/latest/admin/availability-zones.html&#34;&gt;Nova documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Availability Zones are an end-user visible logical abstraction for partitioning a cloud without knowing the physical
infrastructure. They can be used to partition a cloud on arbitrary factors, such as location (country, datacenter,
rack), network layout and/or power source.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Nova documentation then goes on to specifically note that the AZ feature provides no HA benefit in and of itself -
whatever benefits there are are entirely down to how the deployment is designed - thus it&amp;rsquo;s really just a way to signal
something you&amp;rsquo;ve done in your physical deployment. All of this is equally true of both Nova and Cinder, and in my
experience I&amp;rsquo;ve seen AZs used to demarcate both compute and block storage nodes existing on different racks or in
different datacenters.&lt;/p&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;The Networking (Neutron) service also has the concept of Availability Zones. However, I&amp;rsquo;m not at all familiar with how
these work and they&amp;rsquo;re not something I&amp;rsquo;ve ever used. As a result, I&amp;rsquo;m not going to cover them here.&lt;/div&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;configuring-azs-for-hosts&#34;&gt;Configuring AZs for hosts&lt;/h2&gt;
&lt;p&gt;As you might expect, Cinder AZ&amp;rsquo;s are an attribute of the block storage hosts (i.e. hosts running the &lt;code&gt;cinder-volume&lt;/code&gt;
service). As discussed &lt;a href=&#34;#configuration&#34;&gt;later&lt;/a&gt;, you can configure a host&amp;rsquo;s AZ by setting the &lt;code&gt;[DEFAULT] storage_availability_zone&lt;/code&gt; configuration option in &lt;code&gt;cinder.conf&lt;/code&gt;. By comparison, Nova&amp;rsquo;s AZs are not typically configured
via &lt;code&gt;nova.conf&lt;/code&gt; but are actually attributes of host aggregates and can be configured by setting the &lt;code&gt;availability_zone&lt;/code&gt;
metadata key of an aggregate. If a compute host (i.e. a host running the &lt;code&gt;nova-compute&lt;/code&gt; service) belongs to a host
aggregate with the AZ metadata key set then the host will inherit the AZ of that host aggregate. It&amp;rsquo;s only when a host
doesn&amp;rsquo;t belong to a host aggregate - or none of the host aggregates it belongs to have AZ metadata set - that this
information will be sourced from elsewhere, namely the &lt;code&gt;[DEFAULT] default_availability_zone&lt;/code&gt; config option described
&lt;a href=&#34;#configuration&#34;&gt;below&lt;/a&gt;. Unlike Cinder&amp;rsquo;s config option, this is not intended to differ by host and should be set to the
same value across all compute nodes. Nova will prevent you adding a host to more than one aggregate with AZ metadata set
since a host can only belong to one AZ.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate create --zone nova-az1 foo
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate create --zone nova-az2 bar
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate add host foo stephenfin-devstack
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate add host bar stephenfin-devstack
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ConflictException: 409: Client Error &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; url: http://10.0.109.204/compute/v2.1/os-aggregates/13/action, Cannot add host to aggregate 13. Reason: One or more hosts already in availability zone&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;s&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nova-az1&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In addition, if a host has instances on it, the Nova will also prevent you from modifying the AZ metadata of an
aggregate it already belongs to - since this would break the AZ constraint placed on any of the existing instances:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack server show test-server -f value -c OS-EXT-AZ:availability_zone -c OS-EXT-SRV-ATTR:host
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nova-az1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;stephenfin-devstack
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate show foo -f value -c availability_zone -c hosts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nova-az1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;stephenfin-devstack&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate set --zone nova-az2 foo
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;BadRequestException: 400: Client Error &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; url: http://10.0.109.204/compute/v2.1/os-aggregates/12, Cannot update aggregate 12. Reason: One or more hosts contain instances in this zone.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;It should also prevent you from adding/removing a host to/from an aggregate when instances are present but this doesn&amp;rsquo;t
currently happen. This is filed as &lt;a href=&#34;https://bugs.launchpad.net/nova/+bug/1907775&#34;&gt;bug #1907775&lt;/a&gt; and has not yet been
resolved.&lt;/div&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;requesting-azs-for-resources-servers-volumes-volume-backups-&#34;&gt;Requesting AZs for resources (servers, volumes, volume backups, &amp;hellip;)&lt;/h2&gt;
&lt;p&gt;Nova allows you to specify an AZ when creating an instance (or &amp;ldquo;server&amp;rdquo;, in OpenStackClient parlance), while Cinder
allows you to specify them when creating a volume, a volume backup, a volume group, or (volume groups&amp;rsquo; deprecated
predecessor) a consistency group. For example, to create an instance (or &amp;ldquo;server&amp;rdquo;) with an explicit compute AZ:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openstack server create --availability-zone compute-az1 ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Likewise, to create a volume and volume backup with an explicit block storage AZ:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openstack volume create --availability-zone volume-az1 ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openstack volume backup create --availability-zone volume-az2 ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;However, you&amp;rsquo;ll note that these resource types will always have AZ information associated with them, even when an AZ
wasn&amp;rsquo;t specifically requested during creation. This is because, in the absence of specific AZ information, both services
will default to setting the AZ of the resource to the AZ of the host that the resource was created on. Put another way,
if I create instance &lt;code&gt;my-server&lt;/code&gt; with no AZ information and it ends up on host &lt;code&gt;my-host&lt;/code&gt;, then &lt;code&gt;my-server&lt;/code&gt; will inherit
the AZ of &lt;code&gt;my-host&lt;/code&gt;. Block storage resources work in the same way, meaning volume &lt;code&gt;my-volume&lt;/code&gt; will inherit the AZ of the
host it is scheduled to. As a result, there has historically been no way for an end-user to tell if an AZ was explicitly
requested when creating a server or not. In fact, the only way they will find out is if they try to move the server
since Nova will insist of moving the instance to another host within the same AZ (this wouldn&amp;rsquo;t happen for a server that
wasn&amp;rsquo;t explicitly created in an AZ). As we&amp;rsquo;ll touch on in &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;part 2&lt;/a&gt;, this has been rather frustrating from an
OpenShift or Kubernetes perspective since Kubernetes&amp;rsquo; topology feature is a hard requirement and it does not like us
changes the AZ-related labels of &lt;code&gt;Node&lt;/code&gt; or &lt;code&gt;Machine&lt;/code&gt; objects, which can happen when you migrate the underlying server
and the server picks up the AZ of the new host. Fortunately, the 2024.1 (Caracal) release of OpenStack introduced a new
field to the &lt;code&gt;GET /servers/{serverID}&lt;/code&gt; response called &lt;code&gt;pinned_availability_zone&lt;/code&gt; which will show the AZ requested
during initial instance creation, if set and it&amp;rsquo;s just a matter of time before we&amp;rsquo;re able to start consuming this in the
various OpenShift and Kubernetes components.&lt;/p&gt;
&lt;h2 id=&#34;combining-nova-and-cinders-az-features&#34;&gt;Combining Nova and Cinder&amp;rsquo;s AZ features&lt;/h2&gt;
&lt;p&gt;Finally, it&amp;rsquo;s worth exploring the interplay of the Nova and Cinder AZ features since this will be particularly relevant
in &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;part 2&lt;/a&gt;. In a &lt;a href=&#34;https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.2/html-single/hyperconverged_infrastructure_guide/index&#34;&gt;Hyperconverged Infrastructure (HCI)&lt;/a&gt; deployment, where compute and block storage
services run side-by-side on hyperconverged hosts, the compute hosts &lt;em&gt;are&lt;/em&gt; the block storage hosts and there is no
difference between the AZs. In a non-HCI deployment, this is unlikely to be the case but this hasn&amp;rsquo;t prevented people
and applications from frequently munging the two types of AZ, as we will see later. Because this conflation of different
AZ types can happen, the general expectation we would have is that one of the following is true:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;There is only a single compute AZ, a single block storage and they have the same name.&lt;/em&gt; This is the default
configuration if you use &amp;ldquo;stock&amp;rdquo; OpenStack: Nova&amp;rsquo;s default AZ is &lt;code&gt;nova&lt;/code&gt; and Cinder helpfully defaults to the same
value.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;There are multiple compute and block storage AZs, but there is the same number of both and they share the same
name.&lt;/em&gt; For example, both the compute and block storage services have the following AZs defined: &lt;code&gt;AZ0&lt;/code&gt;, &lt;code&gt;AZ1&lt;/code&gt;, and
&lt;code&gt;AZ2&lt;/code&gt;. In this case, users and applications which incorrectly use compute host AZ information to configure the AZ of
volumes and related block storage resources will &amp;ldquo;just work&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;There are multiple compute and block storage AZs, and there is either a different number of each or they have
different names.&lt;/em&gt; For example, the compute services have the &lt;code&gt;compute-az0&lt;/code&gt; and &lt;code&gt;compute-az1&lt;/code&gt; AZs defined while the
block storage services have the &lt;code&gt;volume-az0&lt;/code&gt; and &lt;code&gt;volume-az1&lt;/code&gt; AZs defined. In this case, the users and applications
must be very careful to explicitly specify a correct AZ when creating volumes and related block storage resources and
must ensure Nova is configured to allow attaching volumes in other AZs (more of this later too).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;The last case above isn&amp;rsquo;t helped by the fact that neither Nova nor Cinder provide an API to request the correct block
storage AZ for a given compute host. To be fair, such an API would likely be a rather difficult thing to do, given
multiple backends are a thing to be considered. It would be effectively impossible to do automatically, meaning there
would still be initial manual configuration required. The closest analog we have for his today is the &lt;a href=&#34;https://docs.openstack.org/cinder/2023.1/admin/availability-zone-type.html&#34;&gt;Volume Type AZ
feature&lt;/a&gt;, which allows you to indicate the AZs that can be used when creating a volume with a given
volume type (so that e.g. a particular block storage backend that is only available to one rack can&amp;rsquo;t be requested by
volumes hosted by block storage services running on another rack). As the docs for that indicate, this configuration is
entirely deployment specific and therefore totally manual.&lt;/div&gt;
&lt;/aside&gt;

&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap up&lt;/h2&gt;
&lt;p&gt;That concludes part 1 of this OpenShift-centric examination of OpenStack Availability Zones. In this part we focused
almost exclusively on OpenStack itself, looking at what AZs are, how they&amp;rsquo;re configured and used, and the various issues
people are likely to encounter along the way, but in &lt;a href=&#34;https://that.guru/blog/availability-zones-in-openstack-and-openshift-part-2&#34;&gt;part 2&lt;/a&gt; we&amp;rsquo;re going to turn our focus to how OpenStack AZs
are consumed and represented by OpenShift components when an OpenShift cluster is deployed on an OpenStack cloud. Stay
tuned!&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;Since this feature exists across two services, there are two sets of configuration options to be concerned with.&lt;/p&gt;
&lt;p&gt;As of the 2023.1 (Antelope) release, Nova has three relevant configuration options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[DEFAULT] default_availability_zone&lt;/code&gt; defines the default AZ of each compute host, which can be changed by adding the
host to a host aggregate and setting the special &lt;code&gt;availability_zone&lt;/code&gt; metadata property as described in the &lt;a href=&#34;https://docs.openstack.org/nova/latest/admin/availability-zones.html&#34;&gt;nova
docs&lt;/a&gt;. This option defaults to &lt;code&gt;nova&lt;/code&gt; and as noted in the &lt;a href=&#34;https://docs.openstack.org/nova/latest/admin/availability-zones.html&#34;&gt;nova docs&lt;/a&gt;, the default AZ should never
explicitly requesting this AZ when creating new instances since it will prevent migration of instance between
different hosts in different AZs (which is allowed by default if the AZ was unset during initial creation) as well as
identification of hosts that are missing AZ information. You have been warned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[DEFAULT] default_schedule_zone&lt;/code&gt; defines the default AZ that should be assigned to an instance on creation. If this
is unset, the instance will be assigned an implicit AZ of the host it lands on. You might want to use this if you
wanted the majority of instances to go into a &amp;ldquo;generic&amp;rdquo; AZ while special instances can go into specific AZs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[cinder] cross_az_attach&lt;/code&gt; determines whether volumes are allowed to be attached to an instance if the instance host&amp;rsquo;s
compute AZ differs from that of the volume&amp;rsquo;s block storage AZ. It also determines whether volumes created when
creating a boot-from-volume server have an explicit AZ associated with them or not. This defaults to &lt;code&gt;true&lt;/code&gt; and with
good reason, given the aforementioned caveats around munging of compute and block storage AZs and the need for them to
be identical.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also the &lt;code&gt;[DEFAULT] internal_service_availability_zone&lt;/code&gt; configuration option, but this has no real impact for
end-users.&lt;/p&gt;
&lt;p&gt;As of the 2023.1 (Antelope) release, Cinder has four relevant configuration options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[DEFAULT] storage_availability_zone&lt;/code&gt; defines the default AZ of the block storage host. This defaults to &lt;code&gt;nova&lt;/code&gt; and
can be overridden on a per-backend basis using &lt;code&gt;[foo] backend_availability_zone&lt;/code&gt;. Speaking of which&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[foo] backend_availability_zone&lt;/code&gt; define the default AZ for a specific backend of the block storage host. &lt;code&gt;foo&lt;/code&gt; should
be the name of the volume backend, as defined in &lt;code&gt;[DEFAULT] enabled_backends&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[DEFAULT] default_availability_zone&lt;/code&gt; defines the default AZ that should be assigned to a volume on creation. If this
is unset, the volume will be assigned the AZ of the host it lands on (which in turn defaults to &lt;code&gt;[DEFAULT] storage_availability_zone&lt;/code&gt;, per above).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[DEFAULT] allow_availability_zone_fallback&lt;/code&gt; allows you to ignore an request for an invalid block storage AZ and
instead fallback to the default AZ defined in &lt;code&gt;[DEFAULT] default_availability_zone&lt;/code&gt;. This defaults to &lt;code&gt;false&lt;/code&gt;, though
to be honest &lt;code&gt;true&lt;/code&gt; is probably a sensible value for configurations where e.g. there are multiple compute AZs and a
single volume AZ.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;
&lt;p&gt;Again, since this feature exists across two services, there are two sets of resource types to be concerned with.&lt;/p&gt;
&lt;p&gt;To configure the AZ of a compute host, you configure AZ information for a host aggregate and then add the host to this
aggregate.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate create --zone nova-az1 foo
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack aggregate add host foo stephenfin-devstack
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once this is done, you can request the AZ when creating an instance (or &amp;ldquo;server&amp;rdquo;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack server create --availability-zone nova-az1 ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;On the other hand, the AZ of a storage host is configured via config and there&amp;rsquo;s no API method to configure it. You can
use it when creating a volume just like creating a server though:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openstack volume create --availability-zone volume-az1 ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Or when creating a volume backup:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openstack volume backup create --availability-zone volume-az2 ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Other API libraries like gophercloud also expose these attributes and allow them to be configured, but we won&amp;rsquo;t go into
that here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying MetalLB with BGP on Openstack (Part 2)</title>
      <link>https://that.guru/blog/deploying-metallb-on-openstack-part-2/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/deploying-metallb-on-openstack-part-2/</guid>
      <description>
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This is part two of two. If you&amp;rsquo;re looking for part one, you can find it
&lt;a href=&#34;https://that.guru/blog/deploying-metallb-on-openstack-part-1&#34;&gt;here&lt;/a&gt;.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;As noted &lt;a href=&#34;https://that.guru/blog/deploying-metallb-on-openstack-part-1&#34;&gt;previously&lt;/a&gt;, one of the goals for an upcoming OpenShift release is to formally support
&lt;a href=&#34;https://metallb.universe.tf/&#34;&gt;MetalLB&lt;/a&gt; and the &lt;a href=&#34;https://github.com/metallb/metallb-operator&#34;&gt;MetalLB operator&lt;/a&gt; on the OpenStack platform. In part one of the series,
we configured an environment with OpenStack, OpenShift and BGP software routers. Now, in part two, we&amp;rsquo;re going to focus
on installing and configuring MetalLB itself.&lt;/p&gt;
&lt;h2 id=&#34;install-metallb&#34;&gt;Install MetalLB&lt;/h2&gt;
&lt;p&gt;The first step on the path to using MetalLB is actually installing it. As a reminder, we want to use MetalLB in &lt;em&gt;BGP
mode&lt;/em&gt;. This necessitates things like routers that speak BGP and an OpenStack deployment that is configured to talk to
these routers. These were all discussed in &lt;a href=&#34;https://that.guru/blog/deploying-metallb-on-openstack-part-1&#34;&gt;part one&lt;/a&gt; of this series, and if you followed along with this then
you will currently have a deployment that looks like this:&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://that.guru/media/deploying-metallb-on-openstack-1.png&#34;
    alt=&#34;Image displaying the network topology of VMs in a deployment&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;Our deployment configured&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If this is not the case, you probably want to read that part first. However, assuming it is, we can now proceed with
installation. As a Kubernetes-native project, MetalLB comes with all the usual mechanisms for installation of Kubernetes
components. Plain manifests are provided, as are Helm Charts and an operator, the MetalLB Operator. The various
installation mechanisms are all discussed in the &lt;a href=&#34;https://metallb.universe.tf/installation/&#34;&gt;MetalLB installation guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because operators are awesome, I opted to use the MetalLB Operator to deploy MetalLB and manage its lifecycle. The
MetalLB Operator is available on OperatorHub at operatorhub.io/operator/metallb-operator but when I was testing, the
&lt;code&gt;main&lt;/code&gt; branch contained a feature I wanted, namely the ability to configure a &lt;a href=&#34;https://github.com/metallb/metallb-operator/pull/342&#34;&gt;&lt;code&gt;loadBalancerClass&lt;/code&gt;&lt;/a&gt;. As a
result, I opted to deploy MetalLB Operator from source. The MetalLB Operator provides a very helpful &lt;code&gt;Make&lt;/code&gt; target to do
this, which you can use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ git clone https://github.com/metallb/metallb-operator
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ cd metallb-operator
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ make deploy-openshift
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Deployment takes a while but once finished we can validate that everything is running:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get all -n metallb-system
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Installation complete! Now onto configuration.&lt;/p&gt;
&lt;h2 id=&#34;initial-metallb-configuration&#34;&gt;Initial MetalLB configuration&lt;/h2&gt;
&lt;p&gt;With the MetalLB Operator installed, it&amp;rsquo;s time to &lt;em&gt;configure&lt;/em&gt; MetalLB. By using MetalLB Operator we gain the ability to
manage configuration of MetalLB itself via CRs. First up is the &lt;code&gt;MetalLB&lt;/code&gt; CR. This is primary configuration mechanism
and is the thing that enables MetalLB itself. There should only be one of them, which you can create like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF | oc apply -f -
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: metallb.io/v1beta1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: MetalLB
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    name: metallb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    namespace: metallb-system
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    loadBalancerClass: &amp;#39;metallb.universe.tf/metallb&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    nodeSelector:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        kubernetes.io/os: linux
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        node-role.kubernetes.io/worker: &amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are a couple of things to note here. Firstly, we are configuring &lt;code&gt;spec.loadBalancerClass&lt;/code&gt;. This is necessary
because &lt;em&gt;dev-install&lt;/em&gt; deploys Octavia by default and &lt;code&gt;cloud-provider-openstack&lt;/code&gt; is using this for load balancing by
default. By setting this, we have the ability to later configure &lt;code&gt;Service&lt;/code&gt;s to use MetalLB rather than Octavia.
Secondly, we are restricting the speakers to run on worker nodes by configuring &lt;code&gt;spec.nodeSelector&lt;/code&gt; as there&amp;rsquo;s simply no
need to run them on the master nodes.&lt;/p&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;The choice of &lt;code&gt;metallb.universe.tf/metallb&lt;/code&gt; as the &lt;code&gt;spec.loadBalancerClass&lt;/code&gt; is totally arbitrary. You can use anything
label-like here so long as you use the same value later when creating &lt;code&gt;Service&lt;/code&gt;s.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;Once the CR is created, we can validate that it exists:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system metallb metallb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Assuming so, you can ensure the &lt;code&gt;loadBalancerClass&lt;/code&gt; attribute is in effect by inspecting the underlying service
containers:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system pods
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                                  READY   STATUS    RESTARTS      AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;controller-7df7bbcffb-8cqzb                           1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             5m1s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;metallb-operator-controller-manager-c44c967b9-l6rvx   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             13h
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;metallb-operator-webhook-server-6fdccfb5c5-k8b2m      1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;13h ago&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;   13h
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speaker-bg6pq                                         4/4     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             5m1s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speaker-q6dmg                                         4/4     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             5m1s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speaker-sjmtc                                         4/4     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             5m1s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;speaker-z9zrr                                         4/4     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;             5m1s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system pod controller-7df7bbcffb-8cqzb -o jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{.spec.containers[0].args}&amp;#34;&lt;/span&gt; | yq -P
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7472&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --log-level&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;info
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --cert-service-name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;webhook-service
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --lb-class&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;metallb.universe.tf/metallb
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --webhook-mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;disabled
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system pod speaker-bg6pq -o jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{.spec.containers[0].args}&amp;#34;&lt;/span&gt; | yq -P
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7472&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --log-level&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;info
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- --lb-class&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;metallb.universe.tf/metallb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This all looks good. With this, the initial configuration of MetalLB is also complete. Now to the next step: configuring
MetalLB for our BGP environment.&lt;/p&gt;
&lt;h2 id=&#34;configuring-metallb-to-talk-to-the-router&#34;&gt;Configuring MetalLB to talk to the router&lt;/h2&gt;
&lt;p&gt;MetalLB in BGP mode requires a few bits of information. It needs a list of IP addresses that it can hand out, it needs
information about the routers that it should peer with, and it needs to be told which IP addresses it can advertise via
BGP. This configuration is all done using CRs, namely the &lt;code&gt;IPAddressPool&lt;/code&gt;, &lt;code&gt;BGPPeer&lt;/code&gt;, and &lt;code&gt;BGPAdvertisement&lt;/code&gt; CRs. First
up, the IP address pools.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF | oc apply -f -
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: metallb.io/v1beta1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: IPAddressPool
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  namespace: metallb-system
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: ipaddresspool
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  addresses:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - 192.168.50.1-192.168.50.254
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&amp;rsquo;ve picked a totally arbitrary IP address range, ensuring it doesn&amp;rsquo;t overlap with any other IP address range on the
network. Obviously if you have specific IP addresses you wish to use, you should configure these instead. I&amp;rsquo;ve also only
configured one. This should be loads for our purposes.&lt;/p&gt;
&lt;p&gt;Next, the BGP peers. While we have multiple leaf routers, our master and worker nodes are all talking to the
&lt;code&gt;rack1-gateway&lt;/code&gt; router. As a result, we really only need to create a BGP peer for this router.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF | oc apply -f -
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: metallb.io/v1beta2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: BGPPeer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: rack1-bgp-peer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  namespace: metallb-system
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  myASN: 64998
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  peerASN: 64999
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  peerAddress: 192.168.10.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  password: &amp;#34;f00barZ&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;poc-bgp&lt;/code&gt; project we used to configure our BGP software router uses the &lt;code&gt;64999&lt;/code&gt; ASN for all of the leaf routers and
configured a password of &lt;code&gt;f00barZ&lt;/code&gt;. Since we&amp;rsquo;re pairing with one of these leaf routers, &lt;code&gt;rack1-gateway&lt;/code&gt;, we needed to
configure these. We chose the &lt;code&gt;64998&lt;/code&gt; ASN for ourselves.&lt;/p&gt;
&lt;p&gt;Finally, the BGP advertisement. Because MetalLB supports both a BGP and an L2 mode, it is possible that you could have
some IP addresses that are meant to be assigned using BGP and other IP addresses that are meant to be assigned using
ARP. In our case, we are only configuring MetalLB in BGP mode and only have a single IP address pool, which means we can
advertise all IPs via BGP.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF | oc apply -f -
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: metallb.io/v1beta1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: BGPAdvertisement
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: bgpadvertisement
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  namespace: metallb-system
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With those three CRs created, we can validate that they have been in fact created.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system ipaddresspool
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME            AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ipaddresspool   true          false             &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;192.168.50.1-192.168.50.254&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system bgppeer
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME             ADDRESS        ASN     BFD PROFILE   MULTI HOPS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rack1-bgp-peer   192.168.10.1   &lt;span style=&#34;color:#ae81ff&#34;&gt;64999&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get -n metallb-system bgpadvertisement
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME               IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   PEERS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;bgpadvertisement
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wonderful!&lt;/p&gt;
&lt;h2 id=&#34;dealing-with-port-security-issues&#34;&gt;Dealing with port security issues&lt;/h2&gt;
&lt;p&gt;With the above steps completed, our configuration of MetalLB is complete. However, if you were to create a &lt;code&gt;Service&lt;/code&gt;
with &lt;code&gt;type=LoadBalancer&lt;/code&gt; now, you would find it doesn&amp;rsquo;t actually work. This is because Neutron has MAC spoofing
protection that is enabled by default. The fact that our worker node is advertising IP addresses that neutron does not
know about triggers this protection and results in packets getting dropped as they egress our worker node.&lt;/p&gt;
&lt;p&gt;To work around this issue you have two options: you can disable port security, or you make use of neutron&amp;rsquo;s
&lt;code&gt;allowed-address-pairs&lt;/code&gt; extension to allow additional IPs, subnets, and MAC addresses, other than the fixed IP and MAC
address associated with the port, to act as source addresses for traffic leaving the port/virtual interface. Let&amp;rsquo;s
start with the former.&lt;/p&gt;
&lt;h3 id=&#34;disable-port-security&#34;&gt;Disable port security&lt;/h3&gt;
&lt;p&gt;Disabling port security requires removing any existing allowed address pairs, removing any security groups, and finally
disabling port security in general. If you you opt to do this, you will need to do this it for all worker node ports. As
we only only have one worker node here, there is only one port to worry about, but you can trivially script the removal
of port security for a larger number of ports using &lt;em&gt;openstackclient&lt;/em&gt; or something like Ansible. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ports&lt;span style=&#34;color:#f92672&#34;&gt;=(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    stephenfin-qfnvm-worker-0-97fkv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; port in &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;ports[@]&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    openstack port set --no-allowed-address &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$port&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    openstack port set --no-security-group &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$port&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    openstack port set --disable-port-security &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$port&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;I&amp;rsquo;ve used a static list of ports here. You can find this list of ports by filtering on the &lt;code&gt;openshiftClusterID=foo&lt;/code&gt; tag,
where &lt;code&gt;foo&lt;/code&gt; is the name &lt;code&gt;openshift-installer&lt;/code&gt; (or rather, Terraform) assigned to the cluster. Don&amp;rsquo;t forget to skip the
ingress and API ports!&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;This is definitely the simpler option, though you will obviously be reliant on something else to provide network
security. Let&amp;rsquo;s look at the alternative option.&lt;/p&gt;
&lt;h3 id=&#34;configure-allowed-address-pairs&#34;&gt;Configure allowed address pairs&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;allowed-address-pairs&lt;/code&gt; extension can be used to allow egress traffic from a VM with an IP outside of the one
configured on the neutron port. From the docs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;code&gt;allowed-address-pairs&lt;/code&gt; extension adds an &lt;code&gt;allowed_address_pairs&lt;/code&gt;
attribute to ports. The value of &lt;code&gt;allowed_address_pairs&lt;/code&gt; is an array of
allowed address pair objects, each having an &lt;code&gt;ip_address&lt;/code&gt; and a
&lt;code&gt;mac_address&lt;/code&gt;. The set of allowed address pairs defines IP and MAC address
that the port can use when sending packets if &lt;code&gt;port_security_enabled&lt;/code&gt; is
&lt;code&gt;true&lt;/code&gt; (see the &lt;code&gt;port-security&lt;/code&gt; extension). Note that while the
&lt;code&gt;ip_address&lt;/code&gt; is required in each allowed address pair, the &lt;code&gt;mac_address&lt;/code&gt;
is optional and will be taken from the port if not specified.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you inspect the ports assigned to the master and worker nodes, you&amp;rsquo;ll note that we already have some allowed address
pairs defined. OpenShift on OpenStack uses this feature to enable load balancing of the ingress and API VIPs and we can
opt to use it for load balancing of services also. Unfortunately, there is no easy way to create allowed address pairs
for all IP addresses in a given subnet, nor to create them in a manner where they apply to all hosts. As a result,
applying this will be tedious and scalability may well become a concern, particularly where there is a large number of
worker nodes or a large range of IP addresses (via &lt;code&gt;IPAddressPool&lt;/code&gt;). Since we only have one node and will only create
one example, we can at least try it out.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack port set --allowed-address&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ip-address&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.50.1 stephenfin-qfnvm-worker-0-97fkv-0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you wanted to create multiple services, you would need to create an allowed address pair for each and every IP
address specified in the &lt;code&gt;IPAddressPool&lt;/code&gt;(s).&lt;/p&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;The upcoming Bobcat release of neutron adds a new &lt;code&gt;allowedaddresspairs-atomic&lt;/code&gt; extension, which provides new
&lt;code&gt;add_allowed_address_pairs&lt;/code&gt; and &lt;code&gt;remove_allowed_address_pairs&lt;/code&gt; member actions. Once released, you could use this to bulk
update the allowed-address-pairs in an atomic manner, which would be helpful if you wanted to write a tool to automate
the configuration of allowed address pairs where there are large numbers of possible IP addresses. You will still have
to specify all possible IP addresses individually, however.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;This is the more involved option but either option should work. In any case, with port security issues mitigated, we are
finally in a position to validate behavior.&lt;/p&gt;
&lt;h2 id=&#34;testing-it-out&#34;&gt;Testing it out&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s create a simple &amp;ldquo;Hello, world&amp;rdquo; example to test this out. We&amp;rsquo;ll use the &lt;code&gt;e2e-test-images/agnhost&lt;/code&gt; image to do this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc new-project test-metallb
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc create deployment hello-node --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ cat  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF | oc apply -f -
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: Service
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: test-frr
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  selector:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    app: hello-node
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  ports:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - port: 80
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      protocol: TCP
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      targetPort: 9376
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  loadBalancerClass: metallb.universe.tf/metallb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  type: LoadBalancer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This should look very familiar if you&amp;rsquo;ve ever worked with services. In fact, there&amp;rsquo;s nothing unusual here aside from our
use of &lt;code&gt;spec.type=LoadBalancer&lt;/code&gt; and the declaration of &lt;code&gt;spec.loadBalancerClass&lt;/code&gt;. The former ensure we actually use a
load balancer while the latter ensures that the load balancer used is MetalLB rather than Octavia.&lt;/p&gt;
&lt;p&gt;Once created, inspect the service to find the IP address assigned:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ oc get service test-frr
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME       TYPE           CLUSTER-IP       EXTERNAL-IP    PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;        AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;test-frr   LoadBalancer   172.30.130.128   192.168.50.1   80:32519/TCP   2m13s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From this, we can see that the service has been assigned IP &lt;code&gt;192.168.50.1&lt;/code&gt;. If we &lt;code&gt;curl&lt;/code&gt; this, we should get something
back. Let&amp;rsquo;s test it out.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ curl 192.168.50.1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hello-node-855787d74c-fkbt5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And it works. Good job, people!&lt;/p&gt;
&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap Up&lt;/h2&gt;
&lt;p&gt;In this post, we deployed MetalLB via the MetalLB Operator, configured it a minimal manner, and worked around the port
security issues that using an external routing solution with neutron presents. As you can see though, port security
issues aside, the actual process of deploying and configuring MetalLB is rather effortless.&lt;/p&gt;
&lt;p&gt;In a future post, I plan to outline some of the steps I used to debug and resolve issues I had deploying this initial
configuration (there were a few). For now, I hope this was helpful to someone.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deploying MetalLB with BGP on Openstack (Part 1)</title>
      <link>https://that.guru/blog/deploying-metallb-on-openstack-part-1/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/deploying-metallb-on-openstack-part-1/</guid>
      <description>
&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;This is part one of two. If you&amp;rsquo;re looking for part two, you can find it
&lt;a href=&#34;https://that.guru/blog/deploying-metallb-on-openstack-part-2&#34;&gt;here&lt;/a&gt;.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;One of the goals for an upcoming OpenShift release is to formally support &lt;a href=&#34;https://metallb.universe.tf/&#34;&gt;MetalLB&lt;/a&gt; and the &lt;a href=&#34;https://github.com/metallb/metallb-operator&#34;&gt;MetalLB
operator&lt;/a&gt; on the OpenStack platform. While MetalLB is mainly targeted at bare metal deployments, it
also has value on some on-prem platforms such as OpenStack; for example, if you can&amp;rsquo;t or don&amp;rsquo;t want to deploy the
OpenStack Load Balancing service (Octavia). I&amp;rsquo;ve been investigating how this would work and this post consists of my
testing notes, along with some asides to help flesh things out. I took a few shortcuts, particularly when it comes to
initial deployment, so I don&amp;rsquo;t know how broadly useful this will be. However, there are very few blogs talking about
MetalLB &lt;strong&gt;in BGP mode&lt;/strong&gt; on OpenStack so I imagine there&amp;rsquo;s &lt;em&gt;something&lt;/em&gt; of use here.&lt;/p&gt;
&lt;p&gt;One thing to note from the get-go is that this focuses on using MetalLB as a load balancer for &lt;strong&gt;applications&lt;/strong&gt; or data
plane - that is, &lt;code&gt;Service&lt;/code&gt; instances with &lt;code&gt;type=LoadBalancer&lt;/code&gt;. Like OpenShift on bare metal, OpenShift on OpenStack uses
&lt;em&gt;keepalived&lt;/em&gt; and &lt;em&gt;HAProxy&lt;/em&gt; to load balance the API and ingress VIPs (which you can see in action by inspecting the pods
in &lt;code&gt;openshift-openstack-infra&lt;/code&gt; namespace). There are no current plans to provide a mechanism for using MetalLB to load
balance the control plane.&lt;/p&gt;
&lt;p&gt;This is the first part of two and focuses on deploying OpenStack, configuring a BGP environment, before deploying an
OpenShift cluster. The end result should look something like this:&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://that.guru/media/deploying-metallb-on-openstack-1.png&#34;
    alt=&#34;Image displaying the network topology of VMs in a deployment&#34;&gt;&lt;figcaption&gt;
      &lt;h4&gt;Our proposed deployment&lt;/h4&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is likely the most involved section to do from scratch owing to the variety and varying complexity of options
available for configuring OpenStack, BGP, and OpenShift. It is also heavily dependent on the hardware you have available
to you. As a result, you may wish to ignore this part entirely and figure out your own mechanism for getting an
environment that looks like the above.&lt;/p&gt;
&lt;h2 id=&#34;openstack&#34;&gt;OpenStack&lt;/h2&gt;
&lt;p&gt;Before we even start thinking about OpenShift, we need our OpenStack platform to run things on. You may well have an
OpenStack platform already, in which case you can skip this section entirely. Similarly, you may have a preferred
tool or workflow for deploying OpenStack clusters, meaning again you can skip this section entirely and do your own
thing. The important thing is that whatever environment you have must have enough capacity to run 4 small instances for
BGP routing (a flavour with 1GB RAM, 1 vCPU, and 10GB disk will suffice) and 4 much larger instances for OpenShift nodes
(16GB RAM, 4 vCPU and 40GB disk &lt;em&gt;minimum&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;In my case, I was deploying OpenStack from scratch onto a single beefy bare metal machine (A Dell PowerEdge R640 with 2
Intel Xeon Silver 4216 CPUs, 192 GB RAM and 512GB SSD + 2TB HDD). To do so, I used
&lt;a href=&#34;https://github.com/shiftstack/dev-install/&#34;&gt;github.com/shiftstack/dev-install&lt;/a&gt;, an opinionated wrapper around &lt;a href=&#34;https://docs.openstack.org/project-deploy-guide/tripleo-docs/latest/deployment/standalone.html&#34;&gt;TripleO
Standalone&lt;/a&gt; designed for hosting OpenShift clusters, to deploy an OSP 16.2 cloud (OpenStack Train).
&lt;em&gt;dev-install&lt;/em&gt; is available on GitHub:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ git clone https://github.com/shiftstack/dev-install
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ cd dev-install
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using &lt;em&gt;dev-install&lt;/em&gt; is relatively easy: all it needs is a &lt;code&gt;local_overrides.yaml&lt;/code&gt; file that contains information about
the IP address and hostname of the server, the OSP release you wish to use, and any other overrides necessary. I didn&amp;rsquo;t
need anything special so there were no overrides in my case. This meant my &lt;code&gt;local_overrides.yaml&lt;/code&gt; file looked something
like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;acme&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;public_api&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.1.240.35&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;standalone_host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;acme.shiftstack.test&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;rhos_release&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;16.2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;clouddomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;shiftstack.test&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the &lt;code&gt;local_overrides.yaml&lt;/code&gt; file in place, you can kick off installation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ make config host&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;acme.shiftstack.test
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ make osp_full overrides&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;local_overrides.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Installation should take about 30 minutes and two new entries, &lt;code&gt;standalone&lt;/code&gt; and &lt;code&gt;standalone_openshift&lt;/code&gt;, will be added to
your &lt;code&gt;clouds.yaml&lt;/code&gt; file upon completion.&lt;/p&gt;
&lt;p&gt;Once installation is completed, there is one final step necessary: starting &lt;code&gt;sshuttle&lt;/code&gt;. &lt;em&gt;dev-install&lt;/em&gt; creates a
&lt;code&gt;hostonly&lt;/code&gt; network that is not routable outside the host. As a result, you need something to proxy requests for the
associated &lt;code&gt;hostonly-subnet&lt;/code&gt; subnet to the host and &lt;em&gt;dev-install&lt;/em&gt; opts to use &lt;code&gt;sshuttle&lt;/code&gt; for this. You can start
&lt;code&gt;sshuttle&lt;/code&gt; using the wrapper script provided:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ ./scripts/sshuttle-standalone.sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;bgp&#34;&gt;BGP&lt;/h2&gt;
&lt;p&gt;Next, we need to configure a BGP environment. I didn&amp;rsquo;t have access to some ToR switches with BGP support so we
opted to emulate it using &lt;a href=&#34;https://frrouting.org/&#34;&gt;&lt;code&gt;frr&lt;/code&gt;&lt;/a&gt;, a software routing solution. To do this, I used
&lt;a href=&#34;https://github.com/shiftstack/poc-bgp/&#34;&gt;github.com/shiftstack/poc-bgp&lt;/a&gt;, a set of Ansible playbooks which deployed four instances on my new OpenStack
cloud: a spine router instance and three leaf router instances. Once again, there are other software routing solutions
available and you might even have access to real hardware, so if you opt for another approach you could obviously skip
this section (though if you do, you will likely need to modify your MetalLB configuration when we get to that step in
the next post). Assuming you opt to use &lt;code&gt;poc-bgp&lt;/code&gt;, you&amp;rsquo;ll need a CentOS 9 Stream image to exist on the cloud as well as
a key pair so that you can SSH into the instances it creates. You can create the image like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ wget https://cloud.centos.org/centos/9-stream/x86_64/images/CentOS-Stream-GenericCloud-9-20230417.0.x86_64.qcow2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack image create --public --disk-format qcow2 --file CentOS-Stream-GenericCloud-9-20230417.0.x86_64.qcow2 centos9-stream
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Similarly, you can create the key pair like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack keypair create --public-key ~/.ssh/id_ed25519.pub default
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once these resources exists, configuration and deployment is otherwise relatively uncomplex.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;‚ùØ git clone https://github.com/shiftstack/poc-bgp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;‚ùØ cd poc-bgp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;‚ùØ cat &amp;lt;&amp;lt; EOF &amp;gt; localvars.yaml&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;cloud_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standalone_openshift&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;centos9-stream&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;instance_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;stephenfin-poc-bgp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;external_network&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostonly&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;keypair&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;dns_nameservers&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1.1.1.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;flavor&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;m1.tiny&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;‚ùØ make deploy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once deployed you will end up with 4 instances and 7 networks (6 plus the &lt;code&gt;hostonly&lt;/code&gt; network):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack server list
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;+--------------------------------------+----------------------------------+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------+
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| ID                                   | Name                             | Status | Networks                                                                                                                                                                   | Image          | Flavor    |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;+--------------------------------------+----------------------------------+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------+
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| a3375961-0821-4135-b064-b884460d11c2 | stephenfin-poc-bgp-spine-gateway | ACTIVE | hostonly&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.25.191, 2001:db8::100; stephenfin-poc-bgp-rack1-patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.0.1; stephenfin-poc-bgp-rack2-patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.0.5; stephenfin-poc-bgp-rack3-patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.0.9 | centos9-stream | m1.tiny   |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| f1e3b324-bf60-440e-808a-3e548f571f1a | stephenfin-poc-bgp-rack1-gateway | ACTIVE | stephenfin-poc-bgp-rack1-leaf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.10.1; stephenfin-poc-bgp-rack1-patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.0.2                                                                                     | centos9-stream | m1.tiny   |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 2b6051cb-f98a-4112-8096-c5c96c56f05a | stephenfin-poc-bgp-rack2-gateway | ACTIVE | stephenfin-poc-bgp-rack2-leaf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.20.1; stephenfin-poc-bgp-rack2-patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.0.6                                                                                     | centos9-stream | m1.tiny   |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 2d14cda5-377a-4383-b6e8-feb50455fad0 | stephenfin-poc-bgp-rack3-gateway | ACTIVE | stephenfin-poc-bgp-rack3-leaf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.30.1; stephenfin-poc-bgp-rack3-patch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;192.168.0.10                                                                                    | centos9-stream | m1.tiny   |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;+--------------------------------------+----------------------------------+--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------+
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack network list
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;+--------------------------------------+--------------------------------+----------------------------------------------------------------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| ID                                   | Name                           | Subnets                                                                    |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;+--------------------------------------+--------------------------------+----------------------------------------------------------------------------+
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 19708577-3757-4956-903e-9656229e1286 | stephenfin-poc-bgp-rack1-leaf  | 6152b856-781b-4fc9-9979-e4cfc9e282b8                                       |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 2a28f908-e9ec-4fb5-a4cb-09cd5dae8723 | stephenfin-poc-bgp-rack3-patch | 0f15d6ed-0b8a-435e-90d0-83edb2164100                                       |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 2cd308e3-8370-425b-af1c-0ce5562c2b36 | stephenfin-poc-bgp-rack1-patch | f22c9d7e-9c7f-4985-be7c-63eb05014544                                       |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 3060e6b0-c2c3-4f59-ac13-9d07a7a963f1 | stephenfin-poc-bgp-rack2-leaf  | 824cfd85-f89b-4a07-a330-3c3adb55b0e3                                       |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 538a2436-07c5-48f1-8cd0-39931244bcb7 | stephenfin-poc-bgp-rack3-leaf  | 3109bff8-fdd5-4824-ab2d-58184096e35c                                       |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| 91efea92-eaf8-4146-ac4c-48d3edfedfe4 | hostonly                       | 607ba284-3762-4667-a2fa-2a7f31de6f35, b147f959-3122-4ad9-aaaa-6ff1af41c8df |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;| b15da053-0625-4d33-ad6a-036dd76cfc8d | stephenfin-poc-bgp-rack2-patch | 71a6eeef-2c3d-45e2-80a9-efff6e15a289                                       |
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;+--------------------------------------+--------------------------------+----------------------------------------------------------------------------+
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;spine-gateway&lt;/code&gt; instance is connected to the three &lt;code&gt;rackN-gateway&lt;/code&gt; instances via separate patch networks, while the
three &lt;code&gt;rackN-gateway&lt;/code&gt; instances have their own &lt;code&gt;rackN-leaf&lt;/code&gt; leaf network. We&amp;rsquo;re going to use one of the latter as our
machine subnet when installing OpenShift shortly.&lt;/p&gt;
&lt;p&gt;We have some final steps to do here. First, we need to configure the subnets created by &lt;code&gt;poc-bgp&lt;/code&gt; to keep IP address
&amp;lt; &lt;code&gt;.10&lt;/code&gt; free. This is necessary because we are going to use some of these IP addresses for OpenShift VIPs shortly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack subnet set --no-allocation-pool --allocation-pool &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;start=192.168.10.10,end=192.168.10.239&amp;#34;&lt;/span&gt; stephenfin-poc-bgp-rack1-subnet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack subnet set --no-allocation-pool --allocation-pool &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;start=192.168.20.10,end=192.168.20.239&amp;#34;&lt;/span&gt; stephenfin-poc-bgp-rack2-subnet
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ openstack subnet set --no-allocation-pool --allocation-pool &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;start=192.168.30.10,end=192.168.30.239&amp;#34;&lt;/span&gt; stephenfin-poc-bgp-rack3-subnet
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In addition, we need another instance of &lt;code&gt;sshuttle&lt;/code&gt; to configure traffic for these new networks to route through the
&lt;code&gt;spine-gateway&lt;/code&gt; host. This will look something like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;‚ùØ sshuttle -r cloud-user@&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;GATEWAY_HOST_IP&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt; 192.168.10.0/24 192.168.20.0/24 192.168.30.0/24
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where &lt;code&gt;${GATEWAY_HOST_IP}&lt;/code&gt; is the IP of the &lt;code&gt;spine-gateway&lt;/code&gt; host (&lt;code&gt;192.168.25.191&lt;/code&gt; in my case).&lt;/p&gt;
&lt;p&gt;With our BGP routers in place, we can move onto the final stage of set up: installing OpenShift.&lt;/p&gt;
&lt;h2 id=&#34;openshift&#34;&gt;OpenShift&lt;/h2&gt;
&lt;p&gt;The last step of initial set up is installing OpenShift. I used &lt;code&gt;openshift-install&lt;/code&gt; to do this, deploying a 4.12
OpenShift cluster on my OpenStack cloud. When deploying OpenShift, you&amp;rsquo;ll want to pay special attention to the
networking configuration. As noted above, we want to use one of the &lt;code&gt;rackN-leaf&lt;/code&gt; networks and I chose &lt;code&gt;rack1-leaf&lt;/code&gt;
arbitrarily. You should use IPs from whatever subnet you choose for your API and Ingress VIPs, picking address in the
&amp;lt; &lt;code&gt;.10&lt;/code&gt; range we previously set aside. This means you should end up with configuration similar to the following in
your &lt;code&gt;install-config.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;platform&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;openstack&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;machinesSubnet&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6152b856-781b-4fc9-9979-e4cfc9e282b8 &lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# stephenfin-poc-bgp-rack1-subnet&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;apiVIPs&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.10.5&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# from stephenfin-poc-bgp-rack1-subnet, &amp;lt; .10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ingressVIPs&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.10.7&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# from stephenfin-poc-bgp-rack1-subnet, &amp;lt; .10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;networking&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;clusterNetworks&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.128.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/14&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;hostSubnetLength&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;serviceCIDR&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.30.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/16&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;machineNetwork&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.10.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.25.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In addition, assuming you don&amp;rsquo;t have a local DNS server configured, you will need to modify your &lt;code&gt;/etc/hosts&lt;/code&gt; file to
specify the hostnames and corresponding IP addresses of your OpenShift cluster. This is described in the
&lt;a href=&#34;https://github.com/openshift/installer/blob/master/docs/user/openstack/README.md&#34;&gt;OpenShift Installer docs&lt;/a&gt;, but in summary you&amp;rsquo;ll want something like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# openshift shiftstack nodes
192.168.10.5 api.stephenfin.openshift.shiftstack.test
192.168.10.7 console-openshift-console.apps.stephenfin.openshift.shiftstack.test
192.168.10.7 integrated-oauth-server-openshift-authentication.apps.stephenfin.openshift.shiftstack.test
192.168.10.7 oauth-openshift.apps.stephenfin.openshift.shiftstack.test
192.168.10.7 prometheus-k8s-openshift-monitoring.apps.stephenfin.openshift.shiftstack.test
192.168.10.7 grafana-openshift-monitoring.apps.stephenfin.openshift.shiftstack.test
# End of openshift nodes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should obviously change the hostnames and IP addresses to match whatever you configured in your
&lt;code&gt;install-config.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Once you have these steps completed, you can kick of installation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openshift-install --log-level debug create cluster
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Wait for that to complete, at which point you should have a fully functioning OpenShift deployment that you can interact
with using &lt;code&gt;oc&lt;/code&gt; or &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap Up&lt;/h2&gt;
&lt;p&gt;In this post, we deployed an OpenStack deployment, configured BGP software routers, and deployed OpenShift on OpenStack.
In the next post, we will work to install MetalLB itself. Stay tuned.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;appendix-configuration-files&#34;&gt;Appendix: Configuration files&lt;/h2&gt;
&lt;p&gt;Here are the full configuration files used for my deployment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;dev-install&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;local-overrides.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;acme&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;public_api&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.1.240.35&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;standalone_host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;acme.shiftstack.test&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;rhos_release&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;16.2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;clouddomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;shiftstack.test&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;poc-bgp&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;local_vars.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;cloud_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standalone_openshift&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;centos9-stream&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;instance_name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;stephenfin-poc-bgp&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;external_network&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostonly&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;keypair&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;dns_nameservers&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1.1.1.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;flavor&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;m1.tiny&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;openshift-installer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;install-config.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;baseDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;openshift.shiftstack.test&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;stephenfin&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;controlPlane&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;master&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;platform&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;openstack&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;m1.xlarge&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;compute&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;worker&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;platform&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;openstack&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;m1.xlarge&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;platform&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;openstack&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;defaultMachinePlatform&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;m1.xlarge&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;clusterOSImage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rhcos-4.12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;cloud&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standalone_openshift&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;machinesSubnet&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6152b856-781b-4fc9-9979-e4cfc9e282b8&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;apiVIPs&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.10.5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ingressVIPs&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.10.7&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;networking&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;clusterNetworks&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.128.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/14&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;hostSubnetLength&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;serviceCIDR&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.30.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/16&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;machineNetwork&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.10.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.168.25.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;pullSecret&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;redacted&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Handling the switch to the Cloud Controller Manager (CCM) with OpenShift Operators</title>
      <link>https://that.guru/blog/user-managed-configuration-in-cccmo/</link>
      <pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://that.guru/blog/user-managed-configuration-in-cccmo/</guid>
      <description>&lt;p&gt;Recent versions of Kubernetes have begun moving functionality that previously
existed in the core project out to separate projects. One such set of
functionality is the cloud provider-specific code, which is now handled by the
&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/cloud-controller/&#34;&gt;Cloud Controller Manager&lt;/a&gt; project. This is well described in the
&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/&#34;&gt;Kubernetes documentation&lt;/a&gt;.&lt;/p&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;Cloud providers implemented using Cloud Controller Manager may be referred to
as &lt;em&gt;external&lt;/em&gt; cloud providers (after the argument used to inform &lt;code&gt;kublet&lt;/code&gt; to
use CCM, &lt;code&gt;--cloud-provider=external&lt;/code&gt;) while the existing in-tree cloud
providers may be referred to as &lt;em&gt;internal&lt;/em&gt; or &lt;em&gt;legacy&lt;/em&gt; cloud providers. We use
these terms throughout.&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;In the 4.12 release, we hope to switch OpenShift deployments running on
OpenStack clouds from the legacy OpenStack cloud provider to the external
OpenStack cloud provider, &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/using-openstack-cloud-controller-manager.md&#34;&gt;OpenStack Cloud Controller Manager (OCCM)&lt;/a&gt;.
There are a couple of steps needed to make this happen, one of which is taking
user-provided configuration for the legacy cloud provider and mapping it to
configuration for the shiny new external cloud provider. This is necessary to
ensure any user-provided configuration is retained and the upgrade doesn&amp;rsquo;t
break the deployment. In the case of the OpenStack provider, this configuration
is INI-style and thankfully quite similar for both the legacy and external
cloud provider implementations.&lt;/p&gt;
&lt;p&gt;To handle the migration of configuration in OpenShift deployments, we are
relying on the &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator&#34;&gt;Cluster Cloud Controller Manager Operator (CCCMO)&lt;/a&gt;. This
operator is already responsible for managing the lifecycle of CCM on OpenShift
deployments, including configuration of CCM, so naturally it is a good fit for
this kind of task. A detailed description of the changes we ultimately made,
along with motivation for same, can be found &lt;a href=&#34;https://github.com/openshift/enhancements/pull/1009&#34;&gt;in this enhancement&lt;/a&gt;
(the &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/pull/178&#34;&gt;pull request itself&lt;/a&gt; is probably quite helpful also, if you read Go)
but I hope to explain them at a high level here since the paradigms used are
similar to those found in other operators and are being used to manage other
complex upgrades, such as the switch from in-tree block storage drivers to
Cluster Storage Interface (CSI) drivers.&lt;/p&gt;
&lt;h2 id=&#34;how-cccmo-generates-configuration&#34;&gt;How CCCMO generates configuration&lt;/h2&gt;
&lt;p&gt;The first step in understanding how CCCMO can be used to manage the migration
of configuration is to examine how CCCMO sources configuration - specifically
user-provided configuration - and uses this to generate the configuration
actually used for CCM. Once we understand this, we can decide at what points to
hook in and customise or translate this user-provided configuration. We can
also use this model in other operators. Thankfully, in the case of CCCMO this
sourcing and generation of configuration is pretty simple.&lt;/p&gt;
&lt;p&gt;Firstly, the operator attempt to retrieve config from the
&lt;code&gt;openshift-config-managed / kube-cloud-config&lt;/code&gt; config map:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm/kube-cloud-config -n openshift-config-managed -o yaml
apiVersion: v1
data:
  cloud.conf: |
    [Global]
    secret-name = openstack-credentials
    secret-namespace = kube-system
    region = regionOne
    [LoadBalancer]
    use-octavia = True
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2022-02-25T17:01:58Z&amp;quot;
  name: kube-cloud-config
  namespace: openshift-config-managed
  resourceVersion: &amp;quot;3853&amp;quot;
  uid: c23c14b7-66db-431c-a723-59439f946f80
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/pkg/controllers/cloud_config_sync_controller.go#L63-L68&#34;&gt;here&lt;/a&gt;.
The reason that it searches for this config map specifically is historical:
this is the config map generated by the &lt;a href=&#34;https://github.com/openshift/cluster-config-operator&#34;&gt;Cluster Config Operator (CCO)&lt;/a&gt;,
which is used to configure the legacy cloud provider (among other things). CCO
manipulates user-provided configuration for some cloud providers (specifically
AWS and Azure) so I guess the idea here was to avoid re-implementing this
transformation logic in CCCMO. Everything in the &lt;code&gt;openshift-config-managed&lt;/code&gt;
namespace is owned by CCO and is not intended to be modified by a user (in
fact, attempts to modify it will likely be futile and the operator will quickly
erase those changes).&lt;/p&gt;
&lt;p&gt;If the lookup of the &lt;code&gt;openshift-config-managed / kube-cloud-config&lt;/code&gt; config map
fails, we attempt to retrieve configuration from the &lt;code&gt;openshift-config / cloud-provider-config&lt;/code&gt; config map:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm/cloud-provider-config -n openshift-config -o yaml
apiVersion: v1
data:
  config: |
    [Global]
    secret-name = openstack-credentials
    secret-namespace = kube-system
    region = regionOne
    [LoadBalancer]
    use-octavia = True
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2022-02-25T17:00:15Z&amp;quot;
  name: cloud-provider-config
  namespace: openshift-config
  resourceVersion: &amp;quot;1802&amp;quot;
  uid: 45bda3c8-8866-4aea-92be-921502ff2055
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/pkg/controllers/cloud_config_sync_controller.go#L73-L87&#34;&gt;here&lt;/a&gt;.
Once again, the reason we use this config map is historical and is based on
what CCO uses. While things in the &lt;code&gt;openshift-config-managed&lt;/code&gt; namespace are
not user editable, the &lt;code&gt;openshift-config&lt;/code&gt; namespace is the namespace for
&amp;ldquo;user-managed&amp;rdquo; configuration or configuration that things like operators are
not allowed to modify.&lt;/p&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;The &lt;strong&gt;name&lt;/strong&gt; of this config map (as opposed to the namespace) is actually
cloud/infrastructure dependent and this is simply the OpenStack name. It is
defined as an attribute on the &lt;code&gt;cluster&lt;/code&gt; infrastructure resource.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get infrastructure/cluster -o jsonpath=&amp;quot;{.spec.cloudConfig.name}&amp;quot;
cloud-provider-config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;If the both lookup fails, we error out. However, this is unlikely since the
installer should create it as seen
&lt;a href=&#34;https://github.com/openshift/installer/blob/fd00a659/pkg/asset/manifests/cloudproviderconfig.go#L78-L93&#34;&gt;here&lt;/a&gt;.
Assuming one of them does exist, we sync whatever we found to the
&lt;code&gt;openshift-cloud-controller-manager / cloud-conf&lt;/code&gt; config map:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm/cloud-conf -n openshift-cloud-controller-manager -o yaml
apiVersion: v1
data:
  cloud.conf: |
    [Global]
    secret-name = openstack-credentials
    secret-namespace = kube-system
    region = regionOne
    [LoadBalancer]
    use-octavia = True
kind: ConfigMap
metadata:
  creationTimestamp: &amp;quot;2022-02-25T17:01:08Z&amp;quot;
  name: cloud-conf
  namespace: openshift-cloud-controller-manager
  resourceVersion: &amp;quot;2519&amp;quot;
  uid: cbbeedaf-41ed-41c2-9f37-4885732d3677
&lt;/code&gt;&lt;/pre&gt;

&lt;aside class=&#34;admonition note&#34;&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;&lt;p&gt;You can list all config maps in a namespace like this using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ oc get cm -n openshift-cloud-controller-manager \
    -o jsonpath=&#39;{range .items[*]}{.metadata.name}{&amp;quot;\n&amp;quot;}{end}&#39;
ccm-trusted-ca
cloud-conf
kube-root-ca.crt
openshift-service-ca.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/aside&gt;

&lt;p&gt;This can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/pkg/controllers/cloud_config_sync_controller.go#L96-L100&#34;&gt;here&lt;/a&gt;.
In this instance, the namespace isn&amp;rsquo;t actually locked in. It is possible to
configure the &lt;code&gt;cluster-controller-manager-operator&lt;/code&gt; binary with a &lt;code&gt;--namespace&lt;/code&gt;
argument and this option defaults to &lt;code&gt;openshift-cloud-controller-manager&lt;/code&gt;, as
seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/fde5bd72/cmd/cluster-cloud-controller-manager-operator/main.go#L85-L89&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-cccmo-handles-upgrades&#34;&gt;How CCCMO handles upgrades&lt;/h2&gt;
&lt;p&gt;(for OpenStack clouds on OpenShift 4.11 or later)&lt;/p&gt;
&lt;p&gt;So now that we understand how CCCMO sources user-provided configuration and
generates the resulting configuration used by Cloud Controller Manager, it&amp;rsquo;s
time to examine how we&amp;rsquo;ve decided to handle the migration of configuration for
legacy cloud providers to configuration suitable for external cloud provides.
As noted above, previously CCCMO took user-provided configuration from a config
map in one namespace and copied it to a config map in another namespace. It
should be pretty obvious that there&amp;rsquo;s no reason this copy has to be a
straightforward copy: we could modify the input config map before we dump it
back out. This is of course exactly what we did.&lt;/p&gt;
&lt;p&gt;Starting with the upcoming OpenShift 4.11 release, CCCMO provides configuration
&amp;ldquo;transformers&amp;rdquo;. Transformers simply load configuration provided by users, do
some basic validation, and then transform things by dropping options that are
no longer relevant, adding options that are now necessary, and renaming or
modifying options that have changed between the legacy. This idea isn&amp;rsquo;t
particularly novel - as noted previously, CCO was already doing something very
similar for AWS and Azure - but it works. Annoyingly these transformers must be
cloud-specific since the CCM binary used for each cloud provider expects a
radically different configuration files (in the case of the OpenStack cloud
provider this is an INI-style configuration file while Azure expects a
YAML-formatted file). As a result, we have only implemented the OpenStack
transformer for now. However, in the future we will likely implement additional
transformers for at least AWS and Azure since as noted previously CCO is
already doing some transformation here.&lt;/p&gt;
&lt;p&gt;Specifically, the transformer for OpenStack clouds in CCCMO currently does the
following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Drops the &lt;code&gt;[Global] secret-name&lt;/code&gt;, &lt;code&gt;[Global] secret-namespace&lt;/code&gt;, and &lt;code&gt;[Global] kubeconfig-path&lt;/code&gt; options, since these aren&amp;rsquo;t applicable for the external
cloud provider (the first two are OpenShift-only modifications). This inline
configuration has been replaced by configuration stored in a &lt;code&gt;clouds.yaml&lt;/code&gt;
file. Speaking of which&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adds the &lt;code&gt;[Global] use-clouds&lt;/code&gt;, &lt;code&gt;[Global] clouds-file&lt;/code&gt;, and &lt;code&gt;[Global] cloud&lt;/code&gt;
options.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Drops the entire &lt;code&gt;[BlockStorage]&lt;/code&gt; section since external cloud providers are
no longer responsible for anything storage&amp;rsquo;y (this is now handled by Cluster
Storage Interface (CSI) drivers, including the Manila CSI driver and Cinder
CSI driver)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adds or sets the &lt;code&gt;[LoadBalancer] use-octavia&lt;/code&gt; and &lt;code&gt;[LoadBalancer] enabled&lt;/code&gt;
options, depending on the specific deployment configuration (i.e. is Kuryr in
use?)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of this can be seen &lt;a href=&#34;https://github.com/openshift/cluster-cloud-controller-manager-operator/blob/13a37fe2/pkg/cloud/openstack/openstack.go#L136-L213&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this helps shine a little light on how CCCMO (and to a lesser degree,
CCM and CCO) works and operates, at least from a OpenStack perspective. For
most users, none of the above should matter: the OpenShift documentation
describes how configuration of the cloud provider, be it internal or external,
should happen via the &lt;code&gt;openshift-config / cloud-provider-config&lt;/code&gt; config map and
all of this transformation logic should be effectively invisible. However, when
things go wrong, it can be helpful to know in which dark corners to look üòÑ&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
